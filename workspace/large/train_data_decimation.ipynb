{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f38e88c4-a180-44c0-930c-ee618a009a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da1be5a8-6fc1-4824-8ef5-dfb35660f8e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-09 15:58:28,344] Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[2024-01-09 15:58:28,347] NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pppc\n",
    "from pppc.configs import PtychoNNTrainingConfigDict\n",
    "from pppc.ptychonn.trainer import PtychoNNTrainer\n",
    "from pppc.ptychonn.dataset_handle import HDF5Dataset\n",
    "from pppc.ptychonn.model import PtychoNNModel, PtychoNNTransposedConvModel\n",
    "from pppc.helper import transform_data_for_ptychonn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc996cd4-d304-4afd-b85a-dc0230c31638",
   "metadata": {},
   "source": [
    "Define some data transform functions for handling raw training data with different sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663610f3-99d2-45b3-b0c2-976c131613ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Raw DP size 512, label 128; crop DPs to 384, then rescale to 128. (use default function in HDF5Dataset)\n",
    "transform_func_512_128 = None\n",
    "transform_func_kwargs_512_128 = None\n",
    "\n",
    "# Raw DP size 512, label 128; crop DPs to 256, then pad to 384, and rescale to 128. (use default function in HDF5Dataset)\n",
    "def transform_func_512_128_pad(dp):\n",
    "    dp = transform_data_for_ptychonn(dp, target_shape=(256, 256), discard_len=(128, 128))\n",
    "    dp = transform_data_for_ptychonn(dp, target_shape=(128, 128), discard_len=(-64, -64))\n",
    "    return dp\n",
    "transform_func_kwargs_512_128_pad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c0c711-4d0a-4924-a515-c9c3fc8904be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-09 15:58:30,612] Decimating dataset to 0.9 of the original size...\n",
      "[2024-01-09 15:58:30,796] Using DataParallel with 2 devices.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 241/241 [04:40<00:00,  1.16s/it]\n",
      "[2024-01-09 16:03:15,111] Saving improved model after Val Loss improved from inf to 0.58433\n",
      "[2024-01-09 16:03:15,212] Epoch: 0 | FT  | Train Loss: 0.67550 | Val Loss: 0.58433\n",
      "[2024-01-09 16:03:15,213] Epoch: 0 | Amp | Train Loss: 0.1012 | Val Loss: 0.0224\n",
      "[2024-01-09 16:03:15,215] Epoch: 0 | Ph  | Train Loss: 0.574 | Val Loss: 0.562\n",
      "[2024-01-09 16:03:15,216] Epoch: 0 | Ending LR: 0.000050 \n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 241/241 [04:37<00:00,  1.15s/it]\n",
      "[2024-01-09 16:07:55,284] Saving improved model after Val Loss improved from 0.58433 to 0.57909\n",
      "[2024-01-09 16:07:55,380] Epoch: 1 | FT  | Train Loss: 0.58539 | Val Loss: 0.57909\n",
      "[2024-01-09 16:07:55,382] Epoch: 1 | Amp | Train Loss: 0.0222 | Val Loss: 0.0229\n",
      "[2024-01-09 16:07:55,383] Epoch: 1 | Ph  | Train Loss: 0.563 | Val Loss: 0.556\n",
      "[2024-01-09 16:07:55,384] Epoch: 1 | Ending LR: 0.000081 \n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 241/241 [04:36<00:00,  1.15s/it]\n",
      "[2024-01-09 16:12:34,383] Saving improved model after Val Loss improved from 0.57909 to 0.42247\n",
      "[2024-01-09 16:12:34,472] Epoch: 2 | FT  | Train Loss: 0.50758 | Val Loss: 0.42247\n",
      "[2024-01-09 16:12:34,473] Epoch: 2 | Amp | Train Loss: 0.0221 | Val Loss: 0.0222\n",
      "[2024-01-09 16:12:34,474] Epoch: 2 | Ph  | Train Loss: 0.485 | Val Loss: 0.400\n",
      "[2024-01-09 16:12:34,474] Epoch: 2 | Ending LR: 0.000111 \n",
      " 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍              | 226/241 [04:27<00:17,  1.18s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decimate_ratio_list = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "\n",
    "for decimate_ratio in decimate_ratio_list:\n",
    "    config_dict = PtychoNNTrainingConfigDict()\n",
    "    \n",
    "    config_dict['dataset_decimation_ratio'] = decimate_ratio\n",
    "    \n",
    "    config_dict['batch_size_per_process'] = 64\n",
    "    config_dict['num_epochs'] = 60\n",
    "    config_dict['learning_rate_per_process'] = 1e-4\n",
    "    config_dict['optimizer'] = 'adam'\n",
    "    config_dict['model_save_dir'] = '../../trained_models/model_36SpiralDatasets_dataDecimation_{}'.format(decimate_ratio)\n",
    "    config_dict['validation_ratio'] = 0.01\n",
    "    # Attention to transform_func and transform_func_kwargs. They must match the training data. \n",
    "    dataset = HDF5Dataset('data/data_train.h5', verbose=False, transform_func=transform_func_512_128, transform_func_kwargs=transform_func_kwargs_512_128)\n",
    "    config_dict['dataset'] = dataset\n",
    "    config_dict['model'] = (PtychoNNModel, {'n_levels': 4})\n",
    "    config_dict['debug'] = False\n",
    "    \n",
    "    trainer = PtychoNNTrainer(config_dict)\n",
    "    trainer.build(seed=196)\n",
    "    \n",
    "    trainer.run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee9d34-d678-46a0-adcf-664720e39601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.plot_training_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
