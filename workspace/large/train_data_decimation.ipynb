{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f38e88c4-a180-44c0-930c-ee618a009a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da1be5a8-6fc1-4824-8ef5-dfb35660f8e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-12 14:28:24,453] Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[2024-02-12 14:28:24,458] NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pppc\n",
    "from pppc.configs import PtychoNNTrainingConfigDict\n",
    "from pppc.ptychonn.trainer import PtychoNNTrainer\n",
    "from pppc.ptychonn.dataset_handle import HDF5Dataset\n",
    "from pppc.ptychonn.model import PtychoNNModel, PtychoNNTransposedConvModel, PtychoNNPhaseOnlyModel\n",
    "from pppc.helper import transform_data_for_ptychonn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc996cd4-d304-4afd-b85a-dc0230c31638",
   "metadata": {},
   "source": [
    "Define some data transform functions for handling raw training data with different sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663610f3-99d2-45b3-b0c2-976c131613ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Raw DP size 512, label 128; crop DPs to 384, then rescale to 128. (use default function in HDF5Dataset)\n",
    "transform_func_512_128 = None\n",
    "transform_func_kwargs_512_128 = None\n",
    "\n",
    "# Raw DP size 512, label 128; crop DPs to 256, then pad to 384, and rescale to 128. (use default function in HDF5Dataset)\n",
    "def transform_func_512_128_pad(dp):\n",
    "    dp = transform_data_for_ptychonn(dp, target_shape=(256, 256), discard_len=(128, 128))\n",
    "    dp = transform_data_for_ptychonn(dp, target_shape=(128, 128), discard_len=(-64, -64))\n",
    "    return dp\n",
    "transform_func_kwargs_512_128_pad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c0c711-4d0a-4924-a515-c9c3fc8904be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-12 14:28:26,389] Decimating dataset to 0.1 of the original size...\n",
      "[2024-02-12 14:28:26,535] Using DataParallel with 2 devices.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.16it/s]\n",
      "[2024-02-12 14:28:40,681] Saving improved model after Val Loss improved from inf to 0.67778\n",
      "[2024-02-12 14:28:40,777] Epoch: 0 | FT  | Train Loss: 0.72265 | Val Loss: 0.67778\n",
      "[2024-02-12 14:28:40,778] Epoch: 0 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:28:40,779] Epoch: 0 | Ph  | Train Loss: 0.723 | Val Loss: 0.678\n",
      "[2024-02-12 14:28:40,780] Epoch: 0 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.42it/s]\n",
      "[2024-02-12 14:28:52,559] Saving improved model after Val Loss improved from 0.67778 to 0.57672\n",
      "[2024-02-12 14:28:52,640] Epoch: 1 | FT  | Train Loss: 0.59425 | Val Loss: 0.57672\n",
      "[2024-02-12 14:28:52,641] Epoch: 1 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:28:52,642] Epoch: 1 | Ph  | Train Loss: 0.594 | Val Loss: 0.577\n",
      "[2024-02-12 14:28:52,643] Epoch: 1 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.52it/s]\n",
      "[2024-02-12 14:29:04,111] Saving improved model after Val Loss improved from 0.57672 to 0.55968\n",
      "[2024-02-12 14:29:04,187] Epoch: 2 | FT  | Train Loss: 0.55806 | Val Loss: 0.55968\n",
      "[2024-02-12 14:29:04,188] Epoch: 2 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:29:04,189] Epoch: 2 | Ph  | Train Loss: 0.558 | Val Loss: 0.560\n",
      "[2024-02-12 14:29:04,189] Epoch: 2 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.26it/s]\n",
      "[2024-02-12 14:29:16,316] Saving improved model after Val Loss improved from 0.55968 to 0.51848\n",
      "[2024-02-12 14:29:16,394] Epoch: 3 | FT  | Train Loss: 0.52469 | Val Loss: 0.51848\n",
      "[2024-02-12 14:29:16,395] Epoch: 3 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:29:16,395] Epoch: 3 | Ph  | Train Loss: 0.525 | Val Loss: 0.518\n",
      "[2024-02-12 14:29:16,396] Epoch: 3 | Ending LR: 0.000140 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.34it/s]\n",
      "[2024-02-12 14:29:28,274] Saving improved model after Val Loss improved from 0.51848 to 0.48422\n",
      "[2024-02-12 14:29:28,345] Epoch: 4 | FT  | Train Loss: 0.48176 | Val Loss: 0.48422\n",
      "[2024-02-12 14:29:28,346] Epoch: 4 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:29:28,346] Epoch: 4 | Ph  | Train Loss: 0.482 | Val Loss: 0.484\n",
      "[2024-02-12 14:29:28,347] Epoch: 4 | Ending LR: 0.000170 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.52it/s]\n",
      "[2024-02-12 14:29:40,052] Saving improved model after Val Loss improved from 0.48422 to 0.44877\n",
      "[2024-02-12 14:29:40,149] Epoch: 5 | FT  | Train Loss: 0.43777 | Val Loss: 0.44877\n",
      "[2024-02-12 14:29:40,149] Epoch: 5 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:29:40,150] Epoch: 5 | Ph  | Train Loss: 0.438 | Val Loss: 0.449\n",
      "[2024-02-12 14:29:40,150] Epoch: 5 | Ending LR: 0.000200 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.46it/s]\n",
      "[2024-02-12 14:29:51,855] Saving improved model after Val Loss improved from 0.44877 to 0.40963\n",
      "[2024-02-12 14:29:51,941] Epoch: 6 | FT  | Train Loss: 0.40297 | Val Loss: 0.40963\n",
      "[2024-02-12 14:29:51,942] Epoch: 6 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:29:51,943] Epoch: 6 | Ph  | Train Loss: 0.403 | Val Loss: 0.410\n",
      "[2024-02-12 14:29:51,943] Epoch: 6 | Ending LR: 0.000170 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.45it/s]\n",
      "[2024-02-12 14:30:03,604] Saving improved model after Val Loss improved from 0.40963 to 0.38221\n",
      "[2024-02-12 14:30:03,686] Epoch: 7 | FT  | Train Loss: 0.37755 | Val Loss: 0.38221\n",
      "[2024-02-12 14:30:03,687] Epoch: 7 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:30:03,688] Epoch: 7 | Ph  | Train Loss: 0.378 | Val Loss: 0.382\n",
      "[2024-02-12 14:30:03,688] Epoch: 7 | Ending LR: 0.000140 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.22it/s]\n",
      "[2024-02-12 14:30:15,999] Saving improved model after Val Loss improved from 0.38221 to 0.37971\n",
      "[2024-02-12 14:30:16,083] Epoch: 8 | FT  | Train Loss: 0.35929 | Val Loss: 0.37971\n",
      "[2024-02-12 14:30:16,084] Epoch: 8 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:30:16,085] Epoch: 8 | Ph  | Train Loss: 0.359 | Val Loss: 0.380\n",
      "[2024-02-12 14:30:16,086] Epoch: 8 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.37it/s]\n",
      "[2024-02-12 14:30:28,569] Saving improved model after Val Loss improved from 0.37971 to 0.35994\n",
      "[2024-02-12 14:30:28,653] Epoch: 9 | FT  | Train Loss: 0.34710 | Val Loss: 0.35994\n",
      "[2024-02-12 14:30:28,656] Epoch: 9 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:30:28,657] Epoch: 9 | Ph  | Train Loss: 0.347 | Val Loss: 0.360\n",
      "[2024-02-12 14:30:28,659] Epoch: 9 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.36it/s]\n",
      "[2024-02-12 14:30:40,525] Saving improved model after Val Loss improved from 0.35994 to 0.35494\n",
      "[2024-02-12 14:30:40,606] Epoch: 10 | FT  | Train Loss: 0.33577 | Val Loss: 0.35494\n",
      "[2024-02-12 14:30:40,607] Epoch: 10 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:30:40,607] Epoch: 10 | Ph  | Train Loss: 0.336 | Val Loss: 0.355\n",
      "[2024-02-12 14:30:40,608] Epoch: 10 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.43it/s]\n",
      "[2024-02-12 14:30:52,571] Saving improved model after Val Loss improved from 0.35494 to 0.35183\n",
      "[2024-02-12 14:30:52,682] Epoch: 11 | FT  | Train Loss: 0.32757 | Val Loss: 0.35183\n",
      "[2024-02-12 14:30:52,685] Epoch: 11 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:30:52,686] Epoch: 11 | Ph  | Train Loss: 0.328 | Val Loss: 0.352\n",
      "[2024-02-12 14:30:52,687] Epoch: 11 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.23it/s]\n",
      "[2024-02-12 14:31:05,012] Saving improved model after Val Loss improved from 0.35183 to 0.35139\n",
      "[2024-02-12 14:31:05,107] Epoch: 12 | FT  | Train Loss: 0.32416 | Val Loss: 0.35139\n",
      "[2024-02-12 14:31:05,108] Epoch: 12 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:31:05,108] Epoch: 12 | Ph  | Train Loss: 0.324 | Val Loss: 0.351\n",
      "[2024-02-12 14:31:05,109] Epoch: 12 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.45it/s]\n",
      "[2024-02-12 14:31:16,842] Saving improved model after Val Loss improved from 0.35139 to 0.34915\n",
      "[2024-02-12 14:31:16,920] Epoch: 13 | FT  | Train Loss: 0.32234 | Val Loss: 0.34915\n",
      "[2024-02-12 14:31:16,921] Epoch: 13 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:31:16,921] Epoch: 13 | Ph  | Train Loss: 0.322 | Val Loss: 0.349\n",
      "[2024-02-12 14:31:16,922] Epoch: 13 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.50it/s]\n",
      "[2024-02-12 14:31:28,642] Saving improved model after Val Loss improved from 0.34915 to 0.34659\n",
      "[2024-02-12 14:31:28,733] Epoch: 14 | FT  | Train Loss: 0.31991 | Val Loss: 0.34659\n",
      "[2024-02-12 14:31:28,734] Epoch: 14 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:31:28,735] Epoch: 14 | Ph  | Train Loss: 0.320 | Val Loss: 0.347\n",
      "[2024-02-12 14:31:28,736] Epoch: 14 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.41it/s]\n",
      "[2024-02-12 14:31:40,466] Epoch: 15 | FT  | Train Loss: 0.31596 | Val Loss: 0.35056\n",
      "[2024-02-12 14:31:40,470] Epoch: 15 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:31:40,471] Epoch: 15 | Ph  | Train Loss: 0.316 | Val Loss: 0.351\n",
      "[2024-02-12 14:31:40,472] Epoch: 15 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.31it/s]\n",
      "[2024-02-12 14:31:52,657] Epoch: 16 | FT  | Train Loss: 0.31392 | Val Loss: 0.35593\n",
      "[2024-02-12 14:31:52,665] Epoch: 16 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:31:52,669] Epoch: 16 | Ph  | Train Loss: 0.314 | Val Loss: 0.356\n",
      "[2024-02-12 14:31:52,671] Epoch: 16 | Ending LR: 0.000095 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.49it/s]\n",
      "[2024-02-12 14:32:04,252] Epoch: 17 | FT  | Train Loss: 0.31086 | Val Loss: 0.38366\n",
      "[2024-02-12 14:32:04,266] Epoch: 17 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:32:04,269] Epoch: 17 | Ph  | Train Loss: 0.311 | Val Loss: 0.384\n",
      "[2024-02-12 14:32:04,271] Epoch: 17 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.61it/s]\n",
      "[2024-02-12 14:32:15,850] Epoch: 18 | FT  | Train Loss: 0.30581 | Val Loss: 0.34715\n",
      "[2024-02-12 14:32:15,852] Epoch: 18 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:32:15,853] Epoch: 18 | Ph  | Train Loss: 0.306 | Val Loss: 0.347\n",
      "[2024-02-12 14:32:15,853] Epoch: 18 | Ending LR: 0.000095 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.44it/s]\n",
      "[2024-02-12 14:32:27,794] Saving improved model after Val Loss improved from 0.34659 to 0.33559\n",
      "[2024-02-12 14:32:27,885] Epoch: 19 | FT  | Train Loss: 0.29905 | Val Loss: 0.33559\n",
      "[2024-02-12 14:32:27,886] Epoch: 19 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:32:27,888] Epoch: 19 | Ph  | Train Loss: 0.299 | Val Loss: 0.336\n",
      "[2024-02-12 14:32:27,889] Epoch: 19 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.20it/s]\n",
      "[2024-02-12 14:32:40,252] Saving improved model after Val Loss improved from 0.33559 to 0.32971\n",
      "[2024-02-12 14:32:40,336] Epoch: 20 | FT  | Train Loss: 0.29214 | Val Loss: 0.32971\n",
      "[2024-02-12 14:32:40,337] Epoch: 20 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:32:40,337] Epoch: 20 | Ph  | Train Loss: 0.292 | Val Loss: 0.330\n",
      "[2024-02-12 14:32:40,338] Epoch: 20 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.35it/s]\n",
      "[2024-02-12 14:32:52,322] Saving improved model after Val Loss improved from 0.32971 to 0.32960\n",
      "[2024-02-12 14:32:52,437] Epoch: 21 | FT  | Train Loss: 0.28708 | Val Loss: 0.32960\n",
      "[2024-02-12 14:32:52,438] Epoch: 21 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:32:52,439] Epoch: 21 | Ph  | Train Loss: 0.287 | Val Loss: 0.330\n",
      "[2024-02-12 14:32:52,440] Epoch: 21 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.63it/s]\n",
      "[2024-02-12 14:33:03,795] Saving improved model after Val Loss improved from 0.32960 to 0.32677\n",
      "[2024-02-12 14:33:03,894] Epoch: 22 | FT  | Train Loss: 0.28279 | Val Loss: 0.32677\n",
      "[2024-02-12 14:33:03,897] Epoch: 22 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:33:03,898] Epoch: 22 | Ph  | Train Loss: 0.283 | Val Loss: 0.327\n",
      "[2024-02-12 14:33:03,899] Epoch: 22 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.49it/s]\n",
      "[2024-02-12 14:33:15,506] Saving improved model after Val Loss improved from 0.32677 to 0.32432\n",
      "[2024-02-12 14:33:15,584] Epoch: 23 | FT  | Train Loss: 0.27828 | Val Loss: 0.32432\n",
      "[2024-02-12 14:33:15,585] Epoch: 23 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:33:15,585] Epoch: 23 | Ph  | Train Loss: 0.278 | Val Loss: 0.324\n",
      "[2024-02-12 14:33:15,586] Epoch: 23 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.29it/s]\n",
      "[2024-02-12 14:33:27,785] Epoch: 24 | FT  | Train Loss: 0.27708 | Val Loss: 0.32579\n",
      "[2024-02-12 14:33:27,789] Epoch: 24 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:33:27,789] Epoch: 24 | Ph  | Train Loss: 0.277 | Val Loss: 0.326\n",
      "[2024-02-12 14:33:27,790] Epoch: 24 | Ending LR: 0.000028 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.32it/s]\n",
      "[2024-02-12 14:33:40,037] Epoch: 25 | FT  | Train Loss: 0.27613 | Val Loss: 0.32669\n",
      "[2024-02-12 14:33:40,039] Epoch: 25 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:33:40,040] Epoch: 25 | Ph  | Train Loss: 0.276 | Val Loss: 0.327\n",
      "[2024-02-12 14:33:40,041] Epoch: 25 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.50it/s]\n",
      "[2024-02-12 14:33:51,685] Epoch: 26 | FT  | Train Loss: 0.27603 | Val Loss: 0.33121\n",
      "[2024-02-12 14:33:51,686] Epoch: 26 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:33:51,687] Epoch: 26 | Ph  | Train Loss: 0.276 | Val Loss: 0.331\n",
      "[2024-02-12 14:33:51,688] Epoch: 26 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.43it/s]\n",
      "[2024-02-12 14:34:03,410] Epoch: 27 | FT  | Train Loss: 0.27585 | Val Loss: 0.32824\n",
      "[2024-02-12 14:34:03,412] Epoch: 27 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:34:03,413] Epoch: 27 | Ph  | Train Loss: 0.276 | Val Loss: 0.328\n",
      "[2024-02-12 14:34:03,414] Epoch: 27 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.32it/s]\n",
      "[2024-02-12 14:34:15,466] Epoch: 28 | FT  | Train Loss: 0.27334 | Val Loss: 0.32636\n",
      "[2024-02-12 14:34:15,467] Epoch: 28 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:34:15,468] Epoch: 28 | Ph  | Train Loss: 0.273 | Val Loss: 0.326\n",
      "[2024-02-12 14:34:15,469] Epoch: 28 | Ending LR: 0.000057 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.24it/s]\n",
      "[2024-02-12 14:34:27,829] Saving improved model after Val Loss improved from 0.32432 to 0.32425\n",
      "[2024-02-12 14:34:27,913] Epoch: 29 | FT  | Train Loss: 0.27347 | Val Loss: 0.32425\n",
      "[2024-02-12 14:34:27,914] Epoch: 29 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:34:27,914] Epoch: 29 | Ph  | Train Loss: 0.273 | Val Loss: 0.324\n",
      "[2024-02-12 14:34:27,914] Epoch: 29 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.52it/s]\n",
      "[2024-02-12 14:34:39,899] Epoch: 30 | FT  | Train Loss: 0.27051 | Val Loss: 0.33346\n",
      "[2024-02-12 14:34:39,901] Epoch: 30 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:34:39,902] Epoch: 30 | Ph  | Train Loss: 0.271 | Val Loss: 0.333\n",
      "[2024-02-12 14:34:39,903] Epoch: 30 | Ending LR: 0.000057 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.36it/s]\n",
      "[2024-02-12 14:34:52,085] Epoch: 31 | FT  | Train Loss: 0.26832 | Val Loss: 0.32489\n",
      "[2024-02-12 14:34:52,087] Epoch: 31 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:34:52,088] Epoch: 31 | Ph  | Train Loss: 0.268 | Val Loss: 0.325\n",
      "[2024-02-12 14:34:52,088] Epoch: 31 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.34it/s]\n",
      "[2024-02-12 14:35:04,311] Epoch: 32 | FT  | Train Loss: 0.26553 | Val Loss: 0.32529\n",
      "[2024-02-12 14:35:04,312] Epoch: 32 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:35:04,313] Epoch: 32 | Ph  | Train Loss: 0.266 | Val Loss: 0.325\n",
      "[2024-02-12 14:35:04,314] Epoch: 32 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.24it/s]\n",
      "[2024-02-12 14:35:16,959] Saving improved model after Val Loss improved from 0.32425 to 0.31976\n",
      "[2024-02-12 14:35:17,044] Epoch: 33 | FT  | Train Loss: 0.26099 | Val Loss: 0.31976\n",
      "[2024-02-12 14:35:17,044] Epoch: 33 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:35:17,045] Epoch: 33 | Ph  | Train Loss: 0.261 | Val Loss: 0.320\n",
      "[2024-02-12 14:35:17,045] Epoch: 33 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.48it/s]\n",
      "[2024-02-12 14:35:28,667] Epoch: 34 | FT  | Train Loss: 0.25828 | Val Loss: 0.32033\n",
      "[2024-02-12 14:35:28,669] Epoch: 34 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:35:28,670] Epoch: 34 | Ph  | Train Loss: 0.258 | Val Loss: 0.320\n",
      "[2024-02-12 14:35:28,671] Epoch: 34 | Ending LR: 0.000028 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:08<00:00,  2.99it/s]\n",
      "[2024-02-12 14:35:44,248] Saving improved model after Val Loss improved from 0.31976 to 0.31895\n",
      "[2024-02-12 14:35:44,342] Epoch: 35 | FT  | Train Loss: 0.25753 | Val Loss: 0.31895\n",
      "[2024-02-12 14:35:44,343] Epoch: 35 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:35:44,344] Epoch: 35 | Ph  | Train Loss: 0.258 | Val Loss: 0.319\n",
      "[2024-02-12 14:35:44,345] Epoch: 35 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.34it/s]\n",
      "[2024-02-12 14:35:56,301] Epoch: 36 | FT  | Train Loss: 0.25494 | Val Loss: 0.31943\n",
      "[2024-02-12 14:35:56,303] Epoch: 36 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:35:56,304] Epoch: 36 | Ph  | Train Loss: 0.255 | Val Loss: 0.319\n",
      "[2024-02-12 14:35:56,305] Epoch: 36 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.58it/s]\n",
      "[2024-02-12 14:36:07,692] Epoch: 37 | FT  | Train Loss: 0.25443 | Val Loss: 0.32042\n",
      "[2024-02-12 14:36:07,694] Epoch: 37 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:36:07,695] Epoch: 37 | Ph  | Train Loss: 0.254 | Val Loss: 0.320\n",
      "[2024-02-12 14:36:07,695] Epoch: 37 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.82it/s]\n",
      "[2024-02-12 14:36:18,472] Epoch: 38 | FT  | Train Loss: 0.25506 | Val Loss: 0.32415\n",
      "[2024-02-12 14:36:18,475] Epoch: 38 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:36:18,475] Epoch: 38 | Ph  | Train Loss: 0.255 | Val Loss: 0.324\n",
      "[2024-02-12 14:36:18,476] Epoch: 38 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.78it/s]\n",
      "[2024-02-12 14:36:29,349] Epoch: 39 | FT  | Train Loss: 0.25406 | Val Loss: 0.31928\n",
      "[2024-02-12 14:36:29,352] Epoch: 39 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:36:29,353] Epoch: 39 | Ph  | Train Loss: 0.254 | Val Loss: 0.319\n",
      "[2024-02-12 14:36:29,353] Epoch: 39 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.76it/s]\n",
      "[2024-02-12 14:36:40,340] Saving improved model after Val Loss improved from 0.31895 to 0.31823\n",
      "[2024-02-12 14:36:40,435] Epoch: 40 | FT  | Train Loss: 0.25264 | Val Loss: 0.31823\n",
      "[2024-02-12 14:36:40,436] Epoch: 40 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:36:40,437] Epoch: 40 | Ph  | Train Loss: 0.253 | Val Loss: 0.318\n",
      "[2024-02-12 14:36:40,437] Epoch: 40 | Ending LR: 0.000039 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.86it/s]\n",
      "[2024-02-12 14:36:51,031] Epoch: 41 | FT  | Train Loss: 0.25255 | Val Loss: 0.31972\n",
      "[2024-02-12 14:36:51,033] Epoch: 41 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:36:51,034] Epoch: 41 | Ph  | Train Loss: 0.253 | Val Loss: 0.320\n",
      "[2024-02-12 14:36:51,034] Epoch: 41 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.84it/s]\n",
      "[2024-02-12 14:37:01,849] Epoch: 42 | FT  | Train Loss: 0.25153 | Val Loss: 0.32070\n",
      "[2024-02-12 14:37:01,850] Epoch: 42 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:37:01,851] Epoch: 42 | Ph  | Train Loss: 0.252 | Val Loss: 0.321\n",
      "[2024-02-12 14:37:01,851] Epoch: 42 | Ending LR: 0.000039 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.81it/s]\n",
      "[2024-02-12 14:37:12,371] Epoch: 43 | FT  | Train Loss: 0.25041 | Val Loss: 0.31871\n",
      "[2024-02-12 14:37:12,372] Epoch: 43 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:37:12,372] Epoch: 43 | Ph  | Train Loss: 0.250 | Val Loss: 0.319\n",
      "[2024-02-12 14:37:12,373] Epoch: 43 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.82it/s]\n",
      "[2024-02-12 14:37:23,039] Epoch: 44 | FT  | Train Loss: 0.24797 | Val Loss: 0.32062\n",
      "[2024-02-12 14:37:23,040] Epoch: 44 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:37:23,041] Epoch: 44 | Ph  | Train Loss: 0.248 | Val Loss: 0.321\n",
      "[2024-02-12 14:37:23,041] Epoch: 44 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.74it/s]\n",
      "[2024-02-12 14:37:33,816] Saving improved model after Val Loss improved from 0.31823 to 0.31790\n",
      "[2024-02-12 14:37:33,906] Epoch: 45 | FT  | Train Loss: 0.24612 | Val Loss: 0.31790\n",
      "[2024-02-12 14:37:33,907] Epoch: 45 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:37:33,907] Epoch: 45 | Ph  | Train Loss: 0.246 | Val Loss: 0.318\n",
      "[2024-02-12 14:37:33,908] Epoch: 45 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.86it/s]\n",
      "[2024-02-12 14:37:44,678] Epoch: 46 | FT  | Train Loss: 0.24510 | Val Loss: 0.31837\n",
      "[2024-02-12 14:37:44,679] Epoch: 46 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:37:44,680] Epoch: 46 | Ph  | Train Loss: 0.245 | Val Loss: 0.318\n",
      "[2024-02-12 14:37:44,680] Epoch: 46 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.69it/s]\n",
      "[2024-02-12 14:37:55,616] Saving improved model after Val Loss improved from 0.31790 to 0.31749\n",
      "[2024-02-12 14:37:55,706] Epoch: 47 | FT  | Train Loss: 0.24406 | Val Loss: 0.31749\n",
      "[2024-02-12 14:37:55,707] Epoch: 47 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:37:55,708] Epoch: 47 | Ph  | Train Loss: 0.244 | Val Loss: 0.317\n",
      "[2024-02-12 14:37:55,708] Epoch: 47 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.77it/s]\n",
      "[2024-02-12 14:38:06,441] Epoch: 48 | FT  | Train Loss: 0.24210 | Val Loss: 0.32308\n",
      "[2024-02-12 14:38:06,442] Epoch: 48 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:38:06,442] Epoch: 48 | Ph  | Train Loss: 0.242 | Val Loss: 0.323\n",
      "[2024-02-12 14:38:06,443] Epoch: 48 | Ending LR: 0.000022 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.74it/s]\n",
      "[2024-02-12 14:38:17,549] Epoch: 49 | FT  | Train Loss: 0.24304 | Val Loss: 0.31978\n",
      "[2024-02-12 14:38:17,550] Epoch: 49 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:38:17,551] Epoch: 49 | Ph  | Train Loss: 0.243 | Val Loss: 0.320\n",
      "[2024-02-12 14:38:17,552] Epoch: 49 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.83it/s]\n",
      "[2024-02-12 14:38:28,430] Epoch: 50 | FT  | Train Loss: 0.24212 | Val Loss: 0.32120\n",
      "[2024-02-12 14:38:28,431] Epoch: 50 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:38:28,432] Epoch: 50 | Ph  | Train Loss: 0.242 | Val Loss: 0.321\n",
      "[2024-02-12 14:38:28,432] Epoch: 50 | Ending LR: 0.000026 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.73it/s]\n",
      "[2024-02-12 14:38:39,796] Epoch: 51 | FT  | Train Loss: 0.23988 | Val Loss: 0.31849\n",
      "[2024-02-12 14:38:39,797] Epoch: 51 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:38:39,798] Epoch: 51 | Ph  | Train Loss: 0.240 | Val Loss: 0.318\n",
      "[2024-02-12 14:38:39,798] Epoch: 51 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.56it/s]\n",
      "[2024-02-12 14:38:51,237] Epoch: 52 | FT  | Train Loss: 0.24000 | Val Loss: 0.31968\n",
      "[2024-02-12 14:38:51,238] Epoch: 52 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:38:51,238] Epoch: 52 | Ph  | Train Loss: 0.240 | Val Loss: 0.320\n",
      "[2024-02-12 14:38:51,239] Epoch: 52 | Ending LR: 0.000029 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.64it/s]\n",
      "[2024-02-12 14:39:02,465] Epoch: 53 | FT  | Train Loss: 0.23938 | Val Loss: 0.32066\n",
      "[2024-02-12 14:39:02,466] Epoch: 53 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:39:02,466] Epoch: 53 | Ph  | Train Loss: 0.239 | Val Loss: 0.321\n",
      "[2024-02-12 14:39:02,467] Epoch: 53 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.94it/s]\n",
      "[2024-02-12 14:39:12,880] Epoch: 54 | FT  | Train Loss: 0.23964 | Val Loss: 0.31906\n",
      "[2024-02-12 14:39:12,881] Epoch: 54 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:39:12,881] Epoch: 54 | Ph  | Train Loss: 0.240 | Val Loss: 0.319\n",
      "[2024-02-12 14:39:12,882] Epoch: 54 | Ending LR: 0.000029 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.85it/s]\n",
      "[2024-02-12 14:39:23,415] Epoch: 55 | FT  | Train Loss: 0.23878 | Val Loss: 0.32138\n",
      "[2024-02-12 14:39:23,416] Epoch: 55 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:39:23,417] Epoch: 55 | Ph  | Train Loss: 0.239 | Val Loss: 0.321\n",
      "[2024-02-12 14:39:23,417] Epoch: 55 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.74it/s]\n",
      "[2024-02-12 14:39:34,191] Epoch: 56 | FT  | Train Loss: 0.23678 | Val Loss: 0.32150\n",
      "[2024-02-12 14:39:34,192] Epoch: 56 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:39:34,193] Epoch: 56 | Ph  | Train Loss: 0.237 | Val Loss: 0.321\n",
      "[2024-02-12 14:39:34,193] Epoch: 56 | Ending LR: 0.000026 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.90it/s]\n",
      "[2024-02-12 14:39:45,190] Epoch: 57 | FT  | Train Loss: 0.23537 | Val Loss: 0.31760\n",
      "[2024-02-12 14:39:45,191] Epoch: 57 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:39:45,191] Epoch: 57 | Ph  | Train Loss: 0.235 | Val Loss: 0.318\n",
      "[2024-02-12 14:39:45,192] Epoch: 57 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.96it/s]\n",
      "[2024-02-12 14:39:55,725] Epoch: 58 | FT  | Train Loss: 0.23485 | Val Loss: 0.31774\n",
      "[2024-02-12 14:39:55,726] Epoch: 58 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:39:55,727] Epoch: 58 | Ph  | Train Loss: 0.235 | Val Loss: 0.318\n",
      "[2024-02-12 14:39:55,727] Epoch: 58 | Ending LR: 0.000022 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.83it/s]\n",
      "[2024-02-12 14:40:06,240] Epoch: 59 | FT  | Train Loss: 0.23509 | Val Loss: 0.31934\n",
      "[2024-02-12 14:40:06,241] Epoch: 59 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:40:06,241] Epoch: 59 | Ph  | Train Loss: 0.235 | Val Loss: 0.319\n",
      "[2024-02-12 14:40:06,242] Epoch: 59 | Ending LR: 0.000020 \n",
      "[2024-02-12 14:40:06,331] Decimating dataset to 0.05 of the original size...\n",
      "[2024-02-12 14:40:06,399] Using DataParallel with 2 devices.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.89it/s]\n",
      "[2024-02-12 14:40:13,831] Saving improved model after Val Loss improved from inf to 0.69438\n",
      "[2024-02-12 14:40:13,863] Epoch: 0 | FT  | Train Loss: 0.76991 | Val Loss: 0.69438\n",
      "[2024-02-12 14:40:13,864] Epoch: 0 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:40:13,865] Epoch: 0 | Ph  | Train Loss: 0.770 | Val Loss: 0.694\n",
      "[2024-02-12 14:40:13,865] Epoch: 0 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.78it/s]\n",
      "[2024-02-12 14:40:21,387] Saving improved model after Val Loss improved from 0.69438 to 0.66603\n",
      "[2024-02-12 14:40:21,465] Epoch: 1 | FT  | Train Loss: 0.65106 | Val Loss: 0.66603\n",
      "[2024-02-12 14:40:21,466] Epoch: 1 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:40:21,467] Epoch: 1 | Ph  | Train Loss: 0.651 | Val Loss: 0.666\n",
      "[2024-02-12 14:40:21,468] Epoch: 1 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.91it/s]\n",
      "[2024-02-12 14:40:28,908] Saving improved model after Val Loss improved from 0.66603 to 0.58347\n",
      "[2024-02-12 14:40:28,986] Epoch: 2 | FT  | Train Loss: 0.58603 | Val Loss: 0.58347\n",
      "[2024-02-12 14:40:28,987] Epoch: 2 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:40:28,987] Epoch: 2 | Ph  | Train Loss: 0.586 | Val Loss: 0.583\n",
      "[2024-02-12 14:40:28,988] Epoch: 2 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.58it/s]\n",
      "[2024-02-12 14:40:36,415] Saving improved model after Val Loss improved from 0.58347 to 0.56733\n",
      "[2024-02-12 14:40:36,512] Epoch: 3 | FT  | Train Loss: 0.56674 | Val Loss: 0.56733\n",
      "[2024-02-12 14:40:36,513] Epoch: 3 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:40:36,514] Epoch: 3 | Ph  | Train Loss: 0.567 | Val Loss: 0.567\n",
      "[2024-02-12 14:40:36,514] Epoch: 3 | Ending LR: 0.000140 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.88it/s]\n",
      "[2024-02-12 14:40:44,062] Saving improved model after Val Loss improved from 0.56733 to 0.55662\n",
      "[2024-02-12 14:40:44,154] Epoch: 4 | FT  | Train Loss: 0.54593 | Val Loss: 0.55662\n",
      "[2024-02-12 14:40:44,155] Epoch: 4 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:40:44,156] Epoch: 4 | Ph  | Train Loss: 0.546 | Val Loss: 0.557\n",
      "[2024-02-12 14:40:44,156] Epoch: 4 | Ending LR: 0.000170 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.98it/s]\n",
      "[2024-02-12 14:40:51,385] Saving improved model after Val Loss improved from 0.55662 to 0.52370\n",
      "[2024-02-12 14:40:51,461] Epoch: 5 | FT  | Train Loss: 0.52231 | Val Loss: 0.52370\n",
      "[2024-02-12 14:40:51,462] Epoch: 5 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:40:51,462] Epoch: 5 | Ph  | Train Loss: 0.522 | Val Loss: 0.524\n",
      "[2024-02-12 14:40:51,463] Epoch: 5 | Ending LR: 0.000200 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  4.03it/s]\n",
      "[2024-02-12 14:40:58,596] Saving improved model after Val Loss improved from 0.52370 to 0.50930\n",
      "[2024-02-12 14:40:58,674] Epoch: 6 | FT  | Train Loss: 0.49516 | Val Loss: 0.50930\n",
      "[2024-02-12 14:40:58,675] Epoch: 6 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:40:58,676] Epoch: 6 | Ph  | Train Loss: 0.495 | Val Loss: 0.509\n",
      "[2024-02-12 14:40:58,676] Epoch: 6 | Ending LR: 0.000170 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.84it/s]\n",
      "[2024-02-12 14:41:06,596] Saving improved model after Val Loss improved from 0.50930 to 0.49474\n",
      "[2024-02-12 14:41:06,680] Epoch: 7 | FT  | Train Loss: 0.47707 | Val Loss: 0.49474\n",
      "[2024-02-12 14:41:06,681] Epoch: 7 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:41:06,682] Epoch: 7 | Ph  | Train Loss: 0.477 | Val Loss: 0.495\n",
      "[2024-02-12 14:41:06,684] Epoch: 7 | Ending LR: 0.000140 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.85it/s]\n",
      "[2024-02-12 14:41:14,482] Saving improved model after Val Loss improved from 0.49474 to 0.47511\n",
      "[2024-02-12 14:41:14,566] Epoch: 8 | FT  | Train Loss: 0.45541 | Val Loss: 0.47511\n",
      "[2024-02-12 14:41:14,567] Epoch: 8 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:41:14,567] Epoch: 8 | Ph  | Train Loss: 0.455 | Val Loss: 0.475\n",
      "[2024-02-12 14:41:14,568] Epoch: 8 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.78it/s]\n",
      "[2024-02-12 14:41:22,259] Saving improved model after Val Loss improved from 0.47511 to 0.45683\n",
      "[2024-02-12 14:41:22,336] Epoch: 9 | FT  | Train Loss: 0.43867 | Val Loss: 0.45683\n",
      "[2024-02-12 14:41:22,337] Epoch: 9 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:41:22,338] Epoch: 9 | Ph  | Train Loss: 0.439 | Val Loss: 0.457\n",
      "[2024-02-12 14:41:22,339] Epoch: 9 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.96it/s]\n",
      "[2024-02-12 14:41:29,682] Saving improved model after Val Loss improved from 0.45683 to 0.44844\n",
      "[2024-02-12 14:41:29,753] Epoch: 10 | FT  | Train Loss: 0.42376 | Val Loss: 0.44844\n",
      "[2024-02-12 14:41:29,754] Epoch: 10 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:41:29,755] Epoch: 10 | Ph  | Train Loss: 0.424 | Val Loss: 0.448\n",
      "[2024-02-12 14:41:29,756] Epoch: 10 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.79it/s]\n",
      "[2024-02-12 14:41:37,623] Saving improved model after Val Loss improved from 0.44844 to 0.44273\n",
      "[2024-02-12 14:41:37,659] Epoch: 11 | FT  | Train Loss: 0.41303 | Val Loss: 0.44273\n",
      "[2024-02-12 14:41:37,660] Epoch: 11 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:41:37,661] Epoch: 11 | Ph  | Train Loss: 0.413 | Val Loss: 0.443\n",
      "[2024-02-12 14:41:37,662] Epoch: 11 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.70it/s]\n",
      "[2024-02-12 14:41:45,498] Epoch: 12 | FT  | Train Loss: 0.40832 | Val Loss: 0.44415\n",
      "[2024-02-12 14:41:45,499] Epoch: 12 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:41:45,500] Epoch: 12 | Ph  | Train Loss: 0.408 | Val Loss: 0.444\n",
      "[2024-02-12 14:41:45,501] Epoch: 12 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.72it/s]\n",
      "[2024-02-12 14:41:54,108] Saving improved model after Val Loss improved from 0.44273 to 0.44262\n",
      "[2024-02-12 14:41:54,182] Epoch: 13 | FT  | Train Loss: 0.40459 | Val Loss: 0.44262\n",
      "[2024-02-12 14:41:54,184] Epoch: 13 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:41:54,185] Epoch: 13 | Ph  | Train Loss: 0.405 | Val Loss: 0.443\n",
      "[2024-02-12 14:41:54,187] Epoch: 13 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:05<00:00,  2.49it/s]\n",
      "[2024-02-12 14:42:04,662] Saving improved model after Val Loss improved from 0.44262 to 0.44241\n",
      "[2024-02-12 14:42:04,758] Epoch: 14 | FT  | Train Loss: 0.39758 | Val Loss: 0.44241\n",
      "[2024-02-12 14:42:04,759] Epoch: 14 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:42:04,760] Epoch: 14 | Ph  | Train Loss: 0.398 | Val Loss: 0.442\n",
      "[2024-02-12 14:42:04,761] Epoch: 14 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:04<00:00,  2.86it/s]\n",
      "[2024-02-12 14:42:14,738] Saving improved model after Val Loss improved from 0.44241 to 0.44166\n",
      "[2024-02-12 14:42:14,813] Epoch: 15 | FT  | Train Loss: 0.39203 | Val Loss: 0.44166\n",
      "[2024-02-12 14:42:14,815] Epoch: 15 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:42:14,815] Epoch: 15 | Ph  | Train Loss: 0.392 | Val Loss: 0.442\n",
      "[2024-02-12 14:42:14,816] Epoch: 15 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:05<00:00,  2.60it/s]\n",
      "[2024-02-12 14:42:25,151] Saving improved model after Val Loss improved from 0.44166 to 0.42792\n",
      "[2024-02-12 14:42:25,245] Epoch: 16 | FT  | Train Loss: 0.38739 | Val Loss: 0.42792\n",
      "[2024-02-12 14:42:25,246] Epoch: 16 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:42:25,247] Epoch: 16 | Ph  | Train Loss: 0.387 | Val Loss: 0.428\n",
      "[2024-02-12 14:42:25,248] Epoch: 16 | Ending LR: 0.000095 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:05<00:00,  2.47it/s]\n",
      "[2024-02-12 14:42:35,821] Epoch: 17 | FT  | Train Loss: 0.37909 | Val Loss: 0.43252\n",
      "[2024-02-12 14:42:35,825] Epoch: 17 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:42:35,826] Epoch: 17 | Ph  | Train Loss: 0.379 | Val Loss: 0.433\n",
      "[2024-02-12 14:42:35,827] Epoch: 17 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:05<00:00,  2.51it/s]\n",
      "[2024-02-12 14:42:46,083] Epoch: 18 | FT  | Train Loss: 0.37455 | Val Loss: 0.43663\n",
      "[2024-02-12 14:42:46,088] Epoch: 18 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:42:46,089] Epoch: 18 | Ph  | Train Loss: 0.375 | Val Loss: 0.437\n",
      "[2024-02-12 14:42:46,090] Epoch: 18 | Ending LR: 0.000095 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:05<00:00,  2.55it/s]\n",
      "[2024-02-12 14:42:56,363] Saving improved model after Val Loss improved from 0.42792 to 0.42582\n",
      "[2024-02-12 14:42:56,473] Epoch: 19 | FT  | Train Loss: 0.36303 | Val Loss: 0.42582\n",
      "[2024-02-12 14:42:56,474] Epoch: 19 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:42:56,475] Epoch: 19 | Ph  | Train Loss: 0.363 | Val Loss: 0.426\n",
      "[2024-02-12 14:42:56,476] Epoch: 19 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:04<00:00,  2.64it/s]\n",
      "[2024-02-12 14:43:06,692] Saving improved model after Val Loss improved from 0.42582 to 0.41588\n",
      "[2024-02-12 14:43:06,789] Epoch: 20 | FT  | Train Loss: 0.35485 | Val Loss: 0.41588\n",
      "[2024-02-12 14:43:06,790] Epoch: 20 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:43:06,791] Epoch: 20 | Ph  | Train Loss: 0.355 | Val Loss: 0.416\n",
      "[2024-02-12 14:43:06,793] Epoch: 20 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:04<00:00,  2.61it/s]\n",
      "[2024-02-12 14:43:16,873] Saving improved model after Val Loss improved from 0.41588 to 0.40943\n",
      "[2024-02-12 14:43:16,960] Epoch: 21 | FT  | Train Loss: 0.34771 | Val Loss: 0.40943\n",
      "[2024-02-12 14:43:16,961] Epoch: 21 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:43:16,961] Epoch: 21 | Ph  | Train Loss: 0.348 | Val Loss: 0.409\n",
      "[2024-02-12 14:43:16,962] Epoch: 21 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:05<00:00,  2.54it/s]\n",
      "[2024-02-12 14:43:26,775] Saving improved model after Val Loss improved from 0.40943 to 0.40914\n",
      "[2024-02-12 14:43:26,858] Epoch: 22 | FT  | Train Loss: 0.34110 | Val Loss: 0.40914\n",
      "[2024-02-12 14:43:26,859] Epoch: 22 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:43:26,859] Epoch: 22 | Ph  | Train Loss: 0.341 | Val Loss: 0.409\n",
      "[2024-02-12 14:43:26,860] Epoch: 22 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.96it/s]\n",
      "[2024-02-12 14:43:34,314] Saving improved model after Val Loss improved from 0.40914 to 0.40394\n",
      "[2024-02-12 14:43:34,385] Epoch: 23 | FT  | Train Loss: 0.33615 | Val Loss: 0.40394\n",
      "[2024-02-12 14:43:34,386] Epoch: 23 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:43:34,387] Epoch: 23 | Ph  | Train Loss: 0.336 | Val Loss: 0.404\n",
      "[2024-02-12 14:43:34,387] Epoch: 23 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  4.03it/s]\n",
      "[2024-02-12 14:43:41,693] Epoch: 24 | FT  | Train Loss: 0.33449 | Val Loss: 0.41143\n",
      "[2024-02-12 14:43:41,694] Epoch: 24 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:43:41,694] Epoch: 24 | Ph  | Train Loss: 0.334 | Val Loss: 0.411\n",
      "[2024-02-12 14:43:41,695] Epoch: 24 | Ending LR: 0.000028 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.87it/s]\n",
      "[2024-02-12 14:43:49,541] Epoch: 25 | FT  | Train Loss: 0.33305 | Val Loss: 0.40814\n",
      "[2024-02-12 14:43:49,543] Epoch: 25 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:43:49,544] Epoch: 25 | Ph  | Train Loss: 0.333 | Val Loss: 0.408\n",
      "[2024-02-12 14:43:49,544] Epoch: 25 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.87it/s]\n",
      "[2024-02-12 14:43:57,196] Epoch: 26 | FT  | Train Loss: 0.32848 | Val Loss: 0.40404\n",
      "[2024-02-12 14:43:57,197] Epoch: 26 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:43:57,197] Epoch: 26 | Ph  | Train Loss: 0.328 | Val Loss: 0.404\n",
      "[2024-02-12 14:43:57,198] Epoch: 26 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.50it/s]\n",
      "[2024-02-12 14:44:04,965] Epoch: 27 | FT  | Train Loss: 0.32850 | Val Loss: 0.40922\n",
      "[2024-02-12 14:44:04,966] Epoch: 27 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:44:04,967] Epoch: 27 | Ph  | Train Loss: 0.329 | Val Loss: 0.409\n",
      "[2024-02-12 14:44:04,968] Epoch: 27 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.91it/s]\n",
      "[2024-02-12 14:44:12,227] Epoch: 28 | FT  | Train Loss: 0.32593 | Val Loss: 0.41130\n",
      "[2024-02-12 14:44:12,228] Epoch: 28 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:44:12,229] Epoch: 28 | Ph  | Train Loss: 0.326 | Val Loss: 0.411\n",
      "[2024-02-12 14:44:12,229] Epoch: 28 | Ending LR: 0.000057 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.95it/s]\n",
      "[2024-02-12 14:44:19,624] Epoch: 29 | FT  | Train Loss: 0.32326 | Val Loss: 0.41569\n",
      "[2024-02-12 14:44:19,627] Epoch: 29 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:44:19,627] Epoch: 29 | Ph  | Train Loss: 0.323 | Val Loss: 0.416\n",
      "[2024-02-12 14:44:19,628] Epoch: 29 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.80it/s]\n",
      "[2024-02-12 14:44:27,178] Epoch: 30 | FT  | Train Loss: 0.32317 | Val Loss: 0.40981\n",
      "[2024-02-12 14:44:27,180] Epoch: 30 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:44:27,180] Epoch: 30 | Ph  | Train Loss: 0.323 | Val Loss: 0.410\n",
      "[2024-02-12 14:44:27,181] Epoch: 30 | Ending LR: 0.000057 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.98it/s]\n",
      "[2024-02-12 14:44:34,797] Saving improved model after Val Loss improved from 0.40394 to 0.40122\n",
      "[2024-02-12 14:44:34,882] Epoch: 31 | FT  | Train Loss: 0.31709 | Val Loss: 0.40122\n",
      "[2024-02-12 14:44:34,883] Epoch: 31 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:44:34,884] Epoch: 31 | Ph  | Train Loss: 0.317 | Val Loss: 0.401\n",
      "[2024-02-12 14:44:34,884] Epoch: 31 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.76it/s]\n",
      "[2024-02-12 14:44:42,610] Saving improved model after Val Loss improved from 0.40122 to 0.40008\n",
      "[2024-02-12 14:44:42,692] Epoch: 32 | FT  | Train Loss: 0.31553 | Val Loss: 0.40008\n",
      "[2024-02-12 14:44:42,693] Epoch: 32 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:44:42,693] Epoch: 32 | Ph  | Train Loss: 0.316 | Val Loss: 0.400\n",
      "[2024-02-12 14:44:42,694] Epoch: 32 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.86it/s]\n",
      "[2024-02-12 14:44:50,199] Saving improved model after Val Loss improved from 0.40008 to 0.39876\n",
      "[2024-02-12 14:44:50,274] Epoch: 33 | FT  | Train Loss: 0.30816 | Val Loss: 0.39876\n",
      "[2024-02-12 14:44:50,275] Epoch: 33 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:44:50,275] Epoch: 33 | Ph  | Train Loss: 0.308 | Val Loss: 0.399\n",
      "[2024-02-12 14:44:50,276] Epoch: 33 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.94it/s]\n",
      "[2024-02-12 14:44:57,527] Saving improved model after Val Loss improved from 0.39876 to 0.39871\n",
      "[2024-02-12 14:44:57,606] Epoch: 34 | FT  | Train Loss: 0.30468 | Val Loss: 0.39871\n",
      "[2024-02-12 14:44:57,607] Epoch: 34 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:44:57,608] Epoch: 34 | Ph  | Train Loss: 0.305 | Val Loss: 0.399\n",
      "[2024-02-12 14:44:57,609] Epoch: 34 | Ending LR: 0.000028 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.97it/s]\n",
      "[2024-02-12 14:45:04,972] Saving improved model after Val Loss improved from 0.39871 to 0.39693\n",
      "[2024-02-12 14:45:05,047] Epoch: 35 | FT  | Train Loss: 0.30038 | Val Loss: 0.39693\n",
      "[2024-02-12 14:45:05,048] Epoch: 35 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:45:05,048] Epoch: 35 | Ph  | Train Loss: 0.300 | Val Loss: 0.397\n",
      "[2024-02-12 14:45:05,049] Epoch: 35 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.96it/s]\n",
      "[2024-02-12 14:45:12,641] Saving improved model after Val Loss improved from 0.39693 to 0.39632\n",
      "[2024-02-12 14:45:12,720] Epoch: 36 | FT  | Train Loss: 0.29919 | Val Loss: 0.39632\n",
      "[2024-02-12 14:45:12,721] Epoch: 36 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:45:12,722] Epoch: 36 | Ph  | Train Loss: 0.299 | Val Loss: 0.396\n",
      "[2024-02-12 14:45:12,722] Epoch: 36 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.99it/s]\n",
      "[2024-02-12 14:45:20,177] Epoch: 37 | FT  | Train Loss: 0.29748 | Val Loss: 0.39856\n",
      "[2024-02-12 14:45:20,178] Epoch: 37 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:45:20,178] Epoch: 37 | Ph  | Train Loss: 0.297 | Val Loss: 0.399\n",
      "[2024-02-12 14:45:20,179] Epoch: 37 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.96it/s]\n",
      "[2024-02-12 14:45:27,587] Epoch: 38 | FT  | Train Loss: 0.29826 | Val Loss: 0.40002\n",
      "[2024-02-12 14:45:27,588] Epoch: 38 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:45:27,589] Epoch: 38 | Ph  | Train Loss: 0.298 | Val Loss: 0.400\n",
      "[2024-02-12 14:45:27,589] Epoch: 38 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.96it/s]\n",
      "[2024-02-12 14:45:35,048] Epoch: 39 | FT  | Train Loss: 0.29609 | Val Loss: 0.39786\n",
      "[2024-02-12 14:45:35,051] Epoch: 39 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:45:35,051] Epoch: 39 | Ph  | Train Loss: 0.296 | Val Loss: 0.398\n",
      "[2024-02-12 14:45:35,052] Epoch: 39 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.98it/s]\n",
      "[2024-02-12 14:45:42,269] Epoch: 40 | FT  | Train Loss: 0.29595 | Val Loss: 0.40073\n",
      "[2024-02-12 14:45:42,271] Epoch: 40 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:45:42,272] Epoch: 40 | Ph  | Train Loss: 0.296 | Val Loss: 0.401\n",
      "[2024-02-12 14:45:42,273] Epoch: 40 | Ending LR: 0.000039 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  4.00it/s]\n",
      "[2024-02-12 14:45:49,551] Epoch: 41 | FT  | Train Loss: 0.29295 | Val Loss: 0.40231\n",
      "[2024-02-12 14:45:49,552] Epoch: 41 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:45:49,552] Epoch: 41 | Ph  | Train Loss: 0.293 | Val Loss: 0.402\n",
      "[2024-02-12 14:45:49,553] Epoch: 41 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.90it/s]\n",
      "[2024-02-12 14:45:56,820] Epoch: 42 | FT  | Train Loss: 0.29116 | Val Loss: 0.39807\n",
      "[2024-02-12 14:45:56,821] Epoch: 42 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:45:56,821] Epoch: 42 | Ph  | Train Loss: 0.291 | Val Loss: 0.398\n",
      "[2024-02-12 14:45:56,822] Epoch: 42 | Ending LR: 0.000039 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  4.02it/s]\n",
      "[2024-02-12 14:46:04,186] Epoch: 43 | FT  | Train Loss: 0.28899 | Val Loss: 0.40093\n",
      "[2024-02-12 14:46:04,187] Epoch: 43 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:46:04,188] Epoch: 43 | Ph  | Train Loss: 0.289 | Val Loss: 0.401\n",
      "[2024-02-12 14:46:04,188] Epoch: 43 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.93it/s]\n",
      "[2024-02-12 14:46:11,432] Epoch: 44 | FT  | Train Loss: 0.28573 | Val Loss: 0.39704\n",
      "[2024-02-12 14:46:11,433] Epoch: 44 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:46:11,433] Epoch: 44 | Ph  | Train Loss: 0.286 | Val Loss: 0.397\n",
      "[2024-02-12 14:46:11,434] Epoch: 44 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.84it/s]\n",
      "[2024-02-12 14:46:18,924] Epoch: 45 | FT  | Train Loss: 0.28291 | Val Loss: 0.40080\n",
      "[2024-02-12 14:46:18,925] Epoch: 45 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:46:18,925] Epoch: 45 | Ph  | Train Loss: 0.283 | Val Loss: 0.401\n",
      "[2024-02-12 14:46:18,926] Epoch: 45 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.93it/s]\n",
      "[2024-02-12 14:46:26,318] Epoch: 46 | FT  | Train Loss: 0.28400 | Val Loss: 0.39702\n",
      "[2024-02-12 14:46:26,319] Epoch: 46 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:46:26,319] Epoch: 46 | Ph  | Train Loss: 0.284 | Val Loss: 0.397\n",
      "[2024-02-12 14:46:26,320] Epoch: 46 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.92it/s]\n",
      "[2024-02-12 14:46:33,625] Epoch: 47 | FT  | Train Loss: 0.28041 | Val Loss: 0.39842\n",
      "[2024-02-12 14:46:33,626] Epoch: 47 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:46:33,627] Epoch: 47 | Ph  | Train Loss: 0.280 | Val Loss: 0.398\n",
      "[2024-02-12 14:46:33,627] Epoch: 47 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.92it/s]\n",
      "[2024-02-12 14:46:41,134] Epoch: 48 | FT  | Train Loss: 0.27899 | Val Loss: 0.39836\n",
      "[2024-02-12 14:46:41,136] Epoch: 48 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:46:41,136] Epoch: 48 | Ph  | Train Loss: 0.279 | Val Loss: 0.398\n",
      "[2024-02-12 14:46:41,138] Epoch: 48 | Ending LR: 0.000022 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.92it/s]\n",
      "[2024-02-12 14:46:48,557] Epoch: 49 | FT  | Train Loss: 0.27694 | Val Loss: 0.39849\n",
      "[2024-02-12 14:46:48,558] Epoch: 49 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:46:48,559] Epoch: 49 | Ph  | Train Loss: 0.277 | Val Loss: 0.398\n",
      "[2024-02-12 14:46:48,559] Epoch: 49 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.98it/s]\n",
      "[2024-02-12 14:46:56,047] Epoch: 50 | FT  | Train Loss: 0.27891 | Val Loss: 0.39789\n",
      "[2024-02-12 14:46:56,050] Epoch: 50 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:46:56,051] Epoch: 50 | Ph  | Train Loss: 0.279 | Val Loss: 0.398\n",
      "[2024-02-12 14:46:56,052] Epoch: 50 | Ending LR: 0.000026 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.81it/s]\n",
      "[2024-02-12 14:47:03,628] Epoch: 51 | FT  | Train Loss: 0.27719 | Val Loss: 0.40102\n",
      "[2024-02-12 14:47:03,629] Epoch: 51 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:47:03,630] Epoch: 51 | Ph  | Train Loss: 0.277 | Val Loss: 0.401\n",
      "[2024-02-12 14:47:03,631] Epoch: 51 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.84it/s]\n",
      "[2024-02-12 14:47:11,313] Epoch: 52 | FT  | Train Loss: 0.27555 | Val Loss: 0.40163\n",
      "[2024-02-12 14:47:11,314] Epoch: 52 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:47:11,315] Epoch: 52 | Ph  | Train Loss: 0.276 | Val Loss: 0.402\n",
      "[2024-02-12 14:47:11,315] Epoch: 52 | Ending LR: 0.000029 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.94it/s]\n",
      "[2024-02-12 14:47:18,796] Epoch: 53 | FT  | Train Loss: 0.27364 | Val Loss: 0.39751\n",
      "[2024-02-12 14:47:18,797] Epoch: 53 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:47:18,797] Epoch: 53 | Ph  | Train Loss: 0.274 | Val Loss: 0.398\n",
      "[2024-02-12 14:47:18,798] Epoch: 53 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.81it/s]\n",
      "[2024-02-12 14:47:26,369] Epoch: 54 | FT  | Train Loss: 0.27320 | Val Loss: 0.39937\n",
      "[2024-02-12 14:47:26,370] Epoch: 54 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:47:26,370] Epoch: 54 | Ph  | Train Loss: 0.273 | Val Loss: 0.399\n",
      "[2024-02-12 14:47:26,371] Epoch: 54 | Ending LR: 0.000029 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.76it/s]\n",
      "[2024-02-12 14:47:33,976] Epoch: 55 | FT  | Train Loss: 0.27223 | Val Loss: 0.39947\n",
      "[2024-02-12 14:47:33,977] Epoch: 55 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:47:33,977] Epoch: 55 | Ph  | Train Loss: 0.272 | Val Loss: 0.399\n",
      "[2024-02-12 14:47:33,978] Epoch: 55 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.94it/s]\n",
      "[2024-02-12 14:47:41,457] Saving improved model after Val Loss improved from 0.39632 to 0.39622\n",
      "[2024-02-12 14:47:41,538] Epoch: 56 | FT  | Train Loss: 0.27092 | Val Loss: 0.39622\n",
      "[2024-02-12 14:47:41,539] Epoch: 56 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:47:41,540] Epoch: 56 | Ph  | Train Loss: 0.271 | Val Loss: 0.396\n",
      "[2024-02-12 14:47:41,541] Epoch: 56 | Ending LR: 0.000026 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.89it/s]\n",
      "[2024-02-12 14:47:49,312] Epoch: 57 | FT  | Train Loss: 0.26865 | Val Loss: 0.39742\n",
      "[2024-02-12 14:47:49,313] Epoch: 57 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:47:49,313] Epoch: 57 | Ph  | Train Loss: 0.269 | Val Loss: 0.397\n",
      "[2024-02-12 14:47:49,314] Epoch: 57 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.90it/s]\n",
      "[2024-02-12 14:47:56,812] Epoch: 58 | FT  | Train Loss: 0.26606 | Val Loss: 0.39874\n",
      "[2024-02-12 14:47:56,813] Epoch: 58 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:47:56,813] Epoch: 58 | Ph  | Train Loss: 0.266 | Val Loss: 0.399\n",
      "[2024-02-12 14:47:56,814] Epoch: 58 | Ending LR: 0.000022 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.91it/s]\n",
      "[2024-02-12 14:48:04,397] Epoch: 59 | FT  | Train Loss: 0.26646 | Val Loss: 0.39724\n",
      "[2024-02-12 14:48:04,398] Epoch: 59 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:48:04,399] Epoch: 59 | Ph  | Train Loss: 0.266 | Val Loss: 0.397\n",
      "[2024-02-12 14:48:04,399] Epoch: 59 | Ending LR: 0.000020 \n",
      "[2024-02-12 14:48:04,504] Decimating dataset to 0.02 of the original size...\n",
      "[2024-02-12 14:48:04,573] Using DataParallel with 2 devices.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.73it/s]\n",
      "[2024-02-12 14:48:10,021] Saving improved model after Val Loss improved from inf to 0.71090\n",
      "[2024-02-12 14:48:10,052] Epoch: 0 | FT  | Train Loss: 0.81762 | Val Loss: 0.71090\n",
      "[2024-02-12 14:48:10,053] Epoch: 0 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:48:10,053] Epoch: 0 | Ph  | Train Loss: 0.818 | Val Loss: 0.711\n",
      "[2024-02-12 14:48:10,054] Epoch: 0 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.92it/s]\n",
      "[2024-02-12 14:48:15,401] Saving improved model after Val Loss improved from 0.71090 to 0.69985\n",
      "[2024-02-12 14:48:15,434] Epoch: 1 | FT  | Train Loss: 0.71847 | Val Loss: 0.69985\n",
      "[2024-02-12 14:48:15,435] Epoch: 1 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:48:15,435] Epoch: 1 | Ph  | Train Loss: 0.718 | Val Loss: 0.700\n",
      "[2024-02-12 14:48:15,436] Epoch: 1 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.45it/s]\n",
      "[2024-02-12 14:48:20,910] Saving improved model after Val Loss improved from 0.69985 to 0.68419\n",
      "[2024-02-12 14:48:20,967] Epoch: 2 | FT  | Train Loss: 0.65187 | Val Loss: 0.68419\n",
      "[2024-02-12 14:48:20,969] Epoch: 2 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:48:20,970] Epoch: 2 | Ph  | Train Loss: 0.652 | Val Loss: 0.684\n",
      "[2024-02-12 14:48:20,971] Epoch: 2 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.69it/s]\n",
      "[2024-02-12 14:48:26,711] Saving improved model after Val Loss improved from 0.68419 to 0.67093\n",
      "[2024-02-12 14:48:26,786] Epoch: 3 | FT  | Train Loss: 0.60549 | Val Loss: 0.67093\n",
      "[2024-02-12 14:48:26,787] Epoch: 3 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:48:26,788] Epoch: 3 | Ph  | Train Loss: 0.605 | Val Loss: 0.671\n",
      "[2024-02-12 14:48:26,788] Epoch: 3 | Ending LR: 0.000140 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.87it/s]\n",
      "[2024-02-12 14:48:32,059] Saving improved model after Val Loss improved from 0.67093 to 0.64951\n",
      "[2024-02-12 14:48:32,143] Epoch: 4 | FT  | Train Loss: 0.58044 | Val Loss: 0.64951\n",
      "[2024-02-12 14:48:32,144] Epoch: 4 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:48:32,145] Epoch: 4 | Ph  | Train Loss: 0.580 | Val Loss: 0.650\n",
      "[2024-02-12 14:48:32,146] Epoch: 4 | Ending LR: 0.000170 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.69it/s]\n",
      "[2024-02-12 14:48:37,625] Saving improved model after Val Loss improved from 0.64951 to 0.60993\n",
      "[2024-02-12 14:48:37,710] Epoch: 5 | FT  | Train Loss: 0.56593 | Val Loss: 0.60993\n",
      "[2024-02-12 14:48:37,712] Epoch: 5 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:48:37,712] Epoch: 5 | Ph  | Train Loss: 0.566 | Val Loss: 0.610\n",
      "[2024-02-12 14:48:37,713] Epoch: 5 | Ending LR: 0.000200 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.91it/s]\n",
      "[2024-02-12 14:48:43,040] Saving improved model after Val Loss improved from 0.60993 to 0.58310\n",
      "[2024-02-12 14:48:43,124] Epoch: 6 | FT  | Train Loss: 0.55366 | Val Loss: 0.58310\n",
      "[2024-02-12 14:48:43,125] Epoch: 6 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:48:43,125] Epoch: 6 | Ph  | Train Loss: 0.554 | Val Loss: 0.583\n",
      "[2024-02-12 14:48:43,126] Epoch: 6 | Ending LR: 0.000170 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.90it/s]\n",
      "[2024-02-12 14:48:48,438] Saving improved model after Val Loss improved from 0.58310 to 0.56997\n",
      "[2024-02-12 14:48:48,541] Epoch: 7 | FT  | Train Loss: 0.54253 | Val Loss: 0.56997\n",
      "[2024-02-12 14:48:48,542] Epoch: 7 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:48:48,543] Epoch: 7 | Ph  | Train Loss: 0.543 | Val Loss: 0.570\n",
      "[2024-02-12 14:48:48,543] Epoch: 7 | Ending LR: 0.000140 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.76it/s]\n",
      "[2024-02-12 14:48:53,959] Saving improved model after Val Loss improved from 0.56997 to 0.56671\n",
      "[2024-02-12 14:48:54,049] Epoch: 8 | FT  | Train Loss: 0.53226 | Val Loss: 0.56671\n",
      "[2024-02-12 14:48:54,050] Epoch: 8 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:48:54,050] Epoch: 8 | Ph  | Train Loss: 0.532 | Val Loss: 0.567\n",
      "[2024-02-12 14:48:54,051] Epoch: 8 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.82it/s]\n",
      "[2024-02-12 14:48:59,426] Saving improved model after Val Loss improved from 0.56671 to 0.55684\n",
      "[2024-02-12 14:48:59,546] Epoch: 9 | FT  | Train Loss: 0.52198 | Val Loss: 0.55684\n",
      "[2024-02-12 14:48:59,547] Epoch: 9 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:48:59,548] Epoch: 9 | Ph  | Train Loss: 0.522 | Val Loss: 0.557\n",
      "[2024-02-12 14:48:59,548] Epoch: 9 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.94it/s]\n",
      "[2024-02-12 14:49:04,863] Saving improved model after Val Loss improved from 0.55684 to 0.55454\n",
      "[2024-02-12 14:49:04,920] Epoch: 10 | FT  | Train Loss: 0.51224 | Val Loss: 0.55454\n",
      "[2024-02-12 14:49:04,920] Epoch: 10 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:49:04,921] Epoch: 10 | Ph  | Train Loss: 0.512 | Val Loss: 0.555\n",
      "[2024-02-12 14:49:04,921] Epoch: 10 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.88it/s]\n",
      "[2024-02-12 14:49:10,293] Saving improved model after Val Loss improved from 0.55454 to 0.54765\n",
      "[2024-02-12 14:49:10,396] Epoch: 11 | FT  | Train Loss: 0.50485 | Val Loss: 0.54765\n",
      "[2024-02-12 14:49:10,397] Epoch: 11 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:49:10,398] Epoch: 11 | Ph  | Train Loss: 0.505 | Val Loss: 0.548\n",
      "[2024-02-12 14:49:10,398] Epoch: 11 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.87it/s]\n",
      "[2024-02-12 14:49:15,626] Saving improved model after Val Loss improved from 0.54765 to 0.54454\n",
      "[2024-02-12 14:49:15,694] Epoch: 12 | FT  | Train Loss: 0.50023 | Val Loss: 0.54454\n",
      "[2024-02-12 14:49:15,695] Epoch: 12 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:49:15,696] Epoch: 12 | Ph  | Train Loss: 0.500 | Val Loss: 0.545\n",
      "[2024-02-12 14:49:15,696] Epoch: 12 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.90it/s]\n",
      "[2024-02-12 14:49:21,008] Saving improved model after Val Loss improved from 0.54454 to 0.54253\n",
      "[2024-02-12 14:49:21,087] Epoch: 13 | FT  | Train Loss: 0.49656 | Val Loss: 0.54253\n",
      "[2024-02-12 14:49:21,088] Epoch: 13 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:49:21,089] Epoch: 13 | Ph  | Train Loss: 0.497 | Val Loss: 0.543\n",
      "[2024-02-12 14:49:21,089] Epoch: 13 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.85it/s]\n",
      "[2024-02-12 14:49:26,522] Saving improved model after Val Loss improved from 0.54253 to 0.54116\n",
      "[2024-02-12 14:49:26,601] Epoch: 14 | FT  | Train Loss: 0.49045 | Val Loss: 0.54116\n",
      "[2024-02-12 14:49:26,602] Epoch: 14 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:49:26,602] Epoch: 14 | Ph  | Train Loss: 0.490 | Val Loss: 0.541\n",
      "[2024-02-12 14:49:26,603] Epoch: 14 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.75it/s]\n",
      "[2024-02-12 14:49:32,357] Epoch: 15 | FT  | Train Loss: 0.48232 | Val Loss: 0.54198\n",
      "[2024-02-12 14:49:32,358] Epoch: 15 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:49:32,359] Epoch: 15 | Ph  | Train Loss: 0.482 | Val Loss: 0.542\n",
      "[2024-02-12 14:49:32,360] Epoch: 15 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.80it/s]\n",
      "[2024-02-12 14:49:37,904] Saving improved model after Val Loss improved from 0.54116 to 0.53888\n",
      "[2024-02-12 14:49:37,976] Epoch: 16 | FT  | Train Loss: 0.47534 | Val Loss: 0.53888\n",
      "[2024-02-12 14:49:37,977] Epoch: 16 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:49:37,978] Epoch: 16 | Ph  | Train Loss: 0.475 | Val Loss: 0.539\n",
      "[2024-02-12 14:49:37,978] Epoch: 16 | Ending LR: 0.000095 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.85it/s]\n",
      "[2024-02-12 14:49:43,423] Saving improved model after Val Loss improved from 0.53888 to 0.53220\n",
      "[2024-02-12 14:49:43,515] Epoch: 17 | FT  | Train Loss: 0.46400 | Val Loss: 0.53220\n",
      "[2024-02-12 14:49:43,516] Epoch: 17 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:49:43,517] Epoch: 17 | Ph  | Train Loss: 0.464 | Val Loss: 0.532\n",
      "[2024-02-12 14:49:43,518] Epoch: 17 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.68it/s]\n",
      "[2024-02-12 14:49:49,105] Epoch: 18 | FT  | Train Loss: 0.45355 | Val Loss: 0.53426\n",
      "[2024-02-12 14:49:49,106] Epoch: 18 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:49:49,106] Epoch: 18 | Ph  | Train Loss: 0.454 | Val Loss: 0.534\n",
      "[2024-02-12 14:49:49,107] Epoch: 18 | Ending LR: 0.000095 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.69it/s]\n",
      "[2024-02-12 14:49:54,519] Saving improved model after Val Loss improved from 0.53220 to 0.52874\n",
      "[2024-02-12 14:49:54,588] Epoch: 19 | FT  | Train Loss: 0.44186 | Val Loss: 0.52874\n",
      "[2024-02-12 14:49:54,590] Epoch: 19 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:49:54,591] Epoch: 19 | Ph  | Train Loss: 0.442 | Val Loss: 0.529\n",
      "[2024-02-12 14:49:54,592] Epoch: 19 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.87it/s]\n",
      "[2024-02-12 14:50:00,037] Saving improved model after Val Loss improved from 0.52874 to 0.52010\n",
      "[2024-02-12 14:50:00,068] Epoch: 20 | FT  | Train Loss: 0.42913 | Val Loss: 0.52010\n",
      "[2024-02-12 14:50:00,069] Epoch: 20 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:50:00,070] Epoch: 20 | Ph  | Train Loss: 0.429 | Val Loss: 0.520\n",
      "[2024-02-12 14:50:00,071] Epoch: 20 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.82it/s]\n",
      "[2024-02-12 14:50:05,374] Saving improved model after Val Loss improved from 0.52010 to 0.51688\n",
      "[2024-02-12 14:50:05,424] Epoch: 21 | FT  | Train Loss: 0.41968 | Val Loss: 0.51688\n",
      "[2024-02-12 14:50:05,425] Epoch: 21 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:50:05,426] Epoch: 21 | Ph  | Train Loss: 0.420 | Val Loss: 0.517\n",
      "[2024-02-12 14:50:05,426] Epoch: 21 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.89it/s]\n",
      "[2024-02-12 14:50:10,707] Saving improved model after Val Loss improved from 0.51688 to 0.51416\n",
      "[2024-02-12 14:50:10,785] Epoch: 22 | FT  | Train Loss: 0.41177 | Val Loss: 0.51416\n",
      "[2024-02-12 14:50:10,787] Epoch: 22 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:50:10,788] Epoch: 22 | Ph  | Train Loss: 0.412 | Val Loss: 0.514\n",
      "[2024-02-12 14:50:10,788] Epoch: 22 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.84it/s]\n",
      "[2024-02-12 14:50:16,086] Saving improved model after Val Loss improved from 0.51416 to 0.51156\n",
      "[2024-02-12 14:50:16,150] Epoch: 23 | FT  | Train Loss: 0.40473 | Val Loss: 0.51156\n",
      "[2024-02-12 14:50:16,151] Epoch: 23 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:50:16,152] Epoch: 23 | Ph  | Train Loss: 0.405 | Val Loss: 0.512\n",
      "[2024-02-12 14:50:16,152] Epoch: 23 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.86it/s]\n",
      "[2024-02-12 14:50:21,903] Epoch: 24 | FT  | Train Loss: 0.39884 | Val Loss: 0.51177\n",
      "[2024-02-12 14:50:21,904] Epoch: 24 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:50:21,905] Epoch: 24 | Ph  | Train Loss: 0.399 | Val Loss: 0.512\n",
      "[2024-02-12 14:50:21,905] Epoch: 24 | Ending LR: 0.000028 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.86it/s]\n",
      "[2024-02-12 14:50:27,278] Saving improved model after Val Loss improved from 0.51156 to 0.51073\n",
      "[2024-02-12 14:50:27,358] Epoch: 25 | FT  | Train Loss: 0.39563 | Val Loss: 0.51073\n",
      "[2024-02-12 14:50:27,360] Epoch: 25 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:50:27,360] Epoch: 25 | Ph  | Train Loss: 0.396 | Val Loss: 0.511\n",
      "[2024-02-12 14:50:27,361] Epoch: 25 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.86it/s]\n",
      "[2024-02-12 14:50:32,704] Epoch: 26 | FT  | Train Loss: 0.39269 | Val Loss: 0.51563\n",
      "[2024-02-12 14:50:32,705] Epoch: 26 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:50:32,705] Epoch: 26 | Ph  | Train Loss: 0.393 | Val Loss: 0.516\n",
      "[2024-02-12 14:50:32,706] Epoch: 26 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.90it/s]\n",
      "[2024-02-12 14:50:38,172] Saving improved model after Val Loss improved from 0.51073 to 0.50985\n",
      "[2024-02-12 14:50:38,231] Epoch: 27 | FT  | Train Loss: 0.38936 | Val Loss: 0.50985\n",
      "[2024-02-12 14:50:38,232] Epoch: 27 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:50:38,233] Epoch: 27 | Ph  | Train Loss: 0.389 | Val Loss: 0.510\n",
      "[2024-02-12 14:50:38,233] Epoch: 27 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.75it/s]\n",
      "[2024-02-12 14:50:43,587] Epoch: 28 | FT  | Train Loss: 0.38155 | Val Loss: 0.51629\n",
      "[2024-02-12 14:50:43,590] Epoch: 28 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:50:43,591] Epoch: 28 | Ph  | Train Loss: 0.382 | Val Loss: 0.516\n",
      "[2024-02-12 14:50:43,591] Epoch: 28 | Ending LR: 0.000057 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.85it/s]\n",
      "[2024-02-12 14:50:49,026] Epoch: 29 | FT  | Train Loss: 0.37527 | Val Loss: 0.51955\n",
      "[2024-02-12 14:50:49,028] Epoch: 29 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:50:49,028] Epoch: 29 | Ph  | Train Loss: 0.375 | Val Loss: 0.520\n",
      "[2024-02-12 14:50:49,030] Epoch: 29 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.69it/s]\n",
      "[2024-02-12 14:50:54,503] Epoch: 30 | FT  | Train Loss: 0.36741 | Val Loss: 0.51056\n",
      "[2024-02-12 14:50:54,506] Epoch: 30 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:50:54,506] Epoch: 30 | Ph  | Train Loss: 0.367 | Val Loss: 0.511\n",
      "[2024-02-12 14:50:54,508] Epoch: 30 | Ending LR: 0.000057 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.78it/s]\n",
      "[2024-02-12 14:50:59,902] Epoch: 31 | FT  | Train Loss: 0.36201 | Val Loss: 0.51119\n",
      "[2024-02-12 14:50:59,903] Epoch: 31 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:50:59,904] Epoch: 31 | Ph  | Train Loss: 0.362 | Val Loss: 0.511\n",
      "[2024-02-12 14:50:59,905] Epoch: 31 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.91it/s]\n",
      "[2024-02-12 14:51:05,350] Saving improved model after Val Loss improved from 0.50985 to 0.50867\n",
      "[2024-02-12 14:51:05,420] Epoch: 32 | FT  | Train Loss: 0.35428 | Val Loss: 0.50867\n",
      "[2024-02-12 14:51:05,421] Epoch: 32 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:51:05,421] Epoch: 32 | Ph  | Train Loss: 0.354 | Val Loss: 0.509\n",
      "[2024-02-12 14:51:05,422] Epoch: 32 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.78it/s]\n",
      "[2024-02-12 14:51:11,137] Saving improved model after Val Loss improved from 0.50867 to 0.50634\n",
      "[2024-02-12 14:51:11,168] Epoch: 33 | FT  | Train Loss: 0.34842 | Val Loss: 0.50634\n",
      "[2024-02-12 14:51:11,169] Epoch: 33 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:51:11,170] Epoch: 33 | Ph  | Train Loss: 0.348 | Val Loss: 0.506\n",
      "[2024-02-12 14:51:11,170] Epoch: 33 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.96it/s]\n",
      "[2024-02-12 14:51:16,372] Epoch: 34 | FT  | Train Loss: 0.34232 | Val Loss: 0.50891\n",
      "[2024-02-12 14:51:16,376] Epoch: 34 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:51:16,377] Epoch: 34 | Ph  | Train Loss: 0.342 | Val Loss: 0.509\n",
      "[2024-02-12 14:51:16,377] Epoch: 34 | Ending LR: 0.000028 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.84it/s]\n",
      "[2024-02-12 14:51:21,801] Epoch: 35 | FT  | Train Loss: 0.33856 | Val Loss: 0.50687\n",
      "[2024-02-12 14:51:21,803] Epoch: 35 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:51:21,804] Epoch: 35 | Ph  | Train Loss: 0.339 | Val Loss: 0.507\n",
      "[2024-02-12 14:51:21,804] Epoch: 35 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.69it/s]\n",
      "[2024-02-12 14:51:27,340] Epoch: 36 | FT  | Train Loss: 0.33497 | Val Loss: 0.51035\n",
      "[2024-02-12 14:51:27,341] Epoch: 36 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:51:27,342] Epoch: 36 | Ph  | Train Loss: 0.335 | Val Loss: 0.510\n",
      "[2024-02-12 14:51:27,343] Epoch: 36 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.93it/s]\n",
      "[2024-02-12 14:51:32,797] Epoch: 37 | FT  | Train Loss: 0.33337 | Val Loss: 0.50807\n",
      "[2024-02-12 14:51:32,798] Epoch: 37 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:51:32,798] Epoch: 37 | Ph  | Train Loss: 0.333 | Val Loss: 0.508\n",
      "[2024-02-12 14:51:32,799] Epoch: 37 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.89it/s]\n",
      "[2024-02-12 14:51:38,254] Epoch: 38 | FT  | Train Loss: 0.33082 | Val Loss: 0.50820\n",
      "[2024-02-12 14:51:38,255] Epoch: 38 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:51:38,256] Epoch: 38 | Ph  | Train Loss: 0.331 | Val Loss: 0.508\n",
      "[2024-02-12 14:51:38,257] Epoch: 38 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.95it/s]\n",
      "[2024-02-12 14:51:43,785] Epoch: 39 | FT  | Train Loss: 0.32671 | Val Loss: 0.51022\n",
      "[2024-02-12 14:51:43,786] Epoch: 39 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:51:43,787] Epoch: 39 | Ph  | Train Loss: 0.327 | Val Loss: 0.510\n",
      "[2024-02-12 14:51:43,787] Epoch: 39 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.70it/s]\n",
      "[2024-02-12 14:51:49,414] Epoch: 40 | FT  | Train Loss: 0.32542 | Val Loss: 0.51096\n",
      "[2024-02-12 14:51:49,415] Epoch: 40 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:51:49,415] Epoch: 40 | Ph  | Train Loss: 0.325 | Val Loss: 0.511\n",
      "[2024-02-12 14:51:49,416] Epoch: 40 | Ending LR: 0.000039 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.84it/s]\n",
      "[2024-02-12 14:51:54,559] Epoch: 41 | FT  | Train Loss: 0.32316 | Val Loss: 0.51415\n",
      "[2024-02-12 14:51:54,560] Epoch: 41 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:51:54,561] Epoch: 41 | Ph  | Train Loss: 0.323 | Val Loss: 0.514\n",
      "[2024-02-12 14:51:54,561] Epoch: 41 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.81it/s]\n",
      "[2024-02-12 14:52:00,071] Epoch: 42 | FT  | Train Loss: 0.32008 | Val Loss: 0.51376\n",
      "[2024-02-12 14:52:00,072] Epoch: 42 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:52:00,073] Epoch: 42 | Ph  | Train Loss: 0.320 | Val Loss: 0.514\n",
      "[2024-02-12 14:52:00,073] Epoch: 42 | Ending LR: 0.000039 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.88it/s]\n",
      "[2024-02-12 14:52:05,446] Epoch: 43 | FT  | Train Loss: 0.31546 | Val Loss: 0.51546\n",
      "[2024-02-12 14:52:05,448] Epoch: 43 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:52:05,448] Epoch: 43 | Ph  | Train Loss: 0.315 | Val Loss: 0.515\n",
      "[2024-02-12 14:52:05,449] Epoch: 43 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.80it/s]\n",
      "[2024-02-12 14:52:10,979] Epoch: 44 | FT  | Train Loss: 0.31401 | Val Loss: 0.51019\n",
      "[2024-02-12 14:52:10,980] Epoch: 44 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:52:10,981] Epoch: 44 | Ph  | Train Loss: 0.314 | Val Loss: 0.510\n",
      "[2024-02-12 14:52:10,981] Epoch: 44 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.76it/s]\n",
      "[2024-02-12 14:52:16,533] Epoch: 45 | FT  | Train Loss: 0.31017 | Val Loss: 0.51022\n",
      "[2024-02-12 14:52:16,534] Epoch: 45 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:52:16,535] Epoch: 45 | Ph  | Train Loss: 0.310 | Val Loss: 0.510\n",
      "[2024-02-12 14:52:16,535] Epoch: 45 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.78it/s]\n",
      "[2024-02-12 14:52:22,029] Epoch: 46 | FT  | Train Loss: 0.30651 | Val Loss: 0.50931\n",
      "[2024-02-12 14:52:22,031] Epoch: 46 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:52:22,031] Epoch: 46 | Ph  | Train Loss: 0.307 | Val Loss: 0.509\n",
      "[2024-02-12 14:52:22,032] Epoch: 46 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.77it/s]\n",
      "[2024-02-12 14:52:27,659] Epoch: 47 | FT  | Train Loss: 0.30314 | Val Loss: 0.50981\n",
      "[2024-02-12 14:52:27,660] Epoch: 47 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:52:27,660] Epoch: 47 | Ph  | Train Loss: 0.303 | Val Loss: 0.510\n",
      "[2024-02-12 14:52:27,661] Epoch: 47 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.81it/s]\n",
      "[2024-02-12 14:52:33,162] Epoch: 48 | FT  | Train Loss: 0.30086 | Val Loss: 0.51263\n",
      "[2024-02-12 14:52:33,163] Epoch: 48 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:52:33,164] Epoch: 48 | Ph  | Train Loss: 0.301 | Val Loss: 0.513\n",
      "[2024-02-12 14:52:33,164] Epoch: 48 | Ending LR: 0.000022 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.83it/s]\n",
      "[2024-02-12 14:52:38,649] Epoch: 49 | FT  | Train Loss: 0.29875 | Val Loss: 0.51230\n",
      "[2024-02-12 14:52:38,650] Epoch: 49 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:52:38,651] Epoch: 49 | Ph  | Train Loss: 0.299 | Val Loss: 0.512\n",
      "[2024-02-12 14:52:38,651] Epoch: 49 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.90it/s]\n",
      "[2024-02-12 14:52:43,938] Epoch: 50 | FT  | Train Loss: 0.29628 | Val Loss: 0.51448\n",
      "[2024-02-12 14:52:43,944] Epoch: 50 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:52:43,944] Epoch: 50 | Ph  | Train Loss: 0.296 | Val Loss: 0.514\n",
      "[2024-02-12 14:52:43,945] Epoch: 50 | Ending LR: 0.000026 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.98it/s]\n",
      "[2024-02-12 14:52:49,318] Epoch: 51 | FT  | Train Loss: 0.29627 | Val Loss: 0.51372\n",
      "[2024-02-12 14:52:49,320] Epoch: 51 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:52:49,321] Epoch: 51 | Ph  | Train Loss: 0.296 | Val Loss: 0.514\n",
      "[2024-02-12 14:52:49,321] Epoch: 51 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.01it/s]\n",
      "[2024-02-12 14:52:54,521] Epoch: 52 | FT  | Train Loss: 0.29447 | Val Loss: 0.51133\n",
      "[2024-02-12 14:52:54,522] Epoch: 52 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:52:54,522] Epoch: 52 | Ph  | Train Loss: 0.294 | Val Loss: 0.511\n",
      "[2024-02-12 14:52:54,523] Epoch: 52 | Ending LR: 0.000029 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.83it/s]\n",
      "[2024-02-12 14:52:59,851] Epoch: 53 | FT  | Train Loss: 0.29396 | Val Loss: 0.51311\n",
      "[2024-02-12 14:52:59,852] Epoch: 53 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:52:59,852] Epoch: 53 | Ph  | Train Loss: 0.294 | Val Loss: 0.513\n",
      "[2024-02-12 14:52:59,853] Epoch: 53 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.96it/s]\n",
      "[2024-02-12 14:53:05,100] Epoch: 54 | FT  | Train Loss: 0.29148 | Val Loss: 0.51409\n",
      "[2024-02-12 14:53:05,101] Epoch: 54 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:53:05,101] Epoch: 54 | Ph  | Train Loss: 0.291 | Val Loss: 0.514\n",
      "[2024-02-12 14:53:05,102] Epoch: 54 | Ending LR: 0.000029 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.97it/s]\n",
      "[2024-02-12 14:53:10,549] Epoch: 55 | FT  | Train Loss: 0.29080 | Val Loss: 0.51285\n",
      "[2024-02-12 14:53:10,550] Epoch: 55 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:53:10,551] Epoch: 55 | Ph  | Train Loss: 0.291 | Val Loss: 0.513\n",
      "[2024-02-12 14:53:10,551] Epoch: 55 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.69it/s]\n",
      "[2024-02-12 14:53:15,886] Epoch: 56 | FT  | Train Loss: 0.28737 | Val Loss: 0.51360\n",
      "[2024-02-12 14:53:15,887] Epoch: 56 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:53:15,888] Epoch: 56 | Ph  | Train Loss: 0.287 | Val Loss: 0.514\n",
      "[2024-02-12 14:53:15,889] Epoch: 56 | Ending LR: 0.000026 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.95it/s]\n",
      "[2024-02-12 14:53:21,105] Epoch: 57 | FT  | Train Loss: 0.28437 | Val Loss: 0.51566\n",
      "[2024-02-12 14:53:21,106] Epoch: 57 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:53:21,107] Epoch: 57 | Ph  | Train Loss: 0.284 | Val Loss: 0.516\n",
      "[2024-02-12 14:53:21,107] Epoch: 57 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.84it/s]\n",
      "[2024-02-12 14:53:26,417] Epoch: 58 | FT  | Train Loss: 0.28211 | Val Loss: 0.51756\n",
      "[2024-02-12 14:53:26,418] Epoch: 58 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:53:26,419] Epoch: 58 | Ph  | Train Loss: 0.282 | Val Loss: 0.518\n",
      "[2024-02-12 14:53:26,420] Epoch: 58 | Ending LR: 0.000022 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.96it/s]\n",
      "[2024-02-12 14:53:31,624] Epoch: 59 | FT  | Train Loss: 0.28026 | Val Loss: 0.51784\n",
      "[2024-02-12 14:53:31,624] Epoch: 59 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:53:31,625] Epoch: 59 | Ph  | Train Loss: 0.280 | Val Loss: 0.518\n",
      "[2024-02-12 14:53:31,626] Epoch: 59 | Ending LR: 0.000020 \n",
      "[2024-02-12 14:53:31,717] Decimating dataset to 0.01 of the original size...\n",
      "[2024-02-12 14:53:31,798] Using DataParallel with 2 devices.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.28it/s]\n",
      "[2024-02-12 14:53:36,513] Saving improved model after Val Loss improved from inf to 0.71257\n",
      "[2024-02-12 14:53:36,543] Epoch: 0 | FT  | Train Loss: 0.84707 | Val Loss: 0.71257\n",
      "[2024-02-12 14:53:36,543] Epoch: 0 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:53:36,544] Epoch: 0 | Ph  | Train Loss: 0.847 | Val Loss: 0.713\n",
      "[2024-02-12 14:53:36,544] Epoch: 0 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.39it/s]\n",
      "[2024-02-12 14:53:41,239] Saving improved model after Val Loss improved from 0.71257 to 0.70906\n",
      "[2024-02-12 14:53:41,317] Epoch: 1 | FT  | Train Loss: 0.75359 | Val Loss: 0.70906\n",
      "[2024-02-12 14:53:41,318] Epoch: 1 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:53:41,318] Epoch: 1 | Ph  | Train Loss: 0.754 | Val Loss: 0.709\n",
      "[2024-02-12 14:53:41,319] Epoch: 1 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.39it/s]\n",
      "[2024-02-12 14:53:46,021] Saving improved model after Val Loss improved from 0.70906 to 0.70127\n",
      "[2024-02-12 14:53:46,103] Epoch: 2 | FT  | Train Loss: 0.69553 | Val Loss: 0.70127\n",
      "[2024-02-12 14:53:46,104] Epoch: 2 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:53:46,105] Epoch: 2 | Ph  | Train Loss: 0.696 | Val Loss: 0.701\n",
      "[2024-02-12 14:53:46,105] Epoch: 2 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.16it/s]\n",
      "[2024-02-12 14:53:50,889] Saving improved model after Val Loss improved from 0.70127 to 0.69031\n",
      "[2024-02-12 14:53:50,986] Epoch: 3 | FT  | Train Loss: 0.65150 | Val Loss: 0.69031\n",
      "[2024-02-12 14:53:50,987] Epoch: 3 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:53:50,987] Epoch: 3 | Ph  | Train Loss: 0.652 | Val Loss: 0.690\n",
      "[2024-02-12 14:53:50,988] Epoch: 3 | Ending LR: 0.000140 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.35it/s]\n",
      "[2024-02-12 14:53:55,819] Saving improved model after Val Loss improved from 0.69031 to 0.68010\n",
      "[2024-02-12 14:53:55,890] Epoch: 4 | FT  | Train Loss: 0.60968 | Val Loss: 0.68010\n",
      "[2024-02-12 14:53:55,891] Epoch: 4 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:53:55,891] Epoch: 4 | Ph  | Train Loss: 0.610 | Val Loss: 0.680\n",
      "[2024-02-12 14:53:55,892] Epoch: 4 | Ending LR: 0.000170 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.18it/s]\n",
      "[2024-02-12 14:54:00,769] Saving improved model after Val Loss improved from 0.68010 to 0.67437\n",
      "[2024-02-12 14:54:00,839] Epoch: 5 | FT  | Train Loss: 0.58101 | Val Loss: 0.67437\n",
      "[2024-02-12 14:54:00,840] Epoch: 5 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:54:00,841] Epoch: 5 | Ph  | Train Loss: 0.581 | Val Loss: 0.674\n",
      "[2024-02-12 14:54:00,841] Epoch: 5 | Ending LR: 0.000200 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.33it/s]\n",
      "[2024-02-12 14:54:05,514] Saving improved model after Val Loss improved from 0.67437 to 0.66977\n",
      "[2024-02-12 14:54:05,593] Epoch: 6 | FT  | Train Loss: 0.57315 | Val Loss: 0.66977\n",
      "[2024-02-12 14:54:05,594] Epoch: 6 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:54:05,595] Epoch: 6 | Ph  | Train Loss: 0.573 | Val Loss: 0.670\n",
      "[2024-02-12 14:54:05,595] Epoch: 6 | Ending LR: 0.000170 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.42it/s]\n",
      "[2024-02-12 14:54:10,241] Saving improved model after Val Loss improved from 0.66977 to 0.65200\n",
      "[2024-02-12 14:54:10,317] Epoch: 7 | FT  | Train Loss: 0.56374 | Val Loss: 0.65200\n",
      "[2024-02-12 14:54:10,318] Epoch: 7 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:54:10,319] Epoch: 7 | Ph  | Train Loss: 0.564 | Val Loss: 0.652\n",
      "[2024-02-12 14:54:10,320] Epoch: 7 | Ending LR: 0.000140 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.32it/s]\n",
      "[2024-02-12 14:54:14,977] Saving improved model after Val Loss improved from 0.65200 to 0.62901\n",
      "[2024-02-12 14:54:15,061] Epoch: 8 | FT  | Train Loss: 0.55144 | Val Loss: 0.62901\n",
      "[2024-02-12 14:54:15,062] Epoch: 8 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:54:15,062] Epoch: 8 | Ph  | Train Loss: 0.551 | Val Loss: 0.629\n",
      "[2024-02-12 14:54:15,063] Epoch: 8 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.38it/s]\n",
      "[2024-02-12 14:54:20,121] Saving improved model after Val Loss improved from 0.62901 to 0.60883\n",
      "[2024-02-12 14:54:20,191] Epoch: 9 | FT  | Train Loss: 0.54594 | Val Loss: 0.60883\n",
      "[2024-02-12 14:54:20,192] Epoch: 9 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:54:20,193] Epoch: 9 | Ph  | Train Loss: 0.546 | Val Loss: 0.609\n",
      "[2024-02-12 14:54:20,193] Epoch: 9 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.19it/s]\n",
      "[2024-02-12 14:54:24,931] Saving improved model after Val Loss improved from 0.60883 to 0.59618\n",
      "[2024-02-12 14:54:25,009] Epoch: 10 | FT  | Train Loss: 0.53566 | Val Loss: 0.59618\n",
      "[2024-02-12 14:54:25,010] Epoch: 10 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:54:25,010] Epoch: 10 | Ph  | Train Loss: 0.536 | Val Loss: 0.596\n",
      "[2024-02-12 14:54:25,011] Epoch: 10 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.17it/s]\n",
      "[2024-02-12 14:54:29,799] Saving improved model after Val Loss improved from 0.59618 to 0.58682\n",
      "[2024-02-12 14:54:29,876] Epoch: 11 | FT  | Train Loss: 0.52561 | Val Loss: 0.58682\n",
      "[2024-02-12 14:54:29,877] Epoch: 11 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:54:29,878] Epoch: 11 | Ph  | Train Loss: 0.526 | Val Loss: 0.587\n",
      "[2024-02-12 14:54:29,879] Epoch: 11 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.36it/s]\n",
      "[2024-02-12 14:54:34,545] Saving improved model after Val Loss improved from 0.58682 to 0.57925\n",
      "[2024-02-12 14:54:34,626] Epoch: 12 | FT  | Train Loss: 0.52110 | Val Loss: 0.57925\n",
      "[2024-02-12 14:54:34,627] Epoch: 12 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:54:34,627] Epoch: 12 | Ph  | Train Loss: 0.521 | Val Loss: 0.579\n",
      "[2024-02-12 14:54:34,628] Epoch: 12 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.36it/s]\n",
      "[2024-02-12 14:54:39,397] Saving improved model after Val Loss improved from 0.57925 to 0.57351\n",
      "[2024-02-12 14:54:39,467] Epoch: 13 | FT  | Train Loss: 0.52078 | Val Loss: 0.57351\n",
      "[2024-02-12 14:54:39,468] Epoch: 13 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:54:39,468] Epoch: 13 | Ph  | Train Loss: 0.521 | Val Loss: 0.574\n",
      "[2024-02-12 14:54:39,469] Epoch: 13 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.42it/s]\n",
      "[2024-02-12 14:54:44,123] Saving improved model after Val Loss improved from 0.57351 to 0.56939\n",
      "[2024-02-12 14:54:44,196] Epoch: 14 | FT  | Train Loss: 0.51292 | Val Loss: 0.56939\n",
      "[2024-02-12 14:54:44,197] Epoch: 14 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:54:44,197] Epoch: 14 | Ph  | Train Loss: 0.513 | Val Loss: 0.569\n",
      "[2024-02-12 14:54:44,198] Epoch: 14 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.15it/s]\n",
      "[2024-02-12 14:54:48,914] Saving improved model after Val Loss improved from 0.56939 to 0.56672\n",
      "[2024-02-12 14:54:49,004] Epoch: 15 | FT  | Train Loss: 0.51099 | Val Loss: 0.56672\n",
      "[2024-02-12 14:54:49,005] Epoch: 15 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:54:49,006] Epoch: 15 | Ph  | Train Loss: 0.511 | Val Loss: 0.567\n",
      "[2024-02-12 14:54:49,006] Epoch: 15 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.40it/s]\n",
      "[2024-02-12 14:54:53,620] Saving improved model after Val Loss improved from 0.56672 to 0.56544\n",
      "[2024-02-12 14:54:53,697] Epoch: 16 | FT  | Train Loss: 0.50041 | Val Loss: 0.56544\n",
      "[2024-02-12 14:54:53,699] Epoch: 16 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:54:53,700] Epoch: 16 | Ph  | Train Loss: 0.500 | Val Loss: 0.565\n",
      "[2024-02-12 14:54:53,700] Epoch: 16 | Ending LR: 0.000095 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.42it/s]\n",
      "[2024-02-12 14:54:58,538] Saving improved model after Val Loss improved from 0.56544 to 0.56445\n",
      "[2024-02-12 14:54:58,626] Epoch: 17 | FT  | Train Loss: 0.49176 | Val Loss: 0.56445\n",
      "[2024-02-12 14:54:58,626] Epoch: 17 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:54:58,627] Epoch: 17 | Ph  | Train Loss: 0.492 | Val Loss: 0.564\n",
      "[2024-02-12 14:54:58,628] Epoch: 17 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.05it/s]\n",
      "[2024-02-12 14:55:03,792] Epoch: 18 | FT  | Train Loss: 0.47913 | Val Loss: 0.56608\n",
      "[2024-02-12 14:55:03,794] Epoch: 18 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:55:03,795] Epoch: 18 | Ph  | Train Loss: 0.479 | Val Loss: 0.566\n",
      "[2024-02-12 14:55:03,796] Epoch: 18 | Ending LR: 0.000095 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.21it/s]\n",
      "[2024-02-12 14:55:08,567] Saving improved model after Val Loss improved from 0.56445 to 0.55926\n",
      "[2024-02-12 14:55:08,650] Epoch: 19 | FT  | Train Loss: 0.47396 | Val Loss: 0.55926\n",
      "[2024-02-12 14:55:08,651] Epoch: 19 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:55:08,652] Epoch: 19 | Ph  | Train Loss: 0.474 | Val Loss: 0.559\n",
      "[2024-02-12 14:55:08,652] Epoch: 19 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.33it/s]\n",
      "[2024-02-12 14:55:13,325] Epoch: 20 | FT  | Train Loss: 0.46413 | Val Loss: 0.55946\n",
      "[2024-02-12 14:55:13,326] Epoch: 20 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:55:13,326] Epoch: 20 | Ph  | Train Loss: 0.464 | Val Loss: 0.559\n",
      "[2024-02-12 14:55:13,327] Epoch: 20 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.41it/s]\n",
      "[2024-02-12 14:55:17,974] Saving improved model after Val Loss improved from 0.55926 to 0.55525\n",
      "[2024-02-12 14:55:18,050] Epoch: 21 | FT  | Train Loss: 0.45453 | Val Loss: 0.55525\n",
      "[2024-02-12 14:55:18,052] Epoch: 21 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:55:18,053] Epoch: 21 | Ph  | Train Loss: 0.455 | Val Loss: 0.555\n",
      "[2024-02-12 14:55:18,053] Epoch: 21 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.16it/s]\n",
      "[2024-02-12 14:55:22,841] Epoch: 22 | FT  | Train Loss: 0.45544 | Val Loss: 0.55710\n",
      "[2024-02-12 14:55:22,842] Epoch: 22 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:55:22,843] Epoch: 22 | Ph  | Train Loss: 0.455 | Val Loss: 0.557\n",
      "[2024-02-12 14:55:22,845] Epoch: 22 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.36it/s]\n",
      "[2024-02-12 14:55:28,461] Saving improved model after Val Loss improved from 0.55525 to 0.55339\n",
      "[2024-02-12 14:55:28,543] Epoch: 23 | FT  | Train Loss: 0.44543 | Val Loss: 0.55339\n",
      "[2024-02-12 14:55:28,544] Epoch: 23 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:55:28,544] Epoch: 23 | Ph  | Train Loss: 0.445 | Val Loss: 0.553\n",
      "[2024-02-12 14:55:28,545] Epoch: 23 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.20it/s]\n",
      "[2024-02-12 14:55:33,161] Saving improved model after Val Loss improved from 0.55339 to 0.55233\n",
      "[2024-02-12 14:55:33,263] Epoch: 24 | FT  | Train Loss: 0.44011 | Val Loss: 0.55233\n",
      "[2024-02-12 14:55:33,264] Epoch: 24 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:55:33,264] Epoch: 24 | Ph  | Train Loss: 0.440 | Val Loss: 0.552\n",
      "[2024-02-12 14:55:33,265] Epoch: 24 | Ending LR: 0.000028 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.27it/s]\n",
      "[2024-02-12 14:55:37,899] Epoch: 25 | FT  | Train Loss: 0.43530 | Val Loss: 0.55256\n",
      "[2024-02-12 14:55:37,900] Epoch: 25 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:55:37,901] Epoch: 25 | Ph  | Train Loss: 0.435 | Val Loss: 0.553\n",
      "[2024-02-12 14:55:37,901] Epoch: 25 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.44it/s]\n",
      "[2024-02-12 14:55:42,838] Epoch: 26 | FT  | Train Loss: 0.43690 | Val Loss: 0.55440\n",
      "[2024-02-12 14:55:42,839] Epoch: 26 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:55:42,840] Epoch: 26 | Ph  | Train Loss: 0.437 | Val Loss: 0.554\n",
      "[2024-02-12 14:55:42,840] Epoch: 26 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.23it/s]\n",
      "[2024-02-12 14:55:47,628] Epoch: 27 | FT  | Train Loss: 0.42782 | Val Loss: 0.55369\n",
      "[2024-02-12 14:55:47,629] Epoch: 27 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:55:47,630] Epoch: 27 | Ph  | Train Loss: 0.428 | Val Loss: 0.554\n",
      "[2024-02-12 14:55:47,631] Epoch: 27 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.18it/s]\n",
      "[2024-02-12 14:55:52,468] Epoch: 28 | FT  | Train Loss: 0.42217 | Val Loss: 0.55424\n",
      "[2024-02-12 14:55:52,469] Epoch: 28 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:55:52,469] Epoch: 28 | Ph  | Train Loss: 0.422 | Val Loss: 0.554\n",
      "[2024-02-12 14:55:52,470] Epoch: 28 | Ending LR: 0.000057 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.31it/s]\n",
      "[2024-02-12 14:55:57,201] Epoch: 29 | FT  | Train Loss: 0.41393 | Val Loss: 0.55607\n",
      "[2024-02-12 14:55:57,202] Epoch: 29 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:55:57,203] Epoch: 29 | Ph  | Train Loss: 0.414 | Val Loss: 0.556\n",
      "[2024-02-12 14:55:57,204] Epoch: 29 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.45it/s]\n",
      "[2024-02-12 14:56:01,757] Epoch: 30 | FT  | Train Loss: 0.40900 | Val Loss: 0.55523\n",
      "[2024-02-12 14:56:01,758] Epoch: 30 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:56:01,759] Epoch: 30 | Ph  | Train Loss: 0.409 | Val Loss: 0.555\n",
      "[2024-02-12 14:56:01,759] Epoch: 30 | Ending LR: 0.000057 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.36it/s]\n",
      "[2024-02-12 14:56:06,581] Epoch: 31 | FT  | Train Loss: 0.39600 | Val Loss: 0.55477\n",
      "[2024-02-12 14:56:06,582] Epoch: 31 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:56:06,582] Epoch: 31 | Ph  | Train Loss: 0.396 | Val Loss: 0.555\n",
      "[2024-02-12 14:56:06,583] Epoch: 31 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.35it/s]\n",
      "[2024-02-12 14:56:11,182] Epoch: 32 | FT  | Train Loss: 0.39398 | Val Loss: 0.55583\n",
      "[2024-02-12 14:56:11,185] Epoch: 32 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:56:11,186] Epoch: 32 | Ph  | Train Loss: 0.394 | Val Loss: 0.556\n",
      "[2024-02-12 14:56:11,186] Epoch: 32 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.15it/s]\n",
      "[2024-02-12 14:56:15,871] Epoch: 33 | FT  | Train Loss: 0.38707 | Val Loss: 0.55684\n",
      "[2024-02-12 14:56:15,873] Epoch: 33 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:56:15,873] Epoch: 33 | Ph  | Train Loss: 0.387 | Val Loss: 0.557\n",
      "[2024-02-12 14:56:15,874] Epoch: 33 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.17it/s]\n",
      "[2024-02-12 14:56:20,433] Epoch: 34 | FT  | Train Loss: 0.38566 | Val Loss: 0.55419\n",
      "[2024-02-12 14:56:20,434] Epoch: 34 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:56:20,435] Epoch: 34 | Ph  | Train Loss: 0.386 | Val Loss: 0.554\n",
      "[2024-02-12 14:56:20,436] Epoch: 34 | Ending LR: 0.000028 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.37it/s]\n",
      "[2024-02-12 14:56:24,834] Epoch: 35 | FT  | Train Loss: 0.37533 | Val Loss: 0.55470\n",
      "[2024-02-12 14:56:24,835] Epoch: 35 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:56:24,836] Epoch: 35 | Ph  | Train Loss: 0.375 | Val Loss: 0.555\n",
      "[2024-02-12 14:56:24,836] Epoch: 35 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.30it/s]\n",
      "[2024-02-12 14:56:29,463] Epoch: 36 | FT  | Train Loss: 0.37246 | Val Loss: 0.55512\n",
      "[2024-02-12 14:56:29,466] Epoch: 36 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:56:29,467] Epoch: 36 | Ph  | Train Loss: 0.372 | Val Loss: 0.555\n",
      "[2024-02-12 14:56:29,467] Epoch: 36 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.08it/s]\n",
      "[2024-02-12 14:56:34,098] Epoch: 37 | FT  | Train Loss: 0.37142 | Val Loss: 0.55399\n",
      "[2024-02-12 14:56:34,099] Epoch: 37 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:56:34,100] Epoch: 37 | Ph  | Train Loss: 0.371 | Val Loss: 0.554\n",
      "[2024-02-12 14:56:34,100] Epoch: 37 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.18it/s]\n",
      "[2024-02-12 14:56:38,893] Epoch: 38 | FT  | Train Loss: 0.36473 | Val Loss: 0.55868\n",
      "[2024-02-12 14:56:38,894] Epoch: 38 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:56:38,895] Epoch: 38 | Ph  | Train Loss: 0.365 | Val Loss: 0.559\n",
      "[2024-02-12 14:56:38,895] Epoch: 38 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.26it/s]\n",
      "[2024-02-12 14:56:43,552] Epoch: 39 | FT  | Train Loss: 0.35718 | Val Loss: 0.55525\n",
      "[2024-02-12 14:56:43,553] Epoch: 39 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:56:43,554] Epoch: 39 | Ph  | Train Loss: 0.357 | Val Loss: 0.555\n",
      "[2024-02-12 14:56:43,554] Epoch: 39 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.13it/s]\n",
      "[2024-02-12 14:56:48,203] Epoch: 40 | FT  | Train Loss: 0.35803 | Val Loss: 0.56552\n",
      "[2024-02-12 14:56:48,204] Epoch: 40 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:56:48,205] Epoch: 40 | Ph  | Train Loss: 0.358 | Val Loss: 0.566\n",
      "[2024-02-12 14:56:48,205] Epoch: 40 | Ending LR: 0.000039 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.41it/s]\n",
      "[2024-02-12 14:56:52,747] Epoch: 41 | FT  | Train Loss: 0.35821 | Val Loss: 0.55705\n",
      "[2024-02-12 14:56:52,748] Epoch: 41 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:56:52,748] Epoch: 41 | Ph  | Train Loss: 0.358 | Val Loss: 0.557\n",
      "[2024-02-12 14:56:52,749] Epoch: 41 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.37it/s]\n",
      "[2024-02-12 14:56:57,390] Epoch: 42 | FT  | Train Loss: 0.35416 | Val Loss: 0.56198\n",
      "[2024-02-12 14:56:57,391] Epoch: 42 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:56:57,392] Epoch: 42 | Ph  | Train Loss: 0.354 | Val Loss: 0.562\n",
      "[2024-02-12 14:56:57,392] Epoch: 42 | Ending LR: 0.000039 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.36it/s]\n",
      "[2024-02-12 14:57:01,967] Epoch: 43 | FT  | Train Loss: 0.34888 | Val Loss: 0.55716\n",
      "[2024-02-12 14:57:01,968] Epoch: 43 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:57:01,968] Epoch: 43 | Ph  | Train Loss: 0.349 | Val Loss: 0.557\n",
      "[2024-02-12 14:57:01,969] Epoch: 43 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.43it/s]\n",
      "[2024-02-12 14:57:06,648] Epoch: 44 | FT  | Train Loss: 0.34318 | Val Loss: 0.55917\n",
      "[2024-02-12 14:57:06,649] Epoch: 44 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:57:06,650] Epoch: 44 | Ph  | Train Loss: 0.343 | Val Loss: 0.559\n",
      "[2024-02-12 14:57:06,650] Epoch: 44 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.43it/s]\n",
      "[2024-02-12 14:57:11,175] Epoch: 45 | FT  | Train Loss: 0.33506 | Val Loss: 0.55845\n",
      "[2024-02-12 14:57:11,176] Epoch: 45 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:57:11,177] Epoch: 45 | Ph  | Train Loss: 0.335 | Val Loss: 0.558\n",
      "[2024-02-12 14:57:11,177] Epoch: 45 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.36it/s]\n",
      "[2024-02-12 14:57:15,738] Epoch: 46 | FT  | Train Loss: 0.33445 | Val Loss: 0.55965\n",
      "[2024-02-12 14:57:15,739] Epoch: 46 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:57:15,740] Epoch: 46 | Ph  | Train Loss: 0.334 | Val Loss: 0.560\n",
      "[2024-02-12 14:57:15,740] Epoch: 46 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.40it/s]\n",
      "[2024-02-12 14:57:20,293] Epoch: 47 | FT  | Train Loss: 0.32742 | Val Loss: 0.56131\n",
      "[2024-02-12 14:57:20,293] Epoch: 47 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:57:20,294] Epoch: 47 | Ph  | Train Loss: 0.327 | Val Loss: 0.561\n",
      "[2024-02-12 14:57:20,294] Epoch: 47 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.44it/s]\n",
      "[2024-02-12 14:57:25,130] Epoch: 48 | FT  | Train Loss: 0.33155 | Val Loss: 0.56148\n",
      "[2024-02-12 14:57:25,131] Epoch: 48 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:57:25,132] Epoch: 48 | Ph  | Train Loss: 0.332 | Val Loss: 0.561\n",
      "[2024-02-12 14:57:25,132] Epoch: 48 | Ending LR: 0.000022 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.48it/s]\n",
      "[2024-02-12 14:57:29,720] Epoch: 49 | FT  | Train Loss: 0.32789 | Val Loss: 0.56146\n",
      "[2024-02-12 14:57:29,721] Epoch: 49 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:57:29,723] Epoch: 49 | Ph  | Train Loss: 0.328 | Val Loss: 0.561\n",
      "[2024-02-12 14:57:29,725] Epoch: 49 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.31it/s]\n",
      "[2024-02-12 14:57:34,534] Epoch: 50 | FT  | Train Loss: 0.32147 | Val Loss: 0.55899\n",
      "[2024-02-12 14:57:34,535] Epoch: 50 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:57:34,536] Epoch: 50 | Ph  | Train Loss: 0.321 | Val Loss: 0.559\n",
      "[2024-02-12 14:57:34,536] Epoch: 50 | Ending LR: 0.000026 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.08it/s]\n",
      "[2024-02-12 14:57:39,244] Epoch: 51 | FT  | Train Loss: 0.32399 | Val Loss: 0.55923\n",
      "[2024-02-12 14:57:39,246] Epoch: 51 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:57:39,247] Epoch: 51 | Ph  | Train Loss: 0.324 | Val Loss: 0.559\n",
      "[2024-02-12 14:57:39,247] Epoch: 51 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.40it/s]\n",
      "[2024-02-12 14:57:43,808] Epoch: 52 | FT  | Train Loss: 0.31920 | Val Loss: 0.55940\n",
      "[2024-02-12 14:57:43,809] Epoch: 52 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:57:43,810] Epoch: 52 | Ph  | Train Loss: 0.319 | Val Loss: 0.559\n",
      "[2024-02-12 14:57:43,810] Epoch: 52 | Ending LR: 0.000029 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.43it/s]\n",
      "[2024-02-12 14:57:48,358] Epoch: 53 | FT  | Train Loss: 0.31761 | Val Loss: 0.56396\n",
      "[2024-02-12 14:57:48,359] Epoch: 53 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:57:48,359] Epoch: 53 | Ph  | Train Loss: 0.318 | Val Loss: 0.564\n",
      "[2024-02-12 14:57:48,360] Epoch: 53 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.49it/s]\n",
      "[2024-02-12 14:57:52,872] Epoch: 54 | FT  | Train Loss: 0.31856 | Val Loss: 0.56476\n",
      "[2024-02-12 14:57:52,873] Epoch: 54 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:57:52,874] Epoch: 54 | Ph  | Train Loss: 0.319 | Val Loss: 0.565\n",
      "[2024-02-12 14:57:52,874] Epoch: 54 | Ending LR: 0.000029 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.32it/s]\n",
      "[2024-02-12 14:57:57,398] Epoch: 55 | FT  | Train Loss: 0.31399 | Val Loss: 0.56457\n",
      "[2024-02-12 14:57:57,398] Epoch: 55 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:57:57,399] Epoch: 55 | Ph  | Train Loss: 0.314 | Val Loss: 0.565\n",
      "[2024-02-12 14:57:57,400] Epoch: 55 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.63it/s]\n",
      "[2024-02-12 14:58:01,692] Epoch: 56 | FT  | Train Loss: 0.31725 | Val Loss: 0.56787\n",
      "[2024-02-12 14:58:01,692] Epoch: 56 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:58:01,693] Epoch: 56 | Ph  | Train Loss: 0.317 | Val Loss: 0.568\n",
      "[2024-02-12 14:58:01,693] Epoch: 56 | Ending LR: 0.000026 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.49it/s]\n",
      "[2024-02-12 14:58:06,184] Epoch: 57 | FT  | Train Loss: 0.31254 | Val Loss: 0.56568\n",
      "[2024-02-12 14:58:06,185] Epoch: 57 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:58:06,186] Epoch: 57 | Ph  | Train Loss: 0.313 | Val Loss: 0.566\n",
      "[2024-02-12 14:58:06,186] Epoch: 57 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.29it/s]\n",
      "[2024-02-12 14:58:10,512] Epoch: 58 | FT  | Train Loss: 0.31068 | Val Loss: 0.56607\n",
      "[2024-02-12 14:58:10,512] Epoch: 58 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:58:10,513] Epoch: 58 | Ph  | Train Loss: 0.311 | Val Loss: 0.566\n",
      "[2024-02-12 14:58:10,513] Epoch: 58 | Ending LR: 0.000022 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.60it/s]\n",
      "[2024-02-12 14:58:15,226] Epoch: 59 | FT  | Train Loss: 0.30822 | Val Loss: 0.56346\n",
      "[2024-02-12 14:58:15,227] Epoch: 59 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:58:15,228] Epoch: 59 | Ph  | Train Loss: 0.308 | Val Loss: 0.563\n",
      "[2024-02-12 14:58:15,229] Epoch: 59 | Ending LR: 0.000020 \n",
      "[2024-02-12 14:58:15,324] Decimating dataset to 0.9 of the original size...\n",
      "[2024-02-12 14:58:15,390] Using DataParallel with 2 devices.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.78it/s]\n",
      "[2024-02-12 14:59:17,582] Saving improved model after Val Loss improved from inf to 0.45374\n",
      "[2024-02-12 14:59:17,609] Epoch: 0 | FT  | Train Loss: 0.55753 | Val Loss: 0.45374\n",
      "[2024-02-12 14:59:17,610] Epoch: 0 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 14:59:17,611] Epoch: 0 | Ph  | Train Loss: 0.558 | Val Loss: 0.454\n",
      "[2024-02-12 14:59:17,611] Epoch: 0 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.84it/s]\n",
      "[2024-02-12 15:00:18,667] Saving improved model after Val Loss improved from 0.45374 to 0.35915\n",
      "[2024-02-12 15:00:18,756] Epoch: 1 | FT  | Train Loss: 0.39280 | Val Loss: 0.35915\n",
      "[2024-02-12 15:00:18,757] Epoch: 1 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:00:18,757] Epoch: 1 | Ph  | Train Loss: 0.393 | Val Loss: 0.359\n",
      "[2024-02-12 15:00:18,758] Epoch: 1 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.84it/s]\n",
      "[2024-02-12 15:01:19,717] Saving improved model after Val Loss improved from 0.35915 to 0.31566\n",
      "[2024-02-12 15:01:19,798] Epoch: 2 | FT  | Train Loss: 0.33448 | Val Loss: 0.31566\n",
      "[2024-02-12 15:01:19,799] Epoch: 2 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:01:19,800] Epoch: 2 | Ph  | Train Loss: 0.334 | Val Loss: 0.316\n",
      "[2024-02-12 15:01:19,800] Epoch: 2 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.83it/s]\n",
      "[2024-02-12 15:02:21,281] Saving improved model after Val Loss improved from 0.31566 to 0.30393\n",
      "[2024-02-12 15:02:21,399] Epoch: 3 | FT  | Train Loss: 0.30155 | Val Loss: 0.30393\n",
      "[2024-02-12 15:02:21,400] Epoch: 3 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:02:21,400] Epoch: 3 | Ph  | Train Loss: 0.302 | Val Loss: 0.304\n",
      "[2024-02-12 15:02:21,401] Epoch: 3 | Ending LR: 0.000140 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:56<00:00,  3.84it/s]\n",
      "[2024-02-12 15:03:22,453] Saving improved model after Val Loss improved from 0.30393 to 0.27885\n",
      "[2024-02-12 15:03:22,542] Epoch: 4 | FT  | Train Loss: 0.27980 | Val Loss: 0.27885\n",
      "[2024-02-12 15:03:22,543] Epoch: 4 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:03:22,544] Epoch: 4 | Ph  | Train Loss: 0.280 | Val Loss: 0.279\n",
      "[2024-02-12 15:03:22,544] Epoch: 4 | Ending LR: 0.000170 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.82it/s]\n",
      "[2024-02-12 15:04:24,072] Saving improved model after Val Loss improved from 0.27885 to 0.26418\n",
      "[2024-02-12 15:04:24,156] Epoch: 5 | FT  | Train Loss: 0.26527 | Val Loss: 0.26418\n",
      "[2024-02-12 15:04:24,157] Epoch: 5 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:04:24,158] Epoch: 5 | Ph  | Train Loss: 0.265 | Val Loss: 0.264\n",
      "[2024-02-12 15:04:24,158] Epoch: 5 | Ending LR: 0.000200 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.79it/s]\n",
      "[2024-02-12 15:05:25,981] Saving improved model after Val Loss improved from 0.26418 to 0.25073\n",
      "[2024-02-12 15:05:26,073] Epoch: 6 | FT  | Train Loss: 0.25376 | Val Loss: 0.25073\n",
      "[2024-02-12 15:05:26,073] Epoch: 6 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:05:26,074] Epoch: 6 | Ph  | Train Loss: 0.254 | Val Loss: 0.251\n",
      "[2024-02-12 15:05:26,075] Epoch: 6 | Ending LR: 0.000170 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.81it/s]\n",
      "[2024-02-12 15:06:27,763] Saving improved model after Val Loss improved from 0.25073 to 0.24429\n",
      "[2024-02-12 15:06:27,855] Epoch: 7 | FT  | Train Loss: 0.24334 | Val Loss: 0.24429\n",
      "[2024-02-12 15:06:27,856] Epoch: 7 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:06:27,857] Epoch: 7 | Ph  | Train Loss: 0.243 | Val Loss: 0.244\n",
      "[2024-02-12 15:06:27,857] Epoch: 7 | Ending LR: 0.000140 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:59<00:00,  3.70it/s]\n",
      "[2024-02-12 15:07:31,236] Saving improved model after Val Loss improved from 0.24429 to 0.23969\n",
      "[2024-02-12 15:07:31,322] Epoch: 8 | FT  | Train Loss: 0.23558 | Val Loss: 0.23969\n",
      "[2024-02-12 15:07:31,323] Epoch: 8 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:07:31,323] Epoch: 8 | Ph  | Train Loss: 0.236 | Val Loss: 0.240\n",
      "[2024-02-12 15:07:31,324] Epoch: 8 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:59<00:00,  3.69it/s]\n",
      "[2024-02-12 15:08:34,969] Saving improved model after Val Loss improved from 0.23969 to 0.23540\n",
      "[2024-02-12 15:08:35,063] Epoch: 9 | FT  | Train Loss: 0.23030 | Val Loss: 0.23540\n",
      "[2024-02-12 15:08:35,064] Epoch: 9 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:08:35,065] Epoch: 9 | Ph  | Train Loss: 0.230 | Val Loss: 0.235\n",
      "[2024-02-12 15:08:35,066] Epoch: 9 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.73it/s]\n",
      "[2024-02-12 15:09:37,892] Saving improved model after Val Loss improved from 0.23540 to 0.23167\n",
      "[2024-02-12 15:09:37,985] Epoch: 10 | FT  | Train Loss: 0.22524 | Val Loss: 0.23167\n",
      "[2024-02-12 15:09:37,986] Epoch: 10 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:09:37,987] Epoch: 10 | Ph  | Train Loss: 0.225 | Val Loss: 0.232\n",
      "[2024-02-12 15:09:37,988] Epoch: 10 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.80it/s]\n",
      "[2024-02-12 15:10:39,605] Saving improved model after Val Loss improved from 0.23167 to 0.22600\n",
      "[2024-02-12 15:10:39,719] Epoch: 11 | FT  | Train Loss: 0.22157 | Val Loss: 0.22600\n",
      "[2024-02-12 15:10:39,720] Epoch: 11 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:10:39,721] Epoch: 11 | Ph  | Train Loss: 0.222 | Val Loss: 0.226\n",
      "[2024-02-12 15:10:39,721] Epoch: 11 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.84it/s]\n",
      "[2024-02-12 15:11:40,942] Saving improved model after Val Loss improved from 0.22600 to 0.22597\n",
      "[2024-02-12 15:11:41,034] Epoch: 12 | FT  | Train Loss: 0.21962 | Val Loss: 0.22597\n",
      "[2024-02-12 15:11:41,035] Epoch: 12 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:11:41,035] Epoch: 12 | Ph  | Train Loss: 0.220 | Val Loss: 0.226\n",
      "[2024-02-12 15:11:41,036] Epoch: 12 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.79it/s]\n",
      "[2024-02-12 15:12:42,752] Epoch: 13 | FT  | Train Loss: 0.21980 | Val Loss: 0.22612\n",
      "[2024-02-12 15:12:42,753] Epoch: 13 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:12:42,754] Epoch: 13 | Ph  | Train Loss: 0.220 | Val Loss: 0.226\n",
      "[2024-02-12 15:12:42,755] Epoch: 13 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:56<00:00,  3.88it/s]\n",
      "[2024-02-12 15:13:43,288] Epoch: 14 | FT  | Train Loss: 0.21912 | Val Loss: 0.23258\n",
      "[2024-02-12 15:13:43,290] Epoch: 14 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:13:43,290] Epoch: 14 | Ph  | Train Loss: 0.219 | Val Loss: 0.233\n",
      "[2024-02-12 15:13:43,291] Epoch: 14 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.78it/s]\n",
      "[2024-02-12 15:14:45,635] Epoch: 15 | FT  | Train Loss: 0.21904 | Val Loss: 0.23016\n",
      "[2024-02-12 15:14:45,637] Epoch: 15 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:14:45,638] Epoch: 15 | Ph  | Train Loss: 0.219 | Val Loss: 0.230\n",
      "[2024-02-12 15:14:45,638] Epoch: 15 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.83it/s]\n",
      "[2024-02-12 15:15:46,665] Epoch: 16 | FT  | Train Loss: 0.21781 | Val Loss: 0.23706\n",
      "[2024-02-12 15:15:46,668] Epoch: 16 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:15:46,668] Epoch: 16 | Ph  | Train Loss: 0.218 | Val Loss: 0.237\n",
      "[2024-02-12 15:15:46,669] Epoch: 16 | Ending LR: 0.000095 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.84it/s]\n",
      "[2024-02-12 15:16:47,618] Epoch: 17 | FT  | Train Loss: 0.21611 | Val Loss: 0.22787\n",
      "[2024-02-12 15:16:47,619] Epoch: 17 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:16:47,621] Epoch: 17 | Ph  | Train Loss: 0.216 | Val Loss: 0.228\n",
      "[2024-02-12 15:16:47,622] Epoch: 17 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.82it/s]\n",
      "[2024-02-12 15:17:49,378] Epoch: 18 | FT  | Train Loss: 0.21378 | Val Loss: 0.22663\n",
      "[2024-02-12 15:17:49,378] Epoch: 18 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:17:49,379] Epoch: 18 | Ph  | Train Loss: 0.214 | Val Loss: 0.227\n",
      "[2024-02-12 15:17:49,380] Epoch: 18 | Ending LR: 0.000095 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.72it/s]\n",
      "[2024-02-12 15:18:52,079] Saving improved model after Val Loss improved from 0.22597 to 0.22320\n",
      "[2024-02-12 15:18:52,171] Epoch: 19 | FT  | Train Loss: 0.21060 | Val Loss: 0.22320\n",
      "[2024-02-12 15:18:52,172] Epoch: 19 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:18:52,172] Epoch: 19 | Ph  | Train Loss: 0.211 | Val Loss: 0.223\n",
      "[2024-02-12 15:18:52,173] Epoch: 19 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.79it/s]\n",
      "[2024-02-12 15:19:54,179] Saving improved model after Val Loss improved from 0.22320 to 0.22037\n",
      "[2024-02-12 15:19:54,277] Epoch: 20 | FT  | Train Loss: 0.20741 | Val Loss: 0.22037\n",
      "[2024-02-12 15:19:54,278] Epoch: 20 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:19:54,279] Epoch: 20 | Ph  | Train Loss: 0.207 | Val Loss: 0.220\n",
      "[2024-02-12 15:19:54,280] Epoch: 20 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.80it/s]\n",
      "[2024-02-12 15:20:55,855] Saving improved model after Val Loss improved from 0.22037 to 0.21905\n",
      "[2024-02-12 15:20:55,949] Epoch: 21 | FT  | Train Loss: 0.20417 | Val Loss: 0.21905\n",
      "[2024-02-12 15:20:55,949] Epoch: 21 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:20:55,950] Epoch: 21 | Ph  | Train Loss: 0.204 | Val Loss: 0.219\n",
      "[2024-02-12 15:20:55,950] Epoch: 21 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.79it/s]\n",
      "[2024-02-12 15:21:58,143] Saving improved model after Val Loss improved from 0.21905 to 0.21150\n",
      "[2024-02-12 15:21:58,241] Epoch: 22 | FT  | Train Loss: 0.20174 | Val Loss: 0.21150\n",
      "[2024-02-12 15:21:58,242] Epoch: 22 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:21:58,243] Epoch: 22 | Ph  | Train Loss: 0.202 | Val Loss: 0.211\n",
      "[2024-02-12 15:21:58,243] Epoch: 22 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.80it/s]\n",
      "[2024-02-12 15:23:00,154] Epoch: 23 | FT  | Train Loss: 0.19928 | Val Loss: 0.21184\n",
      "[2024-02-12 15:23:00,155] Epoch: 23 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:23:00,156] Epoch: 23 | Ph  | Train Loss: 0.199 | Val Loss: 0.212\n",
      "[2024-02-12 15:23:00,156] Epoch: 23 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.79it/s]\n",
      "[2024-02-12 15:24:01,862] Epoch: 24 | FT  | Train Loss: 0.19811 | Val Loss: 0.21494\n",
      "[2024-02-12 15:24:01,863] Epoch: 24 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:24:01,864] Epoch: 24 | Ph  | Train Loss: 0.198 | Val Loss: 0.215\n",
      "[2024-02-12 15:24:01,864] Epoch: 24 | Ending LR: 0.000028 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.80it/s]\n",
      "[2024-02-12 15:25:03,539] Saving improved model after Val Loss improved from 0.21150 to 0.21109\n",
      "[2024-02-12 15:25:03,642] Epoch: 25 | FT  | Train Loss: 0.19855 | Val Loss: 0.21109\n",
      "[2024-02-12 15:25:03,643] Epoch: 25 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:25:03,644] Epoch: 25 | Ph  | Train Loss: 0.199 | Val Loss: 0.211\n",
      "[2024-02-12 15:25:03,644] Epoch: 25 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.79it/s]\n",
      "[2024-02-12 15:26:05,694] Epoch: 26 | FT  | Train Loss: 0.19844 | Val Loss: 0.22035\n",
      "[2024-02-12 15:26:05,695] Epoch: 26 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:26:05,696] Epoch: 26 | Ph  | Train Loss: 0.198 | Val Loss: 0.220\n",
      "[2024-02-12 15:26:05,696] Epoch: 26 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.83it/s]\n",
      "[2024-02-12 15:27:07,147] Epoch: 27 | FT  | Train Loss: 0.19864 | Val Loss: 0.21786\n",
      "[2024-02-12 15:27:07,149] Epoch: 27 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:27:07,149] Epoch: 27 | Ph  | Train Loss: 0.199 | Val Loss: 0.218\n",
      "[2024-02-12 15:27:07,150] Epoch: 27 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.76it/s]\n",
      "[2024-02-12 15:28:09,653] Epoch: 28 | FT  | Train Loss: 0.19828 | Val Loss: 0.21379\n",
      "[2024-02-12 15:28:09,654] Epoch: 28 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:28:09,654] Epoch: 28 | Ph  | Train Loss: 0.198 | Val Loss: 0.214\n",
      "[2024-02-12 15:28:09,655] Epoch: 28 | Ending LR: 0.000057 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.78it/s]\n",
      "[2024-02-12 15:29:11,589] Epoch: 29 | FT  | Train Loss: 0.19780 | Val Loss: 0.21546\n",
      "[2024-02-12 15:29:11,590] Epoch: 29 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:29:11,591] Epoch: 29 | Ph  | Train Loss: 0.198 | Val Loss: 0.215\n",
      "[2024-02-12 15:29:11,591] Epoch: 29 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.77it/s]\n",
      "[2024-02-12 15:30:13,645] Epoch: 30 | FT  | Train Loss: 0.19690 | Val Loss: 0.21874\n",
      "[2024-02-12 15:30:13,647] Epoch: 30 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:30:13,648] Epoch: 30 | Ph  | Train Loss: 0.197 | Val Loss: 0.219\n",
      "[2024-02-12 15:30:13,649] Epoch: 30 | Ending LR: 0.000057 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.83it/s]\n",
      "[2024-02-12 15:31:15,029] Saving improved model after Val Loss improved from 0.21109 to 0.21070\n",
      "[2024-02-12 15:31:15,127] Epoch: 31 | FT  | Train Loss: 0.19530 | Val Loss: 0.21070\n",
      "[2024-02-12 15:31:15,128] Epoch: 31 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:31:15,129] Epoch: 31 | Ph  | Train Loss: 0.195 | Val Loss: 0.211\n",
      "[2024-02-12 15:31:15,130] Epoch: 31 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:59<00:00,  3.70it/s]\n",
      "[2024-02-12 15:32:18,232] Saving improved model after Val Loss improved from 0.21070 to 0.20689\n",
      "[2024-02-12 15:32:18,334] Epoch: 32 | FT  | Train Loss: 0.19348 | Val Loss: 0.20689\n",
      "[2024-02-12 15:32:18,335] Epoch: 32 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:32:18,336] Epoch: 32 | Ph  | Train Loss: 0.193 | Val Loss: 0.207\n",
      "[2024-02-12 15:32:18,336] Epoch: 32 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.74it/s]\n",
      "[2024-02-12 15:33:20,889] Epoch: 33 | FT  | Train Loss: 0.19131 | Val Loss: 0.21028\n",
      "[2024-02-12 15:33:20,891] Epoch: 33 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:33:20,891] Epoch: 33 | Ph  | Train Loss: 0.191 | Val Loss: 0.210\n",
      "[2024-02-12 15:33:20,892] Epoch: 33 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.83it/s]\n",
      "[2024-02-12 15:34:22,080] Saving improved model after Val Loss improved from 0.20689 to 0.20612\n",
      "[2024-02-12 15:34:22,203] Epoch: 34 | FT  | Train Loss: 0.18991 | Val Loss: 0.20612\n",
      "[2024-02-12 15:34:22,205] Epoch: 34 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:34:22,206] Epoch: 34 | Ph  | Train Loss: 0.190 | Val Loss: 0.206\n",
      "[2024-02-12 15:34:22,207] Epoch: 34 | Ending LR: 0.000028 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.79it/s]\n",
      "[2024-02-12 15:35:23,905] Epoch: 35 | FT  | Train Loss: 0.18825 | Val Loss: 0.20677\n",
      "[2024-02-12 15:35:23,906] Epoch: 35 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:35:23,907] Epoch: 35 | Ph  | Train Loss: 0.188 | Val Loss: 0.207\n",
      "[2024-02-12 15:35:23,908] Epoch: 35 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.80it/s]\n",
      "[2024-02-12 15:36:25,411] Saving improved model after Val Loss improved from 0.20612 to 0.20482\n",
      "[2024-02-12 15:36:25,510] Epoch: 36 | FT  | Train Loss: 0.18764 | Val Loss: 0.20482\n",
      "[2024-02-12 15:36:25,512] Epoch: 36 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:36:25,512] Epoch: 36 | Ph  | Train Loss: 0.188 | Val Loss: 0.205\n",
      "[2024-02-12 15:36:25,513] Epoch: 36 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.81it/s]\n",
      "[2024-02-12 15:37:27,489] Epoch: 37 | FT  | Train Loss: 0.18766 | Val Loss: 0.20635\n",
      "[2024-02-12 15:37:27,490] Epoch: 37 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:37:27,491] Epoch: 37 | Ph  | Train Loss: 0.188 | Val Loss: 0.206\n",
      "[2024-02-12 15:37:27,491] Epoch: 37 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.73it/s]\n",
      "[2024-02-12 15:38:30,225] Epoch: 38 | FT  | Train Loss: 0.18755 | Val Loss: 0.20568\n",
      "[2024-02-12 15:38:30,227] Epoch: 38 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:38:30,228] Epoch: 38 | Ph  | Train Loss: 0.188 | Val Loss: 0.206\n",
      "[2024-02-12 15:38:30,229] Epoch: 38 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.79it/s]\n",
      "[2024-02-12 15:39:32,144] Epoch: 39 | FT  | Train Loss: 0.18756 | Val Loss: 0.20506\n",
      "[2024-02-12 15:39:32,145] Epoch: 39 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:39:32,146] Epoch: 39 | Ph  | Train Loss: 0.188 | Val Loss: 0.205\n",
      "[2024-02-12 15:39:32,146] Epoch: 39 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.75it/s]\n",
      "[2024-02-12 15:40:34,887] Saving improved model after Val Loss improved from 0.20482 to 0.20451\n",
      "[2024-02-12 15:40:35,005] Epoch: 40 | FT  | Train Loss: 0.18736 | Val Loss: 0.20451\n",
      "[2024-02-12 15:40:35,006] Epoch: 40 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:40:35,006] Epoch: 40 | Ph  | Train Loss: 0.187 | Val Loss: 0.205\n",
      "[2024-02-12 15:40:35,007] Epoch: 40 | Ending LR: 0.000039 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.83it/s]\n",
      "[2024-02-12 15:41:36,423] Epoch: 41 | FT  | Train Loss: 0.18737 | Val Loss: 0.20642\n",
      "[2024-02-12 15:41:36,426] Epoch: 41 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:41:36,427] Epoch: 41 | Ph  | Train Loss: 0.187 | Val Loss: 0.206\n",
      "[2024-02-12 15:41:36,427] Epoch: 41 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.75it/s]\n",
      "[2024-02-12 15:42:39,031] Epoch: 42 | FT  | Train Loss: 0.18656 | Val Loss: 0.20575\n",
      "[2024-02-12 15:42:39,032] Epoch: 42 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:42:39,033] Epoch: 42 | Ph  | Train Loss: 0.187 | Val Loss: 0.206\n",
      "[2024-02-12 15:42:39,033] Epoch: 42 | Ending LR: 0.000039 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.83it/s]\n",
      "[2024-02-12 15:43:40,379] Epoch: 43 | FT  | Train Loss: 0.18531 | Val Loss: 0.20669\n",
      "[2024-02-12 15:43:40,380] Epoch: 43 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:43:40,380] Epoch: 43 | Ph  | Train Loss: 0.185 | Val Loss: 0.207\n",
      "[2024-02-12 15:43:40,381] Epoch: 43 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.83it/s]\n",
      "[2024-02-12 15:44:41,644] Saving improved model after Val Loss improved from 0.20451 to 0.20383\n",
      "[2024-02-12 15:44:41,745] Epoch: 44 | FT  | Train Loss: 0.18403 | Val Loss: 0.20383\n",
      "[2024-02-12 15:44:41,746] Epoch: 44 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:44:41,747] Epoch: 44 | Ph  | Train Loss: 0.184 | Val Loss: 0.204\n",
      "[2024-02-12 15:44:41,747] Epoch: 44 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.77it/s]\n",
      "[2024-02-12 15:45:44,001] Saving improved model after Val Loss improved from 0.20383 to 0.20277\n",
      "[2024-02-12 15:45:44,110] Epoch: 45 | FT  | Train Loss: 0.18316 | Val Loss: 0.20277\n",
      "[2024-02-12 15:45:44,111] Epoch: 45 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:45:44,112] Epoch: 45 | Ph  | Train Loss: 0.183 | Val Loss: 0.203\n",
      "[2024-02-12 15:45:44,112] Epoch: 45 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.82it/s]\n",
      "[2024-02-12 15:46:45,496] Saving improved model after Val Loss improved from 0.20277 to 0.20156\n",
      "[2024-02-12 15:46:45,606] Epoch: 46 | FT  | Train Loss: 0.18209 | Val Loss: 0.20156\n",
      "[2024-02-12 15:46:45,607] Epoch: 46 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:46:45,608] Epoch: 46 | Ph  | Train Loss: 0.182 | Val Loss: 0.202\n",
      "[2024-02-12 15:46:45,609] Epoch: 46 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.73it/s]\n",
      "[2024-02-12 15:47:48,659] Saving improved model after Val Loss improved from 0.20156 to 0.19947\n",
      "[2024-02-12 15:47:48,761] Epoch: 47 | FT  | Train Loss: 0.18126 | Val Loss: 0.19947\n",
      "[2024-02-12 15:47:48,762] Epoch: 47 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:47:48,763] Epoch: 47 | Ph  | Train Loss: 0.181 | Val Loss: 0.199\n",
      "[2024-02-12 15:47:48,764] Epoch: 47 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.81it/s]\n",
      "[2024-02-12 15:48:50,373] Epoch: 48 | FT  | Train Loss: 0.18083 | Val Loss: 0.20028\n",
      "[2024-02-12 15:48:50,374] Epoch: 48 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:48:50,375] Epoch: 48 | Ph  | Train Loss: 0.181 | Val Loss: 0.200\n",
      "[2024-02-12 15:48:50,376] Epoch: 48 | Ending LR: 0.000022 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.77it/s]\n",
      "[2024-02-12 15:49:52,418] Epoch: 49 | FT  | Train Loss: 0.18081 | Val Loss: 0.20157\n",
      "[2024-02-12 15:49:52,419] Epoch: 49 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:49:52,420] Epoch: 49 | Ph  | Train Loss: 0.181 | Val Loss: 0.202\n",
      "[2024-02-12 15:49:52,420] Epoch: 49 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.77it/s]\n",
      "[2024-02-12 15:50:54,612] Epoch: 50 | FT  | Train Loss: 0.18088 | Val Loss: 0.20188\n",
      "[2024-02-12 15:50:54,614] Epoch: 50 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:50:54,615] Epoch: 50 | Ph  | Train Loss: 0.181 | Val Loss: 0.202\n",
      "[2024-02-12 15:50:54,617] Epoch: 50 | Ending LR: 0.000026 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.82it/s]\n",
      "[2024-02-12 15:51:55,939] Epoch: 51 | FT  | Train Loss: 0.18049 | Val Loss: 0.20161\n",
      "[2024-02-12 15:51:55,940] Epoch: 51 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:51:55,941] Epoch: 51 | Ph  | Train Loss: 0.180 | Val Loss: 0.202\n",
      "[2024-02-12 15:51:55,941] Epoch: 51 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.79it/s]\n",
      "[2024-02-12 15:52:57,791] Epoch: 52 | FT  | Train Loss: 0.18039 | Val Loss: 0.20109\n",
      "[2024-02-12 15:52:57,793] Epoch: 52 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:52:57,794] Epoch: 52 | Ph  | Train Loss: 0.180 | Val Loss: 0.201\n",
      "[2024-02-12 15:52:57,794] Epoch: 52 | Ending LR: 0.000029 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.80it/s]\n",
      "[2024-02-12 15:53:59,727] Epoch: 53 | FT  | Train Loss: 0.18017 | Val Loss: 0.20330\n",
      "[2024-02-12 15:53:59,728] Epoch: 53 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:53:59,729] Epoch: 53 | Ph  | Train Loss: 0.180 | Val Loss: 0.203\n",
      "[2024-02-12 15:53:59,729] Epoch: 53 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.72it/s]\n",
      "[2024-02-12 15:55:02,785] Epoch: 54 | FT  | Train Loss: 0.17976 | Val Loss: 0.20857\n",
      "[2024-02-12 15:55:02,786] Epoch: 54 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:55:02,787] Epoch: 54 | Ph  | Train Loss: 0.180 | Val Loss: 0.209\n",
      "[2024-02-12 15:55:02,787] Epoch: 54 | Ending LR: 0.000029 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.77it/s]\n",
      "[2024-02-12 15:56:05,141] Epoch: 55 | FT  | Train Loss: 0.17909 | Val Loss: 0.20161\n",
      "[2024-02-12 15:56:05,142] Epoch: 55 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:56:05,143] Epoch: 55 | Ph  | Train Loss: 0.179 | Val Loss: 0.202\n",
      "[2024-02-12 15:56:05,144] Epoch: 55 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.78it/s]\n",
      "[2024-02-12 15:57:07,483] Epoch: 56 | FT  | Train Loss: 0.17865 | Val Loss: 0.20024\n",
      "[2024-02-12 15:57:07,485] Epoch: 56 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:57:07,485] Epoch: 56 | Ph  | Train Loss: 0.179 | Val Loss: 0.200\n",
      "[2024-02-12 15:57:07,486] Epoch: 56 | Ending LR: 0.000026 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.83it/s]\n",
      "[2024-02-12 15:58:08,802] Epoch: 57 | FT  | Train Loss: 0.17804 | Val Loss: 0.20031\n",
      "[2024-02-12 15:58:08,804] Epoch: 57 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:58:08,804] Epoch: 57 | Ph  | Train Loss: 0.178 | Val Loss: 0.200\n",
      "[2024-02-12 15:58:08,805] Epoch: 57 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:58<00:00,  3.76it/s]\n",
      "[2024-02-12 15:59:11,243] Epoch: 58 | FT  | Train Loss: 0.17760 | Val Loss: 0.20027\n",
      "[2024-02-12 15:59:11,244] Epoch: 58 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 15:59:11,245] Epoch: 58 | Ph  | Train Loss: 0.178 | Val Loss: 0.200\n",
      "[2024-02-12 15:59:11,245] Epoch: 58 | Ending LR: 0.000022 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 219/219 [00:57<00:00,  3.78it/s]\n",
      "[2024-02-12 16:00:13,170] Epoch: 59 | FT  | Train Loss: 0.17672 | Val Loss: 0.19978\n",
      "[2024-02-12 16:00:13,172] Epoch: 59 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:00:13,172] Epoch: 59 | Ph  | Train Loss: 0.177 | Val Loss: 0.200\n",
      "[2024-02-12 16:00:13,173] Epoch: 59 | Ending LR: 0.000020 \n",
      "[2024-02-12 16:00:13,297] Decimating dataset to 0.8 of the original size...\n",
      "[2024-02-12 16:00:13,371] Using DataParallel with 2 devices.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [00:51<00:00,  3.81it/s]\n",
      "[2024-02-12 16:01:08,561] Saving improved model after Val Loss improved from inf to 0.48018\n",
      "[2024-02-12 16:01:08,593] Epoch: 0 | FT  | Train Loss: 0.57657 | Val Loss: 0.48018\n",
      "[2024-02-12 16:01:08,594] Epoch: 0 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:01:08,595] Epoch: 0 | Ph  | Train Loss: 0.577 | Val Loss: 0.480\n",
      "[2024-02-12 16:01:08,595] Epoch: 0 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [00:52<00:00,  3.68it/s]\n",
      "[2024-02-12 16:02:05,663] Saving improved model after Val Loss improved from 0.48018 to 0.38104\n",
      "[2024-02-12 16:02:05,737] Epoch: 1 | FT  | Train Loss: 0.41458 | Val Loss: 0.38104\n",
      "[2024-02-12 16:02:05,738] Epoch: 1 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:02:05,739] Epoch: 1 | Ph  | Train Loss: 0.415 | Val Loss: 0.381\n",
      "[2024-02-12 16:02:05,739] Epoch: 1 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [00:53<00:00,  3.68it/s]\n",
      "[2024-02-12 16:03:02,813] Saving improved model after Val Loss improved from 0.38104 to 0.32395\n",
      "[2024-02-12 16:03:02,934] Epoch: 2 | FT  | Train Loss: 0.34236 | Val Loss: 0.32395\n",
      "[2024-02-12 16:03:02,935] Epoch: 2 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:03:02,936] Epoch: 2 | Ph  | Train Loss: 0.342 | Val Loss: 0.324\n",
      "[2024-02-12 16:03:02,936] Epoch: 2 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [00:53<00:00,  3.63it/s]\n",
      "[2024-02-12 16:04:01,088] Saving improved model after Val Loss improved from 0.32395 to 0.29880\n",
      "[2024-02-12 16:04:01,210] Epoch: 3 | FT  | Train Loss: 0.30638 | Val Loss: 0.29880\n",
      "[2024-02-12 16:04:01,211] Epoch: 3 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:04:01,213] Epoch: 3 | Ph  | Train Loss: 0.306 | Val Loss: 0.299\n",
      "[2024-02-12 16:04:01,214] Epoch: 3 | Ending LR: 0.000140 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:05<00:00,  3.00it/s]\n",
      "[2024-02-12 16:05:11,711] Saving improved model after Val Loss improved from 0.29880 to 0.29213\n",
      "[2024-02-12 16:05:11,808] Epoch: 4 | FT  | Train Loss: 0.28537 | Val Loss: 0.29213\n",
      "[2024-02-12 16:05:11,810] Epoch: 4 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:05:11,811] Epoch: 4 | Ph  | Train Loss: 0.285 | Val Loss: 0.292\n",
      "[2024-02-12 16:05:11,819] Epoch: 4 | Ending LR: 0.000170 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:14<00:00,  2.61it/s]\n",
      "[2024-02-12 16:06:31,473] Saving improved model after Val Loss improved from 0.29213 to 0.27619\n",
      "[2024-02-12 16:06:31,561] Epoch: 5 | FT  | Train Loss: 0.26966 | Val Loss: 0.27619\n",
      "[2024-02-12 16:06:31,561] Epoch: 5 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:06:31,562] Epoch: 5 | Ph  | Train Loss: 0.270 | Val Loss: 0.276\n",
      "[2024-02-12 16:06:31,562] Epoch: 5 | Ending LR: 0.000200 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:15<00:00,  2.58it/s]\n",
      "[2024-02-12 16:07:52,058] Saving improved model after Val Loss improved from 0.27619 to 0.25976\n",
      "[2024-02-12 16:07:52,138] Epoch: 6 | FT  | Train Loss: 0.25654 | Val Loss: 0.25976\n",
      "[2024-02-12 16:07:52,141] Epoch: 6 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:07:52,144] Epoch: 6 | Ph  | Train Loss: 0.257 | Val Loss: 0.260\n",
      "[2024-02-12 16:07:52,146] Epoch: 6 | Ending LR: 0.000170 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:13<00:00,  2.65it/s]\n",
      "[2024-02-12 16:09:11,123] Saving improved model after Val Loss improved from 0.25976 to 0.24915\n",
      "[2024-02-12 16:09:11,209] Epoch: 7 | FT  | Train Loss: 0.24589 | Val Loss: 0.24915\n",
      "[2024-02-12 16:09:11,211] Epoch: 7 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:09:11,211] Epoch: 7 | Ph  | Train Loss: 0.246 | Val Loss: 0.249\n",
      "[2024-02-12 16:09:11,212] Epoch: 7 | Ending LR: 0.000140 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:17<00:00,  2.50it/s]\n",
      "[2024-02-12 16:10:34,225] Saving improved model after Val Loss improved from 0.24915 to 0.23998\n",
      "[2024-02-12 16:10:34,343] Epoch: 8 | FT  | Train Loss: 0.23769 | Val Loss: 0.23998\n",
      "[2024-02-12 16:10:34,344] Epoch: 8 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:10:34,345] Epoch: 8 | Ph  | Train Loss: 0.238 | Val Loss: 0.240\n",
      "[2024-02-12 16:10:34,345] Epoch: 8 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:13<00:00,  2.66it/s]\n",
      "[2024-02-12 16:11:52,792] Saving improved model after Val Loss improved from 0.23998 to 0.23492\n",
      "[2024-02-12 16:11:52,897] Epoch: 9 | FT  | Train Loss: 0.23181 | Val Loss: 0.23492\n",
      "[2024-02-12 16:11:52,898] Epoch: 9 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:11:52,898] Epoch: 9 | Ph  | Train Loss: 0.232 | Val Loss: 0.235\n",
      "[2024-02-12 16:11:52,899] Epoch: 9 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:14<00:00,  2.60it/s]\n",
      "[2024-02-12 16:13:12,978] Saving improved model after Val Loss improved from 0.23492 to 0.23158\n",
      "[2024-02-12 16:13:13,130] Epoch: 10 | FT  | Train Loss: 0.22633 | Val Loss: 0.23158\n",
      "[2024-02-12 16:13:13,131] Epoch: 10 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:13:13,132] Epoch: 10 | Ph  | Train Loss: 0.226 | Val Loss: 0.232\n",
      "[2024-02-12 16:13:13,133] Epoch: 10 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:14<00:00,  2.61it/s]\n",
      "[2024-02-12 16:14:33,166] Saving improved model after Val Loss improved from 0.23158 to 0.22695\n",
      "[2024-02-12 16:14:33,258] Epoch: 11 | FT  | Train Loss: 0.22176 | Val Loss: 0.22695\n",
      "[2024-02-12 16:14:33,266] Epoch: 11 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:14:33,266] Epoch: 11 | Ph  | Train Loss: 0.222 | Val Loss: 0.227\n",
      "[2024-02-12 16:14:33,267] Epoch: 11 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:14<00:00,  2.62it/s]\n",
      "[2024-02-12 16:15:53,264] Epoch: 12 | FT  | Train Loss: 0.22010 | Val Loss: 0.22896\n",
      "[2024-02-12 16:15:53,265] Epoch: 12 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:15:53,266] Epoch: 12 | Ph  | Train Loss: 0.220 | Val Loss: 0.229\n",
      "[2024-02-12 16:15:53,267] Epoch: 12 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:17<00:00,  2.51it/s]\n",
      "[2024-02-12 16:17:15,946] Epoch: 13 | FT  | Train Loss: 0.22008 | Val Loss: 0.23169\n",
      "[2024-02-12 16:17:15,959] Epoch: 13 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:17:15,962] Epoch: 13 | Ph  | Train Loss: 0.220 | Val Loss: 0.232\n",
      "[2024-02-12 16:17:15,964] Epoch: 13 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:14<00:00,  2.61it/s]\n",
      "[2024-02-12 16:18:35,594] Epoch: 14 | FT  | Train Loss: 0.21973 | Val Loss: 0.23151\n",
      "[2024-02-12 16:18:35,595] Epoch: 14 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:18:35,596] Epoch: 14 | Ph  | Train Loss: 0.220 | Val Loss: 0.232\n",
      "[2024-02-12 16:18:35,596] Epoch: 14 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:18<00:00,  2.50it/s]\n",
      "[2024-02-12 16:19:58,920] Epoch: 15 | FT  | Train Loss: 0.21962 | Val Loss: 0.22923\n",
      "[2024-02-12 16:19:58,923] Epoch: 15 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:19:58,924] Epoch: 15 | Ph  | Train Loss: 0.220 | Val Loss: 0.229\n",
      "[2024-02-12 16:19:58,925] Epoch: 15 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:10<00:00,  2.75it/s]\n",
      "[2024-02-12 16:21:14,942] Epoch: 16 | FT  | Train Loss: 0.21793 | Val Loss: 0.23117\n",
      "[2024-02-12 16:21:14,945] Epoch: 16 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:21:14,946] Epoch: 16 | Ph  | Train Loss: 0.218 | Val Loss: 0.231\n",
      "[2024-02-12 16:21:14,947] Epoch: 16 | Ending LR: 0.000095 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:17<00:00,  2.52it/s]\n",
      "[2024-02-12 16:22:37,538] Epoch: 17 | FT  | Train Loss: 0.21689 | Val Loss: 0.22984\n",
      "[2024-02-12 16:22:37,541] Epoch: 17 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:22:37,542] Epoch: 17 | Ph  | Train Loss: 0.217 | Val Loss: 0.230\n",
      "[2024-02-12 16:22:37,542] Epoch: 17 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:20<00:00,  2.41it/s]\n",
      "[2024-02-12 16:24:03,387] Saving improved model after Val Loss improved from 0.22695 to 0.22448\n",
      "[2024-02-12 16:24:03,486] Epoch: 18 | FT  | Train Loss: 0.21396 | Val Loss: 0.22448\n",
      "[2024-02-12 16:24:03,487] Epoch: 18 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:24:03,488] Epoch: 18 | Ph  | Train Loss: 0.214 | Val Loss: 0.224\n",
      "[2024-02-12 16:24:03,489] Epoch: 18 | Ending LR: 0.000095 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:13<00:00,  2.67it/s]\n",
      "[2024-02-12 16:25:21,654] Saving improved model after Val Loss improved from 0.22448 to 0.22176\n",
      "[2024-02-12 16:25:21,753] Epoch: 19 | FT  | Train Loss: 0.20967 | Val Loss: 0.22176\n",
      "[2024-02-12 16:25:21,754] Epoch: 19 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:25:21,754] Epoch: 19 | Ph  | Train Loss: 0.210 | Val Loss: 0.222\n",
      "[2024-02-12 16:25:21,754] Epoch: 19 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:14<00:00,  2.63it/s]\n",
      "[2024-02-12 16:26:41,097] Saving improved model after Val Loss improved from 0.22176 to 0.21803\n",
      "[2024-02-12 16:26:41,233] Epoch: 20 | FT  | Train Loss: 0.20634 | Val Loss: 0.21803\n",
      "[2024-02-12 16:26:41,235] Epoch: 20 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:26:41,236] Epoch: 20 | Ph  | Train Loss: 0.206 | Val Loss: 0.218\n",
      "[2024-02-12 16:26:41,237] Epoch: 20 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:18<00:00,  2.48it/s]\n",
      "[2024-02-12 16:28:04,775] Saving improved model after Val Loss improved from 0.21803 to 0.21612\n",
      "[2024-02-12 16:28:04,881] Epoch: 21 | FT  | Train Loss: 0.20311 | Val Loss: 0.21612\n",
      "[2024-02-12 16:28:04,885] Epoch: 21 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:28:04,886] Epoch: 21 | Ph  | Train Loss: 0.203 | Val Loss: 0.216\n",
      "[2024-02-12 16:28:04,887] Epoch: 21 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:13<00:00,  2.67it/s]\n",
      "[2024-02-12 16:29:22,972] Saving improved model after Val Loss improved from 0.21612 to 0.21258\n",
      "[2024-02-12 16:29:23,060] Epoch: 22 | FT  | Train Loss: 0.20065 | Val Loss: 0.21258\n",
      "[2024-02-12 16:29:23,061] Epoch: 22 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:29:23,062] Epoch: 22 | Ph  | Train Loss: 0.201 | Val Loss: 0.213\n",
      "[2024-02-12 16:29:23,065] Epoch: 22 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:13<00:00,  2.66it/s]\n",
      "[2024-02-12 16:30:41,564] Epoch: 23 | FT  | Train Loss: 0.19832 | Val Loss: 0.21302\n",
      "[2024-02-12 16:30:41,565] Epoch: 23 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:30:41,566] Epoch: 23 | Ph  | Train Loss: 0.198 | Val Loss: 0.213\n",
      "[2024-02-12 16:30:41,566] Epoch: 23 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:16<00:00,  2.55it/s]\n",
      "[2024-02-12 16:32:02,991] Saving improved model after Val Loss improved from 0.21258 to 0.21169\n",
      "[2024-02-12 16:32:03,079] Epoch: 24 | FT  | Train Loss: 0.19712 | Val Loss: 0.21169\n",
      "[2024-02-12 16:32:03,080] Epoch: 24 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:32:03,081] Epoch: 24 | Ph  | Train Loss: 0.197 | Val Loss: 0.212\n",
      "[2024-02-12 16:32:03,081] Epoch: 24 | Ending LR: 0.000028 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:11<00:00,  2.74it/s]\n",
      "[2024-02-12 16:33:19,806] Epoch: 25 | FT  | Train Loss: 0.19724 | Val Loss: 0.21253\n",
      "[2024-02-12 16:33:19,807] Epoch: 25 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:33:19,808] Epoch: 25 | Ph  | Train Loss: 0.197 | Val Loss: 0.213\n",
      "[2024-02-12 16:33:19,808] Epoch: 25 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:14<00:00,  2.61it/s]\n",
      "[2024-02-12 16:34:39,647] Epoch: 26 | FT  | Train Loss: 0.19713 | Val Loss: 0.21355\n",
      "[2024-02-12 16:34:39,649] Epoch: 26 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:34:39,649] Epoch: 26 | Ph  | Train Loss: 0.197 | Val Loss: 0.214\n",
      "[2024-02-12 16:34:39,650] Epoch: 26 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:18<00:00,  2.49it/s]\n",
      "[2024-02-12 16:36:03,282] Epoch: 27 | FT  | Train Loss: 0.19702 | Val Loss: 0.22036\n",
      "[2024-02-12 16:36:03,284] Epoch: 27 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:36:03,285] Epoch: 27 | Ph  | Train Loss: 0.197 | Val Loss: 0.220\n",
      "[2024-02-12 16:36:03,285] Epoch: 27 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:08<00:00,  2.85it/s]\n",
      "[2024-02-12 16:37:17,369] Epoch: 28 | FT  | Train Loss: 0.19669 | Val Loss: 0.21980\n",
      "[2024-02-12 16:37:17,372] Epoch: 28 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:37:17,372] Epoch: 28 | Ph  | Train Loss: 0.197 | Val Loss: 0.220\n",
      "[2024-02-12 16:37:17,373] Epoch: 28 | Ending LR: 0.000057 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:12<00:00,  2.70it/s]\n",
      "[2024-02-12 16:38:34,795] Epoch: 29 | FT  | Train Loss: 0.19667 | Val Loss: 0.21406\n",
      "[2024-02-12 16:38:34,796] Epoch: 29 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:38:34,797] Epoch: 29 | Ph  | Train Loss: 0.197 | Val Loss: 0.214\n",
      "[2024-02-12 16:38:34,797] Epoch: 29 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:15<00:00,  2.57it/s]\n",
      "[2024-02-12 16:39:56,041] Epoch: 30 | FT  | Train Loss: 0.19484 | Val Loss: 0.21335\n",
      "[2024-02-12 16:39:56,044] Epoch: 30 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:39:56,044] Epoch: 30 | Ph  | Train Loss: 0.195 | Val Loss: 0.213\n",
      "[2024-02-12 16:39:56,045] Epoch: 30 | Ending LR: 0.000057 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:13<00:00,  2.64it/s]\n",
      "[2024-02-12 16:41:15,402] Saving improved model after Val Loss improved from 0.21169 to 0.21043\n",
      "[2024-02-12 16:41:15,522] Epoch: 31 | FT  | Train Loss: 0.19327 | Val Loss: 0.21043\n",
      "[2024-02-12 16:41:15,523] Epoch: 31 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:41:15,524] Epoch: 31 | Ph  | Train Loss: 0.193 | Val Loss: 0.210\n",
      "[2024-02-12 16:41:15,525] Epoch: 31 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:16<00:00,  2.56it/s]\n",
      "[2024-02-12 16:42:36,891] Epoch: 32 | FT  | Train Loss: 0.19139 | Val Loss: 0.21565\n",
      "[2024-02-12 16:42:36,896] Epoch: 32 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:42:36,897] Epoch: 32 | Ph  | Train Loss: 0.191 | Val Loss: 0.216\n",
      "[2024-02-12 16:42:36,898] Epoch: 32 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:16<00:00,  2.54it/s]\n",
      "[2024-02-12 16:43:58,773] Saving improved model after Val Loss improved from 0.21043 to 0.20800\n",
      "[2024-02-12 16:43:58,876] Epoch: 33 | FT  | Train Loss: 0.18963 | Val Loss: 0.20800\n",
      "[2024-02-12 16:43:58,877] Epoch: 33 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:43:58,878] Epoch: 33 | Ph  | Train Loss: 0.190 | Val Loss: 0.208\n",
      "[2024-02-12 16:43:58,879] Epoch: 33 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:11<00:00,  2.71it/s]\n",
      "[2024-02-12 16:45:15,951] Saving improved model after Val Loss improved from 0.20800 to 0.20657\n",
      "[2024-02-12 16:45:16,060] Epoch: 34 | FT  | Train Loss: 0.18816 | Val Loss: 0.20657\n",
      "[2024-02-12 16:45:16,061] Epoch: 34 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:45:16,062] Epoch: 34 | Ph  | Train Loss: 0.188 | Val Loss: 0.207\n",
      "[2024-02-12 16:45:16,062] Epoch: 34 | Ending LR: 0.000028 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:16<00:00,  2.55it/s]\n",
      "[2024-02-12 16:46:37,977] Saving improved model after Val Loss improved from 0.20657 to 0.20522\n",
      "[2024-02-12 16:46:38,072] Epoch: 35 | FT  | Train Loss: 0.18658 | Val Loss: 0.20522\n",
      "[2024-02-12 16:46:38,073] Epoch: 35 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:46:38,074] Epoch: 35 | Ph  | Train Loss: 0.187 | Val Loss: 0.205\n",
      "[2024-02-12 16:46:38,074] Epoch: 35 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:17<00:00,  2.53it/s]\n",
      "[2024-02-12 16:48:00,366] Epoch: 36 | FT  | Train Loss: 0.18586 | Val Loss: 0.20647\n",
      "[2024-02-12 16:48:00,373] Epoch: 36 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:48:00,375] Epoch: 36 | Ph  | Train Loss: 0.186 | Val Loss: 0.206\n",
      "[2024-02-12 16:48:00,377] Epoch: 36 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:14<00:00,  2.63it/s]\n",
      "[2024-02-12 16:49:19,790] Epoch: 37 | FT  | Train Loss: 0.18581 | Val Loss: 0.20704\n",
      "[2024-02-12 16:49:19,792] Epoch: 37 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:49:19,792] Epoch: 37 | Ph  | Train Loss: 0.186 | Val Loss: 0.207\n",
      "[2024-02-12 16:49:19,794] Epoch: 37 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:16<00:00,  2.56it/s]\n",
      "[2024-02-12 16:50:41,130] Epoch: 38 | FT  | Train Loss: 0.18587 | Val Loss: 0.20612\n",
      "[2024-02-12 16:50:41,132] Epoch: 38 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:50:41,134] Epoch: 38 | Ph  | Train Loss: 0.186 | Val Loss: 0.206\n",
      "[2024-02-12 16:50:41,135] Epoch: 38 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:11<00:00,  2.74it/s]\n",
      "[2024-02-12 16:51:58,129] Epoch: 39 | FT  | Train Loss: 0.18569 | Val Loss: 0.21000\n",
      "[2024-02-12 16:51:58,140] Epoch: 39 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:51:58,143] Epoch: 39 | Ph  | Train Loss: 0.186 | Val Loss: 0.210\n",
      "[2024-02-12 16:51:58,146] Epoch: 39 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:20<00:00,  2.42it/s]\n",
      "[2024-02-12 16:53:23,944] Epoch: 40 | FT  | Train Loss: 0.18542 | Val Loss: 0.20589\n",
      "[2024-02-12 16:53:23,957] Epoch: 40 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:53:23,960] Epoch: 40 | Ph  | Train Loss: 0.185 | Val Loss: 0.206\n",
      "[2024-02-12 16:53:23,964] Epoch: 40 | Ending LR: 0.000039 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:12<00:00,  2.70it/s]\n",
      "[2024-02-12 16:54:41,314] Epoch: 41 | FT  | Train Loss: 0.18510 | Val Loss: 0.20561\n",
      "[2024-02-12 16:54:41,334] Epoch: 41 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:54:41,338] Epoch: 41 | Ph  | Train Loss: 0.185 | Val Loss: 0.206\n",
      "[2024-02-12 16:54:41,339] Epoch: 41 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:16<00:00,  2.56it/s]\n",
      "[2024-02-12 16:56:02,804] Epoch: 42 | FT  | Train Loss: 0.18474 | Val Loss: 0.20896\n",
      "[2024-02-12 16:56:02,806] Epoch: 42 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:56:02,807] Epoch: 42 | Ph  | Train Loss: 0.185 | Val Loss: 0.209\n",
      "[2024-02-12 16:56:02,808] Epoch: 42 | Ending LR: 0.000039 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:14<00:00,  2.60it/s]\n",
      "[2024-02-12 16:57:22,949] Epoch: 43 | FT  | Train Loss: 0.18367 | Val Loss: 0.20970\n",
      "[2024-02-12 16:57:22,950] Epoch: 43 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:57:22,951] Epoch: 43 | Ph  | Train Loss: 0.184 | Val Loss: 0.210\n",
      "[2024-02-12 16:57:22,951] Epoch: 43 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:11<00:00,  2.72it/s]\n",
      "[2024-02-12 16:58:39,867] Saving improved model after Val Loss improved from 0.20522 to 0.20398\n",
      "[2024-02-12 16:58:39,989] Epoch: 44 | FT  | Train Loss: 0.18229 | Val Loss: 0.20398\n",
      "[2024-02-12 16:58:39,990] Epoch: 44 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:58:39,991] Epoch: 44 | Ph  | Train Loss: 0.182 | Val Loss: 0.204\n",
      "[2024-02-12 16:58:39,993] Epoch: 44 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:11<00:00,  2.73it/s]\n",
      "[2024-02-12 16:59:56,581] Epoch: 45 | FT  | Train Loss: 0.18135 | Val Loss: 0.20787\n",
      "[2024-02-12 16:59:56,582] Epoch: 45 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 16:59:56,583] Epoch: 45 | Ph  | Train Loss: 0.181 | Val Loss: 0.208\n",
      "[2024-02-12 16:59:56,583] Epoch: 45 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:17<00:00,  2.51it/s]\n",
      "[2024-02-12 17:01:19,304] Epoch: 46 | FT  | Train Loss: 0.18055 | Val Loss: 0.20438\n",
      "[2024-02-12 17:01:19,316] Epoch: 46 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:01:19,318] Epoch: 46 | Ph  | Train Loss: 0.181 | Val Loss: 0.204\n",
      "[2024-02-12 17:01:19,320] Epoch: 46 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:11<00:00,  2.74it/s]\n",
      "[2024-02-12 17:02:35,518] Saving improved model after Val Loss improved from 0.20398 to 0.20293\n",
      "[2024-02-12 17:02:35,642] Epoch: 47 | FT  | Train Loss: 0.17963 | Val Loss: 0.20293\n",
      "[2024-02-12 17:02:35,643] Epoch: 47 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:02:35,644] Epoch: 47 | Ph  | Train Loss: 0.180 | Val Loss: 0.203\n",
      "[2024-02-12 17:02:35,645] Epoch: 47 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:16<00:00,  2.56it/s]\n",
      "[2024-02-12 17:03:56,695] Epoch: 48 | FT  | Train Loss: 0.17898 | Val Loss: 0.20418\n",
      "[2024-02-12 17:03:56,707] Epoch: 48 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:03:56,709] Epoch: 48 | Ph  | Train Loss: 0.179 | Val Loss: 0.204\n",
      "[2024-02-12 17:03:56,712] Epoch: 48 | Ending LR: 0.000022 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:11<00:00,  2.72it/s]\n",
      "[2024-02-12 17:05:13,764] Epoch: 49 | FT  | Train Loss: 0.17897 | Val Loss: 0.20482\n",
      "[2024-02-12 17:05:13,766] Epoch: 49 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:05:13,767] Epoch: 49 | Ph  | Train Loss: 0.179 | Val Loss: 0.205\n",
      "[2024-02-12 17:05:13,768] Epoch: 49 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:10<00:00,  2.76it/s]\n",
      "[2024-02-12 17:06:30,093] Saving improved model after Val Loss improved from 0.20293 to 0.20268\n",
      "[2024-02-12 17:06:30,212] Epoch: 50 | FT  | Train Loss: 0.17881 | Val Loss: 0.20268\n",
      "[2024-02-12 17:06:30,214] Epoch: 50 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:06:30,214] Epoch: 50 | Ph  | Train Loss: 0.179 | Val Loss: 0.203\n",
      "[2024-02-12 17:06:30,215] Epoch: 50 | Ending LR: 0.000026 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:11<00:00,  2.72it/s]\n",
      "[2024-02-12 17:07:47,191] Epoch: 51 | FT  | Train Loss: 0.17865 | Val Loss: 0.20374\n",
      "[2024-02-12 17:07:47,193] Epoch: 51 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:07:47,194] Epoch: 51 | Ph  | Train Loss: 0.179 | Val Loss: 0.204\n",
      "[2024-02-12 17:07:47,195] Epoch: 51 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:13<00:00,  2.66it/s]\n",
      "[2024-02-12 17:09:05,216] Epoch: 52 | FT  | Train Loss: 0.17834 | Val Loss: 0.20579\n",
      "[2024-02-12 17:09:05,218] Epoch: 52 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:09:05,219] Epoch: 52 | Ph  | Train Loss: 0.178 | Val Loss: 0.206\n",
      "[2024-02-12 17:09:05,220] Epoch: 52 | Ending LR: 0.000029 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:11<00:00,  2.73it/s]\n",
      "[2024-02-12 17:10:21,768] Epoch: 53 | FT  | Train Loss: 0.17817 | Val Loss: 0.20773\n",
      "[2024-02-12 17:10:21,770] Epoch: 53 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:10:21,771] Epoch: 53 | Ph  | Train Loss: 0.178 | Val Loss: 0.208\n",
      "[2024-02-12 17:10:21,772] Epoch: 53 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:15<00:00,  2.57it/s]\n",
      "[2024-02-12 17:11:42,574] Saving improved model after Val Loss improved from 0.20268 to 0.20255\n",
      "[2024-02-12 17:11:42,687] Epoch: 54 | FT  | Train Loss: 0.17772 | Val Loss: 0.20255\n",
      "[2024-02-12 17:11:42,688] Epoch: 54 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:11:42,689] Epoch: 54 | Ph  | Train Loss: 0.178 | Val Loss: 0.203\n",
      "[2024-02-12 17:11:42,689] Epoch: 54 | Ending LR: 0.000029 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:13<00:00,  2.64it/s]\n",
      "[2024-02-12 17:13:01,810] Epoch: 55 | FT  | Train Loss: 0.17709 | Val Loss: 0.20279\n",
      "[2024-02-12 17:13:01,822] Epoch: 55 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:13:01,825] Epoch: 55 | Ph  | Train Loss: 0.177 | Val Loss: 0.203\n",
      "[2024-02-12 17:13:01,827] Epoch: 55 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:15<00:00,  2.59it/s]\n",
      "[2024-02-12 17:14:22,683] Saving improved model after Val Loss improved from 0.20255 to 0.20090\n",
      "[2024-02-12 17:14:22,811] Epoch: 56 | FT  | Train Loss: 0.17664 | Val Loss: 0.20090\n",
      "[2024-02-12 17:14:22,812] Epoch: 56 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:14:22,813] Epoch: 56 | Ph  | Train Loss: 0.177 | Val Loss: 0.201\n",
      "[2024-02-12 17:14:22,814] Epoch: 56 | Ending LR: 0.000026 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:13<00:00,  2.64it/s]\n",
      "[2024-02-12 17:15:43,037] Epoch: 57 | FT  | Train Loss: 0.17590 | Val Loss: 0.20412\n",
      "[2024-02-12 17:15:43,039] Epoch: 57 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:15:43,040] Epoch: 57 | Ph  | Train Loss: 0.176 | Val Loss: 0.204\n",
      "[2024-02-12 17:15:43,041] Epoch: 57 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:16<00:00,  2.54it/s]\n",
      "[2024-02-12 17:17:05,371] Epoch: 58 | FT  | Train Loss: 0.17519 | Val Loss: 0.20216\n",
      "[2024-02-12 17:17:05,374] Epoch: 58 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:17:05,375] Epoch: 58 | Ph  | Train Loss: 0.175 | Val Loss: 0.202\n",
      "[2024-02-12 17:17:05,376] Epoch: 58 | Ending LR: 0.000022 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195/195 [01:12<00:00,  2.67it/s]\n",
      "[2024-02-12 17:18:23,581] Epoch: 59 | FT  | Train Loss: 0.17439 | Val Loss: 0.20272\n",
      "[2024-02-12 17:18:23,599] Epoch: 59 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:18:23,602] Epoch: 59 | Ph  | Train Loss: 0.174 | Val Loss: 0.203\n",
      "[2024-02-12 17:18:23,604] Epoch: 59 | Ending LR: 0.000020 \n",
      "[2024-02-12 17:18:23,736] Decimating dataset to 0.7 of the original size...\n",
      "[2024-02-12 17:18:23,851] Using DataParallel with 2 devices.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:05<00:00,  2.62it/s]\n",
      "[2024-02-12 17:19:34,040] Saving improved model after Val Loss improved from inf to 0.50163\n",
      "[2024-02-12 17:19:34,080] Epoch: 0 | FT  | Train Loss: 0.58769 | Val Loss: 0.50163\n",
      "[2024-02-12 17:19:34,081] Epoch: 0 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:19:34,082] Epoch: 0 | Ph  | Train Loss: 0.588 | Val Loss: 0.502\n",
      "[2024-02-12 17:19:34,083] Epoch: 0 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:04<00:00,  2.64it/s]\n",
      "[2024-02-12 17:20:43,863] Saving improved model after Val Loss improved from 0.50163 to 0.38702\n",
      "[2024-02-12 17:20:43,953] Epoch: 1 | FT  | Train Loss: 0.43057 | Val Loss: 0.38702\n",
      "[2024-02-12 17:20:43,956] Epoch: 1 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:20:43,957] Epoch: 1 | Ph  | Train Loss: 0.431 | Val Loss: 0.387\n",
      "[2024-02-12 17:20:43,958] Epoch: 1 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:05<00:00,  2.62it/s]\n",
      "[2024-02-12 17:21:54,515] Saving improved model after Val Loss improved from 0.38702 to 0.33863\n",
      "[2024-02-12 17:21:54,610] Epoch: 2 | FT  | Train Loss: 0.35501 | Val Loss: 0.33863\n",
      "[2024-02-12 17:21:54,613] Epoch: 2 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:21:54,615] Epoch: 2 | Ph  | Train Loss: 0.355 | Val Loss: 0.339\n",
      "[2024-02-12 17:21:54,617] Epoch: 2 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:05<00:00,  2.61it/s]\n",
      "[2024-02-12 17:23:05,306] Saving improved model after Val Loss improved from 0.33863 to 0.31177\n",
      "[2024-02-12 17:23:05,390] Epoch: 3 | FT  | Train Loss: 0.31718 | Val Loss: 0.31177\n",
      "[2024-02-12 17:23:05,393] Epoch: 3 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:23:05,395] Epoch: 3 | Ph  | Train Loss: 0.317 | Val Loss: 0.312\n",
      "[2024-02-12 17:23:05,398] Epoch: 3 | Ending LR: 0.000140 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:07<00:00,  2.53it/s]\n",
      "[2024-02-12 17:24:18,043] Saving improved model after Val Loss improved from 0.31177 to 0.29832\n",
      "[2024-02-12 17:24:18,145] Epoch: 4 | FT  | Train Loss: 0.29314 | Val Loss: 0.29832\n",
      "[2024-02-12 17:24:18,146] Epoch: 4 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:24:18,147] Epoch: 4 | Ph  | Train Loss: 0.293 | Val Loss: 0.298\n",
      "[2024-02-12 17:24:18,147] Epoch: 4 | Ending LR: 0.000170 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:06<00:00,  2.59it/s]\n",
      "[2024-02-12 17:25:29,573] Saving improved model after Val Loss improved from 0.29832 to 0.27854\n",
      "[2024-02-12 17:25:29,668] Epoch: 5 | FT  | Train Loss: 0.27611 | Val Loss: 0.27854\n",
      "[2024-02-12 17:25:29,669] Epoch: 5 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:25:29,670] Epoch: 5 | Ph  | Train Loss: 0.276 | Val Loss: 0.279\n",
      "[2024-02-12 17:25:29,670] Epoch: 5 | Ending LR: 0.000200 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:02<00:00,  2.72it/s]\n",
      "[2024-02-12 17:26:38,070] Saving improved model after Val Loss improved from 0.27854 to 0.26509\n",
      "[2024-02-12 17:26:38,188] Epoch: 6 | FT  | Train Loss: 0.26230 | Val Loss: 0.26509\n",
      "[2024-02-12 17:26:38,190] Epoch: 6 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:26:38,191] Epoch: 6 | Ph  | Train Loss: 0.262 | Val Loss: 0.265\n",
      "[2024-02-12 17:26:38,192] Epoch: 6 | Ending LR: 0.000170 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:03<00:00,  2.68it/s]\n",
      "[2024-02-12 17:27:47,512] Saving improved model after Val Loss improved from 0.26509 to 0.25901\n",
      "[2024-02-12 17:27:47,604] Epoch: 7 | FT  | Train Loss: 0.25105 | Val Loss: 0.25901\n",
      "[2024-02-12 17:27:47,605] Epoch: 7 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:27:47,606] Epoch: 7 | Ph  | Train Loss: 0.251 | Val Loss: 0.259\n",
      "[2024-02-12 17:27:47,606] Epoch: 7 | Ending LR: 0.000140 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:06<00:00,  2.58it/s]\n",
      "[2024-02-12 17:28:59,785] Saving improved model after Val Loss improved from 0.25901 to 0.24602\n",
      "[2024-02-12 17:28:59,827] Epoch: 8 | FT  | Train Loss: 0.24295 | Val Loss: 0.24602\n",
      "[2024-02-12 17:28:59,830] Epoch: 8 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:28:59,833] Epoch: 8 | Ph  | Train Loss: 0.243 | Val Loss: 0.246\n",
      "[2024-02-12 17:28:59,835] Epoch: 8 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:04<00:00,  2.66it/s]\n",
      "[2024-02-12 17:30:09,193] Saving improved model after Val Loss improved from 0.24602 to 0.24424\n",
      "[2024-02-12 17:30:09,279] Epoch: 9 | FT  | Train Loss: 0.23630 | Val Loss: 0.24424\n",
      "[2024-02-12 17:30:09,281] Epoch: 9 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:30:09,281] Epoch: 9 | Ph  | Train Loss: 0.236 | Val Loss: 0.244\n",
      "[2024-02-12 17:30:09,282] Epoch: 9 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:01<00:00,  2.76it/s]\n",
      "[2024-02-12 17:31:16,346] Saving improved model after Val Loss improved from 0.24424 to 0.24165\n",
      "[2024-02-12 17:31:16,438] Epoch: 10 | FT  | Train Loss: 0.23189 | Val Loss: 0.24165\n",
      "[2024-02-12 17:31:16,439] Epoch: 10 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:31:16,440] Epoch: 10 | Ph  | Train Loss: 0.232 | Val Loss: 0.242\n",
      "[2024-02-12 17:31:16,441] Epoch: 10 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:03<00:00,  2.70it/s]\n",
      "[2024-02-12 17:32:25,115] Saving improved model after Val Loss improved from 0.24165 to 0.23372\n",
      "[2024-02-12 17:32:25,209] Epoch: 11 | FT  | Train Loss: 0.22696 | Val Loss: 0.23372\n",
      "[2024-02-12 17:32:25,209] Epoch: 11 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:32:25,210] Epoch: 11 | Ph  | Train Loss: 0.227 | Val Loss: 0.234\n",
      "[2024-02-12 17:32:25,210] Epoch: 11 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:01<00:00,  2.78it/s]\n",
      "[2024-02-12 17:33:31,659] Saving improved model after Val Loss improved from 0.23372 to 0.23367\n",
      "[2024-02-12 17:33:31,782] Epoch: 12 | FT  | Train Loss: 0.22495 | Val Loss: 0.23367\n",
      "[2024-02-12 17:33:31,784] Epoch: 12 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:33:31,785] Epoch: 12 | Ph  | Train Loss: 0.225 | Val Loss: 0.234\n",
      "[2024-02-12 17:33:31,786] Epoch: 12 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:04<00:00,  2.63it/s]\n",
      "[2024-02-12 17:34:41,633] Epoch: 13 | FT  | Train Loss: 0.22490 | Val Loss: 0.23478\n",
      "[2024-02-12 17:34:41,635] Epoch: 13 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:34:41,636] Epoch: 13 | Ph  | Train Loss: 0.225 | Val Loss: 0.235\n",
      "[2024-02-12 17:34:41,637] Epoch: 13 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:05<00:00,  2.60it/s]\n",
      "[2024-02-12 17:35:52,622] Epoch: 14 | FT  | Train Loss: 0.22446 | Val Loss: 0.23374\n",
      "[2024-02-12 17:35:52,626] Epoch: 14 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:35:52,627] Epoch: 14 | Ph  | Train Loss: 0.224 | Val Loss: 0.234\n",
      "[2024-02-12 17:35:52,628] Epoch: 14 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:03<00:00,  2.70it/s]\n",
      "[2024-02-12 17:37:00,851] Epoch: 15 | FT  | Train Loss: 0.22411 | Val Loss: 0.23596\n",
      "[2024-02-12 17:37:00,853] Epoch: 15 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:37:00,854] Epoch: 15 | Ph  | Train Loss: 0.224 | Val Loss: 0.236\n",
      "[2024-02-12 17:37:00,855] Epoch: 15 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:59<00:00,  2.87it/s]\n",
      "[2024-02-12 17:38:05,743] Saving improved model after Val Loss improved from 0.23367 to 0.23343\n",
      "[2024-02-12 17:38:05,856] Epoch: 16 | FT  | Train Loss: 0.22188 | Val Loss: 0.23343\n",
      "[2024-02-12 17:38:05,857] Epoch: 16 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:38:05,858] Epoch: 16 | Ph  | Train Loss: 0.222 | Val Loss: 0.233\n",
      "[2024-02-12 17:38:05,859] Epoch: 16 | Ending LR: 0.000095 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:07<00:00,  2.52it/s]\n",
      "[2024-02-12 17:39:18,876] Epoch: 17 | FT  | Train Loss: 0.22105 | Val Loss: 0.25237\n",
      "[2024-02-12 17:39:18,878] Epoch: 17 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:39:18,879] Epoch: 17 | Ph  | Train Loss: 0.221 | Val Loss: 0.252\n",
      "[2024-02-12 17:39:18,880] Epoch: 17 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:05<00:00,  2.62it/s]\n",
      "[2024-02-12 17:40:28,857] Saving improved model after Val Loss improved from 0.23343 to 0.22856\n",
      "[2024-02-12 17:40:28,968] Epoch: 18 | FT  | Train Loss: 0.21803 | Val Loss: 0.22856\n",
      "[2024-02-12 17:40:28,970] Epoch: 18 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:40:28,970] Epoch: 18 | Ph  | Train Loss: 0.218 | Val Loss: 0.229\n",
      "[2024-02-12 17:40:28,971] Epoch: 18 | Ending LR: 0.000095 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [01:06<00:00,  2.57it/s]\n",
      "[2024-02-12 17:41:40,523] Saving improved model after Val Loss improved from 0.22856 to 0.22561\n",
      "[2024-02-12 17:41:40,633] Epoch: 19 | FT  | Train Loss: 0.21400 | Val Loss: 0.22561\n",
      "[2024-02-12 17:41:40,634] Epoch: 19 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:41:40,636] Epoch: 19 | Ph  | Train Loss: 0.214 | Val Loss: 0.226\n",
      "[2024-02-12 17:41:40,636] Epoch: 19 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:46<00:00,  3.67it/s]\n",
      "[2024-02-12 17:42:31,252] Epoch: 20 | FT  | Train Loss: 0.21086 | Val Loss: 0.23222\n",
      "[2024-02-12 17:42:31,254] Epoch: 20 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:42:31,254] Epoch: 20 | Ph  | Train Loss: 0.211 | Val Loss: 0.232\n",
      "[2024-02-12 17:42:31,255] Epoch: 20 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.80it/s]\n",
      "[2024-02-12 17:43:20,266] Epoch: 21 | FT  | Train Loss: 0.20757 | Val Loss: 0.23091\n",
      "[2024-02-12 17:43:20,267] Epoch: 21 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:43:20,268] Epoch: 21 | Ph  | Train Loss: 0.208 | Val Loss: 0.231\n",
      "[2024-02-12 17:43:20,268] Epoch: 21 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:44<00:00,  3.85it/s]\n",
      "[2024-02-12 17:44:08,927] Saving improved model after Val Loss improved from 0.22561 to 0.22045\n",
      "[2024-02-12 17:44:09,018] Epoch: 22 | FT  | Train Loss: 0.20494 | Val Loss: 0.22045\n",
      "[2024-02-12 17:44:09,019] Epoch: 22 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:44:09,019] Epoch: 22 | Ph  | Train Loss: 0.205 | Val Loss: 0.220\n",
      "[2024-02-12 17:44:09,020] Epoch: 22 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:44<00:00,  3.83it/s]\n",
      "[2024-02-12 17:44:57,662] Saving improved model after Val Loss improved from 0.22045 to 0.21661\n",
      "[2024-02-12 17:44:57,760] Epoch: 23 | FT  | Train Loss: 0.20243 | Val Loss: 0.21661\n",
      "[2024-02-12 17:44:57,761] Epoch: 23 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:44:57,761] Epoch: 23 | Ph  | Train Loss: 0.202 | Val Loss: 0.217\n",
      "[2024-02-12 17:44:57,762] Epoch: 23 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.76it/s]\n",
      "[2024-02-12 17:45:47,575] Epoch: 24 | FT  | Train Loss: 0.20122 | Val Loss: 0.21772\n",
      "[2024-02-12 17:45:47,577] Epoch: 24 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:45:47,577] Epoch: 24 | Ph  | Train Loss: 0.201 | Val Loss: 0.218\n",
      "[2024-02-12 17:45:47,578] Epoch: 24 | Ending LR: 0.000028 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.79it/s]\n",
      "[2024-02-12 17:46:36,810] Epoch: 25 | FT  | Train Loss: 0.20116 | Val Loss: 0.21956\n",
      "[2024-02-12 17:46:36,811] Epoch: 25 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:46:36,812] Epoch: 25 | Ph  | Train Loss: 0.201 | Val Loss: 0.220\n",
      "[2024-02-12 17:46:36,812] Epoch: 25 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.79it/s]\n",
      "[2024-02-12 17:47:26,175] Epoch: 26 | FT  | Train Loss: 0.20116 | Val Loss: 0.21972\n",
      "[2024-02-12 17:47:26,176] Epoch: 26 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:47:26,177] Epoch: 26 | Ph  | Train Loss: 0.201 | Val Loss: 0.220\n",
      "[2024-02-12 17:47:26,177] Epoch: 26 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.76it/s]\n",
      "[2024-02-12 17:48:15,775] Epoch: 27 | FT  | Train Loss: 0.20125 | Val Loss: 0.22458\n",
      "[2024-02-12 17:48:15,777] Epoch: 27 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:48:15,777] Epoch: 27 | Ph  | Train Loss: 0.201 | Val Loss: 0.225\n",
      "[2024-02-12 17:48:15,778] Epoch: 27 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.77it/s]\n",
      "[2024-02-12 17:49:05,177] Epoch: 28 | FT  | Train Loss: 0.20121 | Val Loss: 0.22522\n",
      "[2024-02-12 17:49:05,179] Epoch: 28 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:49:05,179] Epoch: 28 | Ph  | Train Loss: 0.201 | Val Loss: 0.225\n",
      "[2024-02-12 17:49:05,180] Epoch: 28 | Ending LR: 0.000057 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.77it/s]\n",
      "[2024-02-12 17:49:54,853] Epoch: 29 | FT  | Train Loss: 0.20072 | Val Loss: 0.21896\n",
      "[2024-02-12 17:49:54,854] Epoch: 29 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:49:54,855] Epoch: 29 | Ph  | Train Loss: 0.201 | Val Loss: 0.219\n",
      "[2024-02-12 17:49:54,855] Epoch: 29 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.73it/s]\n",
      "[2024-02-12 17:50:44,669] Epoch: 30 | FT  | Train Loss: 0.19865 | Val Loss: 0.21735\n",
      "[2024-02-12 17:50:44,671] Epoch: 30 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:50:44,671] Epoch: 30 | Ph  | Train Loss: 0.199 | Val Loss: 0.217\n",
      "[2024-02-12 17:50:44,672] Epoch: 30 | Ending LR: 0.000057 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.76it/s]\n",
      "[2024-02-12 17:51:34,343] Saving improved model after Val Loss improved from 0.21661 to 0.21426\n",
      "[2024-02-12 17:51:34,443] Epoch: 31 | FT  | Train Loss: 0.19699 | Val Loss: 0.21426\n",
      "[2024-02-12 17:51:34,444] Epoch: 31 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:51:34,445] Epoch: 31 | Ph  | Train Loss: 0.197 | Val Loss: 0.214\n",
      "[2024-02-12 17:51:34,445] Epoch: 31 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.73it/s]\n",
      "[2024-02-12 17:52:24,433] Epoch: 32 | FT  | Train Loss: 0.19488 | Val Loss: 0.21491\n",
      "[2024-02-12 17:52:24,434] Epoch: 32 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:52:24,435] Epoch: 32 | Ph  | Train Loss: 0.195 | Val Loss: 0.215\n",
      "[2024-02-12 17:52:24,436] Epoch: 32 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:44<00:00,  3.82it/s]\n",
      "[2024-02-12 17:53:13,250] Epoch: 33 | FT  | Train Loss: 0.19333 | Val Loss: 0.21555\n",
      "[2024-02-12 17:53:13,251] Epoch: 33 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:53:13,252] Epoch: 33 | Ph  | Train Loss: 0.193 | Val Loss: 0.216\n",
      "[2024-02-12 17:53:13,253] Epoch: 33 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.79it/s]\n",
      "[2024-02-12 17:54:02,678] Saving improved model after Val Loss improved from 0.21426 to 0.21281\n",
      "[2024-02-12 17:54:02,772] Epoch: 34 | FT  | Train Loss: 0.19176 | Val Loss: 0.21281\n",
      "[2024-02-12 17:54:02,774] Epoch: 34 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:54:02,775] Epoch: 34 | Ph  | Train Loss: 0.192 | Val Loss: 0.213\n",
      "[2024-02-12 17:54:02,776] Epoch: 34 | Ending LR: 0.000028 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.77it/s]\n",
      "[2024-02-12 17:54:52,275] Epoch: 35 | FT  | Train Loss: 0.19008 | Val Loss: 0.21394\n",
      "[2024-02-12 17:54:52,276] Epoch: 35 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:54:52,276] Epoch: 35 | Ph  | Train Loss: 0.190 | Val Loss: 0.214\n",
      "[2024-02-12 17:54:52,277] Epoch: 35 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.76it/s]\n",
      "[2024-02-12 17:55:41,774] Saving improved model after Val Loss improved from 0.21281 to 0.21120\n",
      "[2024-02-12 17:55:41,875] Epoch: 36 | FT  | Train Loss: 0.18938 | Val Loss: 0.21120\n",
      "[2024-02-12 17:55:41,876] Epoch: 36 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:55:41,877] Epoch: 36 | Ph  | Train Loss: 0.189 | Val Loss: 0.211\n",
      "[2024-02-12 17:55:41,878] Epoch: 36 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:44<00:00,  3.81it/s]\n",
      "[2024-02-12 17:56:30,668] Epoch: 37 | FT  | Train Loss: 0.18927 | Val Loss: 0.21205\n",
      "[2024-02-12 17:56:30,670] Epoch: 37 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:56:30,671] Epoch: 37 | Ph  | Train Loss: 0.189 | Val Loss: 0.212\n",
      "[2024-02-12 17:56:30,671] Epoch: 37 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.76it/s]\n",
      "[2024-02-12 17:57:20,096] Epoch: 38 | FT  | Train Loss: 0.18924 | Val Loss: 0.21260\n",
      "[2024-02-12 17:57:20,097] Epoch: 38 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:57:20,098] Epoch: 38 | Ph  | Train Loss: 0.189 | Val Loss: 0.213\n",
      "[2024-02-12 17:57:20,098] Epoch: 38 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.78it/s]\n",
      "[2024-02-12 17:58:09,285] Epoch: 39 | FT  | Train Loss: 0.18894 | Val Loss: 0.21744\n",
      "[2024-02-12 17:58:09,286] Epoch: 39 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:58:09,286] Epoch: 39 | Ph  | Train Loss: 0.189 | Val Loss: 0.217\n",
      "[2024-02-12 17:58:09,287] Epoch: 39 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:44<00:00,  3.85it/s]\n",
      "[2024-02-12 17:58:57,709] Epoch: 40 | FT  | Train Loss: 0.18890 | Val Loss: 0.21193\n",
      "[2024-02-12 17:58:57,710] Epoch: 40 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:58:57,711] Epoch: 40 | Ph  | Train Loss: 0.189 | Val Loss: 0.212\n",
      "[2024-02-12 17:58:57,711] Epoch: 40 | Ending LR: 0.000039 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:44<00:00,  3.82it/s]\n",
      "[2024-02-12 17:59:46,619] Epoch: 41 | FT  | Train Loss: 0.18876 | Val Loss: 0.21678\n",
      "[2024-02-12 17:59:46,621] Epoch: 41 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 17:59:46,621] Epoch: 41 | Ph  | Train Loss: 0.189 | Val Loss: 0.217\n",
      "[2024-02-12 17:59:46,622] Epoch: 41 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:44<00:00,  3.82it/s]\n",
      "[2024-02-12 18:00:35,520] Saving improved model after Val Loss improved from 0.21120 to 0.21001\n",
      "[2024-02-12 18:00:35,638] Epoch: 42 | FT  | Train Loss: 0.18831 | Val Loss: 0.21001\n",
      "[2024-02-12 18:00:35,639] Epoch: 42 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:00:35,639] Epoch: 42 | Ph  | Train Loss: 0.188 | Val Loss: 0.210\n",
      "[2024-02-12 18:00:35,640] Epoch: 42 | Ending LR: 0.000039 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.79it/s]\n",
      "[2024-02-12 18:01:24,802] Epoch: 43 | FT  | Train Loss: 0.18657 | Val Loss: 0.21329\n",
      "[2024-02-12 18:01:24,803] Epoch: 43 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:01:24,804] Epoch: 43 | Ph  | Train Loss: 0.187 | Val Loss: 0.213\n",
      "[2024-02-12 18:01:24,805] Epoch: 43 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.79it/s]\n",
      "[2024-02-12 18:02:13,965] Epoch: 44 | FT  | Train Loss: 0.18568 | Val Loss: 0.21088\n",
      "[2024-02-12 18:02:13,966] Epoch: 44 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:02:13,967] Epoch: 44 | Ph  | Train Loss: 0.186 | Val Loss: 0.211\n",
      "[2024-02-12 18:02:13,967] Epoch: 44 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:44<00:00,  3.83it/s]\n",
      "[2024-02-12 18:03:02,639] Epoch: 45 | FT  | Train Loss: 0.18452 | Val Loss: 0.21303\n",
      "[2024-02-12 18:03:02,641] Epoch: 45 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:03:02,642] Epoch: 45 | Ph  | Train Loss: 0.185 | Val Loss: 0.213\n",
      "[2024-02-12 18:03:02,642] Epoch: 45 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:46<00:00,  3.70it/s]\n",
      "[2024-02-12 18:03:53,280] Epoch: 46 | FT  | Train Loss: 0.18367 | Val Loss: 0.21416\n",
      "[2024-02-12 18:03:53,282] Epoch: 46 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:03:53,283] Epoch: 46 | Ph  | Train Loss: 0.184 | Val Loss: 0.214\n",
      "[2024-02-12 18:03:53,284] Epoch: 46 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:44<00:00,  3.81it/s]\n",
      "[2024-02-12 18:04:42,193] Saving improved model after Val Loss improved from 0.21001 to 0.20771\n",
      "[2024-02-12 18:04:42,292] Epoch: 47 | FT  | Train Loss: 0.18260 | Val Loss: 0.20771\n",
      "[2024-02-12 18:04:42,293] Epoch: 47 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:04:42,293] Epoch: 47 | Ph  | Train Loss: 0.183 | Val Loss: 0.208\n",
      "[2024-02-12 18:04:42,294] Epoch: 47 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:44<00:00,  3.80it/s]\n",
      "[2024-02-12 18:05:31,258] Epoch: 48 | FT  | Train Loss: 0.18211 | Val Loss: 0.20848\n",
      "[2024-02-12 18:05:31,261] Epoch: 48 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:05:31,261] Epoch: 48 | Ph  | Train Loss: 0.182 | Val Loss: 0.208\n",
      "[2024-02-12 18:05:31,262] Epoch: 48 | Ending LR: 0.000022 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:44<00:00,  3.81it/s]\n",
      "[2024-02-12 18:06:20,223] Epoch: 49 | FT  | Train Loss: 0.18186 | Val Loss: 0.21471\n",
      "[2024-02-12 18:06:20,224] Epoch: 49 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:06:20,225] Epoch: 49 | Ph  | Train Loss: 0.182 | Val Loss: 0.215\n",
      "[2024-02-12 18:06:20,225] Epoch: 49 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.79it/s]\n",
      "[2024-02-12 18:07:09,732] Saving improved model after Val Loss improved from 0.20771 to 0.20716\n",
      "[2024-02-12 18:07:09,845] Epoch: 50 | FT  | Train Loss: 0.18188 | Val Loss: 0.20716\n",
      "[2024-02-12 18:07:09,846] Epoch: 50 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:07:09,847] Epoch: 50 | Ph  | Train Loss: 0.182 | Val Loss: 0.207\n",
      "[2024-02-12 18:07:09,848] Epoch: 50 | Ending LR: 0.000026 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:44<00:00,  3.81it/s]\n",
      "[2024-02-12 18:07:59,006] Epoch: 51 | FT  | Train Loss: 0.18149 | Val Loss: 0.20820\n",
      "[2024-02-12 18:07:59,007] Epoch: 51 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:07:59,008] Epoch: 51 | Ph  | Train Loss: 0.181 | Val Loss: 0.208\n",
      "[2024-02-12 18:07:59,009] Epoch: 51 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.75it/s]\n",
      "[2024-02-12 18:08:48,696] Epoch: 52 | FT  | Train Loss: 0.18174 | Val Loss: 0.20960\n",
      "[2024-02-12 18:08:48,697] Epoch: 52 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:08:48,698] Epoch: 52 | Ph  | Train Loss: 0.182 | Val Loss: 0.210\n",
      "[2024-02-12 18:08:48,699] Epoch: 52 | Ending LR: 0.000029 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.80it/s]\n",
      "[2024-02-12 18:09:37,590] Epoch: 53 | FT  | Train Loss: 0.18142 | Val Loss: 0.20760\n",
      "[2024-02-12 18:09:37,594] Epoch: 53 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:09:37,594] Epoch: 53 | Ph  | Train Loss: 0.181 | Val Loss: 0.208\n",
      "[2024-02-12 18:09:37,595] Epoch: 53 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:43<00:00,  3.89it/s]\n",
      "[2024-02-12 18:10:25,699] Epoch: 54 | FT  | Train Loss: 0.18110 | Val Loss: 0.21252\n",
      "[2024-02-12 18:10:25,703] Epoch: 54 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:10:25,703] Epoch: 54 | Ph  | Train Loss: 0.181 | Val Loss: 0.213\n",
      "[2024-02-12 18:10:25,704] Epoch: 54 | Ending LR: 0.000029 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:44<00:00,  3.83it/s]\n",
      "[2024-02-12 18:11:14,414] Saving improved model after Val Loss improved from 0.20716 to 0.20684\n",
      "[2024-02-12 18:11:14,520] Epoch: 55 | FT  | Train Loss: 0.18030 | Val Loss: 0.20684\n",
      "[2024-02-12 18:11:14,521] Epoch: 55 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:11:14,521] Epoch: 55 | Ph  | Train Loss: 0.180 | Val Loss: 0.207\n",
      "[2024-02-12 18:11:14,522] Epoch: 55 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:46<00:00,  3.72it/s]\n",
      "[2024-02-12 18:12:05,023] Epoch: 56 | FT  | Train Loss: 0.17915 | Val Loss: 0.20808\n",
      "[2024-02-12 18:12:05,025] Epoch: 56 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:12:05,025] Epoch: 56 | Ph  | Train Loss: 0.179 | Val Loss: 0.208\n",
      "[2024-02-12 18:12:05,026] Epoch: 56 | Ending LR: 0.000026 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.72it/s]\n",
      "[2024-02-12 18:12:55,166] Epoch: 57 | FT  | Train Loss: 0.17862 | Val Loss: 0.20798\n",
      "[2024-02-12 18:12:55,168] Epoch: 57 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:12:55,168] Epoch: 57 | Ph  | Train Loss: 0.179 | Val Loss: 0.208\n",
      "[2024-02-12 18:12:55,169] Epoch: 57 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.78it/s]\n",
      "[2024-02-12 18:13:44,434] Epoch: 58 | FT  | Train Loss: 0.17821 | Val Loss: 0.20714\n",
      "[2024-02-12 18:13:44,435] Epoch: 58 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:13:44,436] Epoch: 58 | Ph  | Train Loss: 0.178 | Val Loss: 0.207\n",
      "[2024-02-12 18:13:44,437] Epoch: 58 | Ending LR: 0.000022 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171/171 [00:45<00:00,  3.80it/s]\n",
      "[2024-02-12 18:14:33,649] Epoch: 59 | FT  | Train Loss: 0.17755 | Val Loss: 0.20838\n",
      "[2024-02-12 18:14:33,650] Epoch: 59 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:14:33,651] Epoch: 59 | Ph  | Train Loss: 0.178 | Val Loss: 0.208\n",
      "[2024-02-12 18:14:33,651] Epoch: 59 | Ending LR: 0.000020 \n",
      "[2024-02-12 18:14:33,775] Decimating dataset to 0.6 of the original size...\n",
      "[2024-02-12 18:14:33,846] Using DataParallel with 2 devices.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:39<00:00,  3.73it/s]\n",
      "[2024-02-12 18:15:17,084] Saving improved model after Val Loss improved from inf to 0.51328\n",
      "[2024-02-12 18:15:17,116] Epoch: 0 | FT  | Train Loss: 0.59687 | Val Loss: 0.51328\n",
      "[2024-02-12 18:15:17,116] Epoch: 0 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:15:17,117] Epoch: 0 | Ph  | Train Loss: 0.597 | Val Loss: 0.513\n",
      "[2024-02-12 18:15:17,117] Epoch: 0 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.81it/s]\n",
      "[2024-02-12 18:15:59,444] Saving improved model after Val Loss improved from 0.51328 to 0.39588\n",
      "[2024-02-12 18:15:59,527] Epoch: 1 | FT  | Train Loss: 0.43598 | Val Loss: 0.39588\n",
      "[2024-02-12 18:15:59,528] Epoch: 1 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:15:59,529] Epoch: 1 | Ph  | Train Loss: 0.436 | Val Loss: 0.396\n",
      "[2024-02-12 18:15:59,530] Epoch: 1 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.83it/s]\n",
      "[2024-02-12 18:16:41,601] Saving improved model after Val Loss improved from 0.39588 to 0.35624\n",
      "[2024-02-12 18:16:41,690] Epoch: 2 | FT  | Train Loss: 0.36200 | Val Loss: 0.35624\n",
      "[2024-02-12 18:16:41,691] Epoch: 2 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:16:41,692] Epoch: 2 | Ph  | Train Loss: 0.362 | Val Loss: 0.356\n",
      "[2024-02-12 18:16:41,693] Epoch: 2 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.82it/s]\n",
      "[2024-02-12 18:17:23,946] Saving improved model after Val Loss improved from 0.35624 to 0.32408\n",
      "[2024-02-12 18:17:24,027] Epoch: 3 | FT  | Train Loss: 0.32464 | Val Loss: 0.32408\n",
      "[2024-02-12 18:17:24,028] Epoch: 3 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:17:24,029] Epoch: 3 | Ph  | Train Loss: 0.325 | Val Loss: 0.324\n",
      "[2024-02-12 18:17:24,030] Epoch: 3 | Ending LR: 0.000140 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.81it/s]\n",
      "[2024-02-12 18:18:06,319] Saving improved model after Val Loss improved from 0.32408 to 0.30756\n",
      "[2024-02-12 18:18:06,404] Epoch: 4 | FT  | Train Loss: 0.30037 | Val Loss: 0.30756\n",
      "[2024-02-12 18:18:06,405] Epoch: 4 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:18:06,406] Epoch: 4 | Ph  | Train Loss: 0.300 | Val Loss: 0.308\n",
      "[2024-02-12 18:18:06,407] Epoch: 4 | Ending LR: 0.000170 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:37<00:00,  3.86it/s]\n",
      "[2024-02-12 18:18:48,448] Saving improved model after Val Loss improved from 0.30756 to 0.28960\n",
      "[2024-02-12 18:18:48,529] Epoch: 5 | FT  | Train Loss: 0.28092 | Val Loss: 0.28960\n",
      "[2024-02-12 18:18:48,530] Epoch: 5 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:18:48,531] Epoch: 5 | Ph  | Train Loss: 0.281 | Val Loss: 0.290\n",
      "[2024-02-12 18:18:48,531] Epoch: 5 | Ending LR: 0.000200 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:37<00:00,  3.85it/s]\n",
      "[2024-02-12 18:19:30,677] Saving improved model after Val Loss improved from 0.28960 to 0.26896\n",
      "[2024-02-12 18:19:30,764] Epoch: 6 | FT  | Train Loss: 0.26835 | Val Loss: 0.26896\n",
      "[2024-02-12 18:19:30,765] Epoch: 6 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:19:30,765] Epoch: 6 | Ph  | Train Loss: 0.268 | Val Loss: 0.269\n",
      "[2024-02-12 18:19:30,766] Epoch: 6 | Ending LR: 0.000170 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.82it/s]\n",
      "[2024-02-12 18:20:13,267] Saving improved model after Val Loss improved from 0.26896 to 0.25965\n",
      "[2024-02-12 18:20:13,371] Epoch: 7 | FT  | Train Loss: 0.25792 | Val Loss: 0.25965\n",
      "[2024-02-12 18:20:13,372] Epoch: 7 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:20:13,373] Epoch: 7 | Ph  | Train Loss: 0.258 | Val Loss: 0.260\n",
      "[2024-02-12 18:20:13,373] Epoch: 7 | Ending LR: 0.000140 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.81it/s]\n",
      "[2024-02-12 18:20:55,733] Saving improved model after Val Loss improved from 0.25965 to 0.25304\n",
      "[2024-02-12 18:20:55,815] Epoch: 8 | FT  | Train Loss: 0.24957 | Val Loss: 0.25304\n",
      "[2024-02-12 18:20:55,816] Epoch: 8 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:20:55,816] Epoch: 8 | Ph  | Train Loss: 0.250 | Val Loss: 0.253\n",
      "[2024-02-12 18:20:55,817] Epoch: 8 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.81it/s]\n",
      "[2024-02-12 18:21:38,190] Saving improved model after Val Loss improved from 0.25304 to 0.24840\n",
      "[2024-02-12 18:21:38,271] Epoch: 9 | FT  | Train Loss: 0.24244 | Val Loss: 0.24840\n",
      "[2024-02-12 18:21:38,272] Epoch: 9 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:21:38,272] Epoch: 9 | Ph  | Train Loss: 0.242 | Val Loss: 0.248\n",
      "[2024-02-12 18:21:38,273] Epoch: 9 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.82it/s]\n",
      "[2024-02-12 18:22:20,620] Saving improved model after Val Loss improved from 0.24840 to 0.24585\n",
      "[2024-02-12 18:22:20,703] Epoch: 10 | FT  | Train Loss: 0.23746 | Val Loss: 0.24585\n",
      "[2024-02-12 18:22:20,704] Epoch: 10 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:22:20,704] Epoch: 10 | Ph  | Train Loss: 0.237 | Val Loss: 0.246\n",
      "[2024-02-12 18:22:20,705] Epoch: 10 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:37<00:00,  3.86it/s]\n",
      "[2024-02-12 18:23:02,628] Saving improved model after Val Loss improved from 0.24585 to 0.24092\n",
      "[2024-02-12 18:23:02,715] Epoch: 11 | FT  | Train Loss: 0.23383 | Val Loss: 0.24092\n",
      "[2024-02-12 18:23:02,715] Epoch: 11 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:23:02,716] Epoch: 11 | Ph  | Train Loss: 0.234 | Val Loss: 0.241\n",
      "[2024-02-12 18:23:02,716] Epoch: 11 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.81it/s]\n",
      "[2024-02-12 18:23:45,061] Epoch: 12 | FT  | Train Loss: 0.23148 | Val Loss: 0.24213\n",
      "[2024-02-12 18:23:45,062] Epoch: 12 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:23:45,063] Epoch: 12 | Ph  | Train Loss: 0.231 | Val Loss: 0.242\n",
      "[2024-02-12 18:23:45,063] Epoch: 12 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.76it/s]\n",
      "[2024-02-12 18:24:27,903] Epoch: 13 | FT  | Train Loss: 0.23179 | Val Loss: 0.24224\n",
      "[2024-02-12 18:24:27,905] Epoch: 13 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:24:27,905] Epoch: 13 | Ph  | Train Loss: 0.232 | Val Loss: 0.242\n",
      "[2024-02-12 18:24:27,906] Epoch: 13 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.75it/s]\n",
      "[2024-02-12 18:25:10,741] Epoch: 14 | FT  | Train Loss: 0.23131 | Val Loss: 0.24966\n",
      "[2024-02-12 18:25:10,744] Epoch: 14 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:25:10,745] Epoch: 14 | Ph  | Train Loss: 0.231 | Val Loss: 0.250\n",
      "[2024-02-12 18:25:10,746] Epoch: 14 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.76it/s]\n",
      "[2024-02-12 18:25:53,637] Epoch: 15 | FT  | Train Loss: 0.23059 | Val Loss: 0.24270\n",
      "[2024-02-12 18:25:53,638] Epoch: 15 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:25:53,639] Epoch: 15 | Ph  | Train Loss: 0.231 | Val Loss: 0.243\n",
      "[2024-02-12 18:25:53,639] Epoch: 15 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.80it/s]\n",
      "[2024-02-12 18:26:36,028] Saving improved model after Val Loss improved from 0.24092 to 0.24017\n",
      "[2024-02-12 18:26:36,154] Epoch: 16 | FT  | Train Loss: 0.22998 | Val Loss: 0.24017\n",
      "[2024-02-12 18:26:36,155] Epoch: 16 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:26:36,156] Epoch: 16 | Ph  | Train Loss: 0.230 | Val Loss: 0.240\n",
      "[2024-02-12 18:26:36,156] Epoch: 16 | Ending LR: 0.000095 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:39<00:00,  3.74it/s]\n",
      "[2024-02-12 18:27:19,336] Saving improved model after Val Loss improved from 0.24017 to 0.23731\n",
      "[2024-02-12 18:27:19,443] Epoch: 17 | FT  | Train Loss: 0.22861 | Val Loss: 0.23731\n",
      "[2024-02-12 18:27:19,444] Epoch: 17 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:27:19,445] Epoch: 17 | Ph  | Train Loss: 0.229 | Val Loss: 0.237\n",
      "[2024-02-12 18:27:19,446] Epoch: 17 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:39<00:00,  3.74it/s]\n",
      "[2024-02-12 18:28:02,543] Epoch: 18 | FT  | Train Loss: 0.22529 | Val Loss: 0.24166\n",
      "[2024-02-12 18:28:02,544] Epoch: 18 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:28:02,545] Epoch: 18 | Ph  | Train Loss: 0.225 | Val Loss: 0.242\n",
      "[2024-02-12 18:28:02,546] Epoch: 18 | Ending LR: 0.000095 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.77it/s]\n",
      "[2024-02-12 18:28:45,768] Saving improved model after Val Loss improved from 0.23731 to 0.23345\n",
      "[2024-02-12 18:28:45,866] Epoch: 19 | FT  | Train Loss: 0.22157 | Val Loss: 0.23345\n",
      "[2024-02-12 18:28:45,866] Epoch: 19 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:28:45,867] Epoch: 19 | Ph  | Train Loss: 0.222 | Val Loss: 0.233\n",
      "[2024-02-12 18:28:45,867] Epoch: 19 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.75it/s]\n",
      "[2024-02-12 18:29:28,996] Saving improved model after Val Loss improved from 0.23345 to 0.23193\n",
      "[2024-02-12 18:29:29,090] Epoch: 20 | FT  | Train Loss: 0.21822 | Val Loss: 0.23193\n",
      "[2024-02-12 18:29:29,092] Epoch: 20 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:29:29,092] Epoch: 20 | Ph  | Train Loss: 0.218 | Val Loss: 0.232\n",
      "[2024-02-12 18:29:29,093] Epoch: 20 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:37<00:00,  3.87it/s]\n",
      "[2024-02-12 18:30:10,709] Saving improved model after Val Loss improved from 0.23193 to 0.22819\n",
      "[2024-02-12 18:30:10,835] Epoch: 21 | FT  | Train Loss: 0.21516 | Val Loss: 0.22819\n",
      "[2024-02-12 18:30:10,836] Epoch: 21 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:30:10,837] Epoch: 21 | Ph  | Train Loss: 0.215 | Val Loss: 0.228\n",
      "[2024-02-12 18:30:10,838] Epoch: 21 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.80it/s]\n",
      "[2024-02-12 18:30:53,116] Saving improved model after Val Loss improved from 0.22819 to 0.22530\n",
      "[2024-02-12 18:30:53,214] Epoch: 22 | FT  | Train Loss: 0.21301 | Val Loss: 0.22530\n",
      "[2024-02-12 18:30:53,215] Epoch: 22 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:30:53,216] Epoch: 22 | Ph  | Train Loss: 0.213 | Val Loss: 0.225\n",
      "[2024-02-12 18:30:53,217] Epoch: 22 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.76it/s]\n",
      "[2024-02-12 18:31:36,704] Saving improved model after Val Loss improved from 0.22530 to 0.22353\n",
      "[2024-02-12 18:31:36,798] Epoch: 23 | FT  | Train Loss: 0.21055 | Val Loss: 0.22353\n",
      "[2024-02-12 18:31:36,799] Epoch: 23 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:31:36,800] Epoch: 23 | Ph  | Train Loss: 0.211 | Val Loss: 0.224\n",
      "[2024-02-12 18:31:36,800] Epoch: 23 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:37<00:00,  3.85it/s]\n",
      "[2024-02-12 18:32:18,687] Epoch: 24 | FT  | Train Loss: 0.20925 | Val Loss: 0.22407\n",
      "[2024-02-12 18:32:18,689] Epoch: 24 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:32:18,690] Epoch: 24 | Ph  | Train Loss: 0.209 | Val Loss: 0.224\n",
      "[2024-02-12 18:32:18,691] Epoch: 24 | Ending LR: 0.000028 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.84it/s]\n",
      "[2024-02-12 18:33:00,742] Epoch: 25 | FT  | Train Loss: 0.20933 | Val Loss: 0.22422\n",
      "[2024-02-12 18:33:00,744] Epoch: 25 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:33:00,744] Epoch: 25 | Ph  | Train Loss: 0.209 | Val Loss: 0.224\n",
      "[2024-02-12 18:33:00,745] Epoch: 25 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.80it/s]\n",
      "[2024-02-12 18:33:43,173] Epoch: 26 | FT  | Train Loss: 0.20916 | Val Loss: 0.22534\n",
      "[2024-02-12 18:33:43,174] Epoch: 26 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:33:43,175] Epoch: 26 | Ph  | Train Loss: 0.209 | Val Loss: 0.225\n",
      "[2024-02-12 18:33:43,175] Epoch: 26 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.82it/s]\n",
      "[2024-02-12 18:34:25,493] Epoch: 27 | FT  | Train Loss: 0.20944 | Val Loss: 0.22614\n",
      "[2024-02-12 18:34:25,495] Epoch: 27 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:34:25,496] Epoch: 27 | Ph  | Train Loss: 0.209 | Val Loss: 0.226\n",
      "[2024-02-12 18:34:25,497] Epoch: 27 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.79it/s]\n",
      "[2024-02-12 18:35:08,017] Epoch: 28 | FT  | Train Loss: 0.20885 | Val Loss: 0.22547\n",
      "[2024-02-12 18:35:08,018] Epoch: 28 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:35:08,019] Epoch: 28 | Ph  | Train Loss: 0.209 | Val Loss: 0.225\n",
      "[2024-02-12 18:35:08,020] Epoch: 28 | Ending LR: 0.000057 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.80it/s]\n",
      "[2024-02-12 18:35:50,695] Epoch: 29 | FT  | Train Loss: 0.20851 | Val Loss: 0.23141\n",
      "[2024-02-12 18:35:50,697] Epoch: 29 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:35:50,697] Epoch: 29 | Ph  | Train Loss: 0.209 | Val Loss: 0.231\n",
      "[2024-02-12 18:35:50,698] Epoch: 29 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:39<00:00,  3.70it/s]\n",
      "[2024-02-12 18:36:34,421] Epoch: 30 | FT  | Train Loss: 0.20713 | Val Loss: 0.22702\n",
      "[2024-02-12 18:36:34,422] Epoch: 30 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:36:34,423] Epoch: 30 | Ph  | Train Loss: 0.207 | Val Loss: 0.227\n",
      "[2024-02-12 18:36:34,424] Epoch: 30 | Ending LR: 0.000057 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.76it/s]\n",
      "[2024-02-12 18:37:17,601] Saving improved model after Val Loss improved from 0.22353 to 0.22270\n",
      "[2024-02-12 18:37:17,699] Epoch: 31 | FT  | Train Loss: 0.20507 | Val Loss: 0.22270\n",
      "[2024-02-12 18:37:17,700] Epoch: 31 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:37:17,701] Epoch: 31 | Ph  | Train Loss: 0.205 | Val Loss: 0.223\n",
      "[2024-02-12 18:37:17,703] Epoch: 31 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.82it/s]\n",
      "[2024-02-12 18:37:59,948] Saving improved model after Val Loss improved from 0.22270 to 0.22011\n",
      "[2024-02-12 18:38:00,042] Epoch: 32 | FT  | Train Loss: 0.20280 | Val Loss: 0.22011\n",
      "[2024-02-12 18:38:00,043] Epoch: 32 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:38:00,044] Epoch: 32 | Ph  | Train Loss: 0.203 | Val Loss: 0.220\n",
      "[2024-02-12 18:38:00,044] Epoch: 32 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.79it/s]\n",
      "[2024-02-12 18:38:42,526] Saving improved model after Val Loss improved from 0.22011 to 0.21922\n",
      "[2024-02-12 18:38:42,629] Epoch: 33 | FT  | Train Loss: 0.20103 | Val Loss: 0.21922\n",
      "[2024-02-12 18:38:42,630] Epoch: 33 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:38:42,630] Epoch: 33 | Ph  | Train Loss: 0.201 | Val Loss: 0.219\n",
      "[2024-02-12 18:38:42,631] Epoch: 33 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:39<00:00,  3.74it/s]\n",
      "[2024-02-12 18:39:25,916] Saving improved model after Val Loss improved from 0.21922 to 0.21855\n",
      "[2024-02-12 18:39:26,005] Epoch: 34 | FT  | Train Loss: 0.19966 | Val Loss: 0.21855\n",
      "[2024-02-12 18:39:26,006] Epoch: 34 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:39:26,006] Epoch: 34 | Ph  | Train Loss: 0.200 | Val Loss: 0.219\n",
      "[2024-02-12 18:39:26,007] Epoch: 34 | Ending LR: 0.000028 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.76it/s]\n",
      "[2024-02-12 18:40:08,775] Saving improved model after Val Loss improved from 0.21855 to 0.21757\n",
      "[2024-02-12 18:40:08,872] Epoch: 35 | FT  | Train Loss: 0.19814 | Val Loss: 0.21757\n",
      "[2024-02-12 18:40:08,873] Epoch: 35 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:40:08,874] Epoch: 35 | Ph  | Train Loss: 0.198 | Val Loss: 0.218\n",
      "[2024-02-12 18:40:08,874] Epoch: 35 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.81it/s]\n",
      "[2024-02-12 18:40:51,351] Epoch: 36 | FT  | Train Loss: 0.19785 | Val Loss: 0.21799\n",
      "[2024-02-12 18:40:51,352] Epoch: 36 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:40:51,353] Epoch: 36 | Ph  | Train Loss: 0.198 | Val Loss: 0.218\n",
      "[2024-02-12 18:40:51,354] Epoch: 36 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.78it/s]\n",
      "[2024-02-12 18:41:34,225] Saving improved model after Val Loss improved from 0.21757 to 0.21746\n",
      "[2024-02-12 18:41:34,328] Epoch: 37 | FT  | Train Loss: 0.19748 | Val Loss: 0.21746\n",
      "[2024-02-12 18:41:34,329] Epoch: 37 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:41:34,330] Epoch: 37 | Ph  | Train Loss: 0.197 | Val Loss: 0.217\n",
      "[2024-02-12 18:41:34,330] Epoch: 37 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.77it/s]\n",
      "[2024-02-12 18:42:17,234] Epoch: 38 | FT  | Train Loss: 0.19721 | Val Loss: 0.22673\n",
      "[2024-02-12 18:42:17,235] Epoch: 38 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:42:17,236] Epoch: 38 | Ph  | Train Loss: 0.197 | Val Loss: 0.227\n",
      "[2024-02-12 18:42:17,237] Epoch: 38 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.77it/s]\n",
      "[2024-02-12 18:43:00,367] Epoch: 39 | FT  | Train Loss: 0.19728 | Val Loss: 0.22109\n",
      "[2024-02-12 18:43:00,367] Epoch: 39 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:43:00,368] Epoch: 39 | Ph  | Train Loss: 0.197 | Val Loss: 0.221\n",
      "[2024-02-12 18:43:00,368] Epoch: 39 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.76it/s]\n",
      "[2024-02-12 18:43:43,100] Epoch: 40 | FT  | Train Loss: 0.19717 | Val Loss: 0.22032\n",
      "[2024-02-12 18:43:43,101] Epoch: 40 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:43:43,102] Epoch: 40 | Ph  | Train Loss: 0.197 | Val Loss: 0.220\n",
      "[2024-02-12 18:43:43,102] Epoch: 40 | Ending LR: 0.000039 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.77it/s]\n",
      "[2024-02-12 18:44:25,858] Epoch: 41 | FT  | Train Loss: 0.19667 | Val Loss: 0.22162\n",
      "[2024-02-12 18:44:25,859] Epoch: 41 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:44:25,860] Epoch: 41 | Ph  | Train Loss: 0.197 | Val Loss: 0.222\n",
      "[2024-02-12 18:44:25,861] Epoch: 41 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.78it/s]\n",
      "[2024-02-12 18:45:09,023] Epoch: 42 | FT  | Train Loss: 0.19641 | Val Loss: 0.21993\n",
      "[2024-02-12 18:45:09,024] Epoch: 42 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:45:09,025] Epoch: 42 | Ph  | Train Loss: 0.196 | Val Loss: 0.220\n",
      "[2024-02-12 18:45:09,026] Epoch: 42 | Ending LR: 0.000039 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.77it/s]\n",
      "[2024-02-12 18:45:51,768] Saving improved model after Val Loss improved from 0.21746 to 0.21686\n",
      "[2024-02-12 18:45:51,875] Epoch: 43 | FT  | Train Loss: 0.19506 | Val Loss: 0.21686\n",
      "[2024-02-12 18:45:51,876] Epoch: 43 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:45:51,876] Epoch: 43 | Ph  | Train Loss: 0.195 | Val Loss: 0.217\n",
      "[2024-02-12 18:45:51,878] Epoch: 43 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.82it/s]\n",
      "[2024-02-12 18:46:34,493] Epoch: 44 | FT  | Train Loss: 0.19350 | Val Loss: 0.21740\n",
      "[2024-02-12 18:46:34,495] Epoch: 44 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:46:34,495] Epoch: 44 | Ph  | Train Loss: 0.193 | Val Loss: 0.217\n",
      "[2024-02-12 18:46:34,497] Epoch: 44 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.78it/s]\n",
      "[2024-02-12 18:47:17,541] Epoch: 45 | FT  | Train Loss: 0.19279 | Val Loss: 0.21790\n",
      "[2024-02-12 18:47:17,543] Epoch: 45 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:47:17,543] Epoch: 45 | Ph  | Train Loss: 0.193 | Val Loss: 0.218\n",
      "[2024-02-12 18:47:17,544] Epoch: 45 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.80it/s]\n",
      "[2024-02-12 18:48:00,345] Saving improved model after Val Loss improved from 0.21686 to 0.21621\n",
      "[2024-02-12 18:48:00,444] Epoch: 46 | FT  | Train Loss: 0.19180 | Val Loss: 0.21621\n",
      "[2024-02-12 18:48:00,445] Epoch: 46 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:48:00,445] Epoch: 46 | Ph  | Train Loss: 0.192 | Val Loss: 0.216\n",
      "[2024-02-12 18:48:00,446] Epoch: 46 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.78it/s]\n",
      "[2024-02-12 18:48:43,416] Saving improved model after Val Loss improved from 0.21621 to 0.21456\n",
      "[2024-02-12 18:48:43,511] Epoch: 47 | FT  | Train Loss: 0.19090 | Val Loss: 0.21456\n",
      "[2024-02-12 18:48:43,512] Epoch: 47 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:48:43,513] Epoch: 47 | Ph  | Train Loss: 0.191 | Val Loss: 0.215\n",
      "[2024-02-12 18:48:43,513] Epoch: 47 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:39<00:00,  3.69it/s]\n",
      "[2024-02-12 18:49:27,283] Epoch: 48 | FT  | Train Loss: 0.19024 | Val Loss: 0.21763\n",
      "[2024-02-12 18:49:27,284] Epoch: 48 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:49:27,285] Epoch: 48 | Ph  | Train Loss: 0.190 | Val Loss: 0.218\n",
      "[2024-02-12 18:49:27,285] Epoch: 48 | Ending LR: 0.000022 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.77it/s]\n",
      "[2024-02-12 18:50:10,308] Epoch: 49 | FT  | Train Loss: 0.19001 | Val Loss: 0.21584\n",
      "[2024-02-12 18:50:10,310] Epoch: 49 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:50:10,311] Epoch: 49 | Ph  | Train Loss: 0.190 | Val Loss: 0.216\n",
      "[2024-02-12 18:50:10,311] Epoch: 49 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:39<00:00,  3.74it/s]\n",
      "[2024-02-12 18:50:53,445] Epoch: 50 | FT  | Train Loss: 0.18956 | Val Loss: 0.21562\n",
      "[2024-02-12 18:50:53,446] Epoch: 50 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:50:53,447] Epoch: 50 | Ph  | Train Loss: 0.190 | Val Loss: 0.216\n",
      "[2024-02-12 18:50:53,448] Epoch: 50 | Ending LR: 0.000026 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.82it/s]\n",
      "[2024-02-12 18:51:35,783] Epoch: 51 | FT  | Train Loss: 0.18989 | Val Loss: 0.21644\n",
      "[2024-02-12 18:51:35,786] Epoch: 51 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:51:35,786] Epoch: 51 | Ph  | Train Loss: 0.190 | Val Loss: 0.216\n",
      "[2024-02-12 18:51:35,787] Epoch: 51 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:37<00:00,  3.84it/s]\n",
      "[2024-02-12 18:52:17,936] Epoch: 52 | FT  | Train Loss: 0.18954 | Val Loss: 0.21842\n",
      "[2024-02-12 18:52:17,937] Epoch: 52 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:52:17,938] Epoch: 52 | Ph  | Train Loss: 0.190 | Val Loss: 0.218\n",
      "[2024-02-12 18:52:17,938] Epoch: 52 | Ending LR: 0.000029 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:39<00:00,  3.72it/s]\n",
      "[2024-02-12 18:53:01,339] Epoch: 53 | FT  | Train Loss: 0.18939 | Val Loss: 0.21499\n",
      "[2024-02-12 18:53:01,341] Epoch: 53 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:53:01,342] Epoch: 53 | Ph  | Train Loss: 0.189 | Val Loss: 0.215\n",
      "[2024-02-12 18:53:01,342] Epoch: 53 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.83it/s]\n",
      "[2024-02-12 18:53:43,450] Epoch: 54 | FT  | Train Loss: 0.18897 | Val Loss: 0.21722\n",
      "[2024-02-12 18:53:43,451] Epoch: 54 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:53:43,452] Epoch: 54 | Ph  | Train Loss: 0.189 | Val Loss: 0.217\n",
      "[2024-02-12 18:53:43,452] Epoch: 54 | Ending LR: 0.000029 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:39<00:00,  3.73it/s]\n",
      "[2024-02-12 18:54:26,766] Epoch: 55 | FT  | Train Loss: 0.18841 | Val Loss: 0.21900\n",
      "[2024-02-12 18:54:26,768] Epoch: 55 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:54:26,768] Epoch: 55 | Ph  | Train Loss: 0.188 | Val Loss: 0.219\n",
      "[2024-02-12 18:54:26,769] Epoch: 55 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.77it/s]\n",
      "[2024-02-12 18:55:09,667] Epoch: 56 | FT  | Train Loss: 0.18757 | Val Loss: 0.21480\n",
      "[2024-02-12 18:55:09,668] Epoch: 56 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:55:09,669] Epoch: 56 | Ph  | Train Loss: 0.188 | Val Loss: 0.215\n",
      "[2024-02-12 18:55:09,670] Epoch: 56 | Ending LR: 0.000026 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.75it/s]\n",
      "[2024-02-12 18:55:52,744] Saving improved model after Val Loss improved from 0.21456 to 0.21361\n",
      "[2024-02-12 18:55:52,796] Epoch: 57 | FT  | Train Loss: 0.18650 | Val Loss: 0.21361\n",
      "[2024-02-12 18:55:52,797] Epoch: 57 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:55:52,798] Epoch: 57 | Ph  | Train Loss: 0.187 | Val Loss: 0.214\n",
      "[2024-02-12 18:55:52,798] Epoch: 57 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.77it/s]\n",
      "[2024-02-12 18:56:35,733] Saving improved model after Val Loss improved from 0.21361 to 0.21328\n",
      "[2024-02-12 18:56:35,839] Epoch: 58 | FT  | Train Loss: 0.18610 | Val Loss: 0.21328\n",
      "[2024-02-12 18:56:35,839] Epoch: 58 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:56:35,840] Epoch: 58 | Ph  | Train Loss: 0.186 | Val Loss: 0.213\n",
      "[2024-02-12 18:56:35,840] Epoch: 58 | Ending LR: 0.000022 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.79it/s]\n",
      "[2024-02-12 18:57:18,744] Epoch: 59 | FT  | Train Loss: 0.18546 | Val Loss: 0.21471\n",
      "[2024-02-12 18:57:18,745] Epoch: 59 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:57:18,746] Epoch: 59 | Ph  | Train Loss: 0.185 | Val Loss: 0.215\n",
      "[2024-02-12 18:57:18,747] Epoch: 59 | Ending LR: 0.000020 \n",
      "[2024-02-12 18:57:18,876] Decimating dataset to 0.5 of the original size...\n",
      "[2024-02-12 18:57:18,940] Using DataParallel with 2 devices.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.76it/s]\n",
      "[2024-02-12 18:57:55,601] Saving improved model after Val Loss improved from inf to 0.53149\n",
      "[2024-02-12 18:57:55,633] Epoch: 0 | FT  | Train Loss: 0.60812 | Val Loss: 0.53149\n",
      "[2024-02-12 18:57:55,634] Epoch: 0 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:57:55,635] Epoch: 0 | Ph  | Train Loss: 0.608 | Val Loss: 0.531\n",
      "[2024-02-12 18:57:55,635] Epoch: 0 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.77it/s]\n",
      "[2024-02-12 18:58:32,306] Saving improved model after Val Loss improved from 0.53149 to 0.41428\n",
      "[2024-02-12 18:58:32,383] Epoch: 1 | FT  | Train Loss: 0.46016 | Val Loss: 0.41428\n",
      "[2024-02-12 18:58:32,384] Epoch: 1 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:58:32,385] Epoch: 1 | Ph  | Train Loss: 0.460 | Val Loss: 0.414\n",
      "[2024-02-12 18:58:32,385] Epoch: 1 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.77it/s]\n",
      "[2024-02-12 18:59:08,877] Saving improved model after Val Loss improved from 0.41428 to 0.36923\n",
      "[2024-02-12 18:59:08,953] Epoch: 2 | FT  | Train Loss: 0.38125 | Val Loss: 0.36923\n",
      "[2024-02-12 18:59:08,954] Epoch: 2 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:59:08,955] Epoch: 2 | Ph  | Train Loss: 0.381 | Val Loss: 0.369\n",
      "[2024-02-12 18:59:08,955] Epoch: 2 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.75it/s]\n",
      "[2024-02-12 18:59:45,603] Saving improved model after Val Loss improved from 0.36923 to 0.33849\n",
      "[2024-02-12 18:59:45,679] Epoch: 3 | FT  | Train Loss: 0.34031 | Val Loss: 0.33849\n",
      "[2024-02-12 18:59:45,680] Epoch: 3 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 18:59:45,680] Epoch: 3 | Ph  | Train Loss: 0.340 | Val Loss: 0.338\n",
      "[2024-02-12 18:59:45,681] Epoch: 3 | Ending LR: 0.000140 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.75it/s]\n",
      "[2024-02-12 19:00:22,235] Saving improved model after Val Loss improved from 0.33849 to 0.31318\n",
      "[2024-02-12 19:00:22,307] Epoch: 4 | FT  | Train Loss: 0.31061 | Val Loss: 0.31318\n",
      "[2024-02-12 19:00:22,307] Epoch: 4 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:00:22,308] Epoch: 4 | Ph  | Train Loss: 0.311 | Val Loss: 0.313\n",
      "[2024-02-12 19:00:22,308] Epoch: 4 | Ending LR: 0.000170 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.80it/s]\n",
      "[2024-02-12 19:00:58,586] Saving improved model after Val Loss improved from 0.31318 to 0.29298\n",
      "[2024-02-12 19:00:58,663] Epoch: 5 | FT  | Train Loss: 0.29248 | Val Loss: 0.29298\n",
      "[2024-02-12 19:00:58,664] Epoch: 5 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:00:58,665] Epoch: 5 | Ph  | Train Loss: 0.292 | Val Loss: 0.293\n",
      "[2024-02-12 19:00:58,665] Epoch: 5 | Ending LR: 0.000200 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.76it/s]\n",
      "[2024-02-12 19:01:35,236] Saving improved model after Val Loss improved from 0.29298 to 0.27483\n",
      "[2024-02-12 19:01:35,349] Epoch: 6 | FT  | Train Loss: 0.27531 | Val Loss: 0.27483\n",
      "[2024-02-12 19:01:35,351] Epoch: 6 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:01:35,352] Epoch: 6 | Ph  | Train Loss: 0.275 | Val Loss: 0.275\n",
      "[2024-02-12 19:01:35,353] Epoch: 6 | Ending LR: 0.000170 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.76it/s]\n",
      "[2024-02-12 19:02:12,128] Saving improved model after Val Loss improved from 0.27483 to 0.26866\n",
      "[2024-02-12 19:02:12,215] Epoch: 7 | FT  | Train Loss: 0.26350 | Val Loss: 0.26866\n",
      "[2024-02-12 19:02:12,216] Epoch: 7 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:02:12,216] Epoch: 7 | Ph  | Train Loss: 0.263 | Val Loss: 0.269\n",
      "[2024-02-12 19:02:12,217] Epoch: 7 | Ending LR: 0.000140 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.75it/s]\n",
      "[2024-02-12 19:02:48,887] Saving improved model after Val Loss improved from 0.26866 to 0.26216\n",
      "[2024-02-12 19:02:48,980] Epoch: 8 | FT  | Train Loss: 0.25514 | Val Loss: 0.26216\n",
      "[2024-02-12 19:02:48,981] Epoch: 8 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:02:48,982] Epoch: 8 | Ph  | Train Loss: 0.255 | Val Loss: 0.262\n",
      "[2024-02-12 19:02:48,982] Epoch: 8 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.75it/s]\n",
      "[2024-02-12 19:03:25,588] Saving improved model after Val Loss improved from 0.26216 to 0.25687\n",
      "[2024-02-12 19:03:25,667] Epoch: 9 | FT  | Train Loss: 0.24873 | Val Loss: 0.25687\n",
      "[2024-02-12 19:03:25,668] Epoch: 9 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:03:25,668] Epoch: 9 | Ph  | Train Loss: 0.249 | Val Loss: 0.257\n",
      "[2024-02-12 19:03:25,669] Epoch: 9 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.74it/s]\n",
      "[2024-02-12 19:04:02,411] Saving improved model after Val Loss improved from 0.25687 to 0.25127\n",
      "[2024-02-12 19:04:02,496] Epoch: 10 | FT  | Train Loss: 0.24283 | Val Loss: 0.25127\n",
      "[2024-02-12 19:04:02,497] Epoch: 10 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:04:02,498] Epoch: 10 | Ph  | Train Loss: 0.243 | Val Loss: 0.251\n",
      "[2024-02-12 19:04:02,498] Epoch: 10 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.77it/s]\n",
      "[2024-02-12 19:04:39,090] Saving improved model after Val Loss improved from 0.25127 to 0.24684\n",
      "[2024-02-12 19:04:39,212] Epoch: 11 | FT  | Train Loss: 0.23892 | Val Loss: 0.24684\n",
      "[2024-02-12 19:04:39,213] Epoch: 11 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:04:39,214] Epoch: 11 | Ph  | Train Loss: 0.239 | Val Loss: 0.247\n",
      "[2024-02-12 19:04:39,214] Epoch: 11 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.76it/s]\n",
      "[2024-02-12 19:05:15,604] Epoch: 12 | FT  | Train Loss: 0.23703 | Val Loss: 0.24796\n",
      "[2024-02-12 19:05:15,605] Epoch: 12 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:05:15,606] Epoch: 12 | Ph  | Train Loss: 0.237 | Val Loss: 0.248\n",
      "[2024-02-12 19:05:15,606] Epoch: 12 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:33<00:00,  3.69it/s]\n",
      "[2024-02-12 19:05:52,637] Epoch: 13 | FT  | Train Loss: 0.23664 | Val Loss: 0.24908\n",
      "[2024-02-12 19:05:52,638] Epoch: 13 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:05:52,639] Epoch: 13 | Ph  | Train Loss: 0.237 | Val Loss: 0.249\n",
      "[2024-02-12 19:05:52,639] Epoch: 13 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:33<00:00,  3.68it/s]\n",
      "[2024-02-12 19:06:30,075] Epoch: 14 | FT  | Train Loss: 0.23604 | Val Loss: 0.24767\n",
      "[2024-02-12 19:06:30,077] Epoch: 14 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:06:30,078] Epoch: 14 | Ph  | Train Loss: 0.236 | Val Loss: 0.248\n",
      "[2024-02-12 19:06:30,079] Epoch: 14 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.75it/s]\n",
      "[2024-02-12 19:07:06,934] Epoch: 15 | FT  | Train Loss: 0.23552 | Val Loss: 0.24863\n",
      "[2024-02-12 19:07:06,935] Epoch: 15 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:07:06,936] Epoch: 15 | Ph  | Train Loss: 0.236 | Val Loss: 0.249\n",
      "[2024-02-12 19:07:06,937] Epoch: 15 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.71it/s]\n",
      "[2024-02-12 19:07:44,004] Epoch: 16 | FT  | Train Loss: 0.23383 | Val Loss: 0.25162\n",
      "[2024-02-12 19:07:44,006] Epoch: 16 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:07:44,007] Epoch: 16 | Ph  | Train Loss: 0.234 | Val Loss: 0.252\n",
      "[2024-02-12 19:07:44,008] Epoch: 16 | Ending LR: 0.000095 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.75it/s]\n",
      "[2024-02-12 19:08:20,895] Epoch: 17 | FT  | Train Loss: 0.23305 | Val Loss: 0.24719\n",
      "[2024-02-12 19:08:20,897] Epoch: 17 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:08:20,897] Epoch: 17 | Ph  | Train Loss: 0.233 | Val Loss: 0.247\n",
      "[2024-02-12 19:08:20,898] Epoch: 17 | Ending LR: 0.000110 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.76it/s]\n",
      "[2024-02-12 19:08:57,676] Saving improved model after Val Loss improved from 0.24684 to 0.24675\n",
      "[2024-02-12 19:08:57,764] Epoch: 18 | FT  | Train Loss: 0.22970 | Val Loss: 0.24675\n",
      "[2024-02-12 19:08:57,765] Epoch: 18 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:08:57,766] Epoch: 18 | Ph  | Train Loss: 0.230 | Val Loss: 0.247\n",
      "[2024-02-12 19:08:57,766] Epoch: 18 | Ending LR: 0.000095 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.73it/s]\n",
      "[2024-02-12 19:09:34,612] Saving improved model after Val Loss improved from 0.24675 to 0.23825\n",
      "[2024-02-12 19:09:34,700] Epoch: 19 | FT  | Train Loss: 0.22495 | Val Loss: 0.23825\n",
      "[2024-02-12 19:09:34,701] Epoch: 19 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:09:34,701] Epoch: 19 | Ph  | Train Loss: 0.225 | Val Loss: 0.238\n",
      "[2024-02-12 19:09:34,702] Epoch: 19 | Ending LR: 0.000080 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.75it/s]\n",
      "[2024-02-12 19:10:11,454] Saving improved model after Val Loss improved from 0.23825 to 0.23435\n",
      "[2024-02-12 19:10:11,539] Epoch: 20 | FT  | Train Loss: 0.22249 | Val Loss: 0.23435\n",
      "[2024-02-12 19:10:11,539] Epoch: 20 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:10:11,540] Epoch: 20 | Ph  | Train Loss: 0.222 | Val Loss: 0.234\n",
      "[2024-02-12 19:10:11,540] Epoch: 20 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.77it/s]\n",
      "[2024-02-12 19:10:48,186] Epoch: 21 | FT  | Train Loss: 0.21961 | Val Loss: 0.23535\n",
      "[2024-02-12 19:10:48,187] Epoch: 21 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:10:48,187] Epoch: 21 | Ph  | Train Loss: 0.220 | Val Loss: 0.235\n",
      "[2024-02-12 19:10:48,188] Epoch: 21 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.71it/s]\n",
      "[2024-02-12 19:11:25,533] Saving improved model after Val Loss improved from 0.23435 to 0.23355\n",
      "[2024-02-12 19:11:25,623] Epoch: 22 | FT  | Train Loss: 0.21699 | Val Loss: 0.23355\n",
      "[2024-02-12 19:11:25,624] Epoch: 22 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:11:25,624] Epoch: 22 | Ph  | Train Loss: 0.217 | Val Loss: 0.234\n",
      "[2024-02-12 19:11:25,625] Epoch: 22 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.79it/s]\n",
      "[2024-02-12 19:12:01,997] Saving improved model after Val Loss improved from 0.23355 to 0.22859\n",
      "[2024-02-12 19:12:02,085] Epoch: 23 | FT  | Train Loss: 0.21455 | Val Loss: 0.22859\n",
      "[2024-02-12 19:12:02,086] Epoch: 23 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:12:02,086] Epoch: 23 | Ph  | Train Loss: 0.215 | Val Loss: 0.229\n",
      "[2024-02-12 19:12:02,087] Epoch: 23 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.79it/s]\n",
      "[2024-02-12 19:12:38,412] Saving improved model after Val Loss improved from 0.22859 to 0.22840\n",
      "[2024-02-12 19:12:38,503] Epoch: 24 | FT  | Train Loss: 0.21347 | Val Loss: 0.22840\n",
      "[2024-02-12 19:12:38,504] Epoch: 24 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:12:38,504] Epoch: 24 | Ph  | Train Loss: 0.213 | Val Loss: 0.228\n",
      "[2024-02-12 19:12:38,505] Epoch: 24 | Ending LR: 0.000028 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.80it/s]\n",
      "[2024-02-12 19:13:14,867] Epoch: 25 | FT  | Train Loss: 0.21332 | Val Loss: 0.23058\n",
      "[2024-02-12 19:13:14,870] Epoch: 25 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:13:14,870] Epoch: 25 | Ph  | Train Loss: 0.213 | Val Loss: 0.231\n",
      "[2024-02-12 19:13:14,871] Epoch: 25 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.75it/s]\n",
      "[2024-02-12 19:13:51,572] Epoch: 26 | FT  | Train Loss: 0.21329 | Val Loss: 0.23045\n",
      "[2024-02-12 19:13:51,574] Epoch: 26 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:13:51,574] Epoch: 26 | Ph  | Train Loss: 0.213 | Val Loss: 0.230\n",
      "[2024-02-12 19:13:51,575] Epoch: 26 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:33<00:00,  3.66it/s]\n",
      "[2024-02-12 19:14:29,316] Epoch: 27 | FT  | Train Loss: 0.21287 | Val Loss: 0.23351\n",
      "[2024-02-12 19:14:29,317] Epoch: 27 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:14:29,318] Epoch: 27 | Ph  | Train Loss: 0.213 | Val Loss: 0.234\n",
      "[2024-02-12 19:14:29,318] Epoch: 27 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.72it/s]\n",
      "[2024-02-12 19:15:06,382] Epoch: 28 | FT  | Train Loss: 0.21311 | Val Loss: 0.24004\n",
      "[2024-02-12 19:15:06,383] Epoch: 28 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:15:06,384] Epoch: 28 | Ph  | Train Loss: 0.213 | Val Loss: 0.240\n",
      "[2024-02-12 19:15:06,384] Epoch: 28 | Ending LR: 0.000057 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.77it/s]\n",
      "[2024-02-12 19:15:42,929] Epoch: 29 | FT  | Train Loss: 0.21167 | Val Loss: 0.23640\n",
      "[2024-02-12 19:15:42,931] Epoch: 29 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:15:42,931] Epoch: 29 | Ph  | Train Loss: 0.212 | Val Loss: 0.236\n",
      "[2024-02-12 19:15:42,932] Epoch: 29 | Ending LR: 0.000065 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:33<00:00,  3.60it/s]\n",
      "[2024-02-12 19:16:21,136] Epoch: 30 | FT  | Train Loss: 0.21105 | Val Loss: 0.23141\n",
      "[2024-02-12 19:16:21,139] Epoch: 30 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:16:21,139] Epoch: 30 | Ph  | Train Loss: 0.211 | Val Loss: 0.231\n",
      "[2024-02-12 19:16:21,140] Epoch: 30 | Ending LR: 0.000057 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.73it/s]\n",
      "[2024-02-12 19:16:58,249] Epoch: 31 | FT  | Train Loss: 0.20877 | Val Loss: 0.22901\n",
      "[2024-02-12 19:16:58,250] Epoch: 31 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:16:58,251] Epoch: 31 | Ph  | Train Loss: 0.209 | Val Loss: 0.229\n",
      "[2024-02-12 19:16:58,251] Epoch: 31 | Ending LR: 0.000050 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.71it/s]\n",
      "[2024-02-12 19:17:35,239] Saving improved model after Val Loss improved from 0.22840 to 0.22558\n",
      "[2024-02-12 19:17:35,332] Epoch: 32 | FT  | Train Loss: 0.20665 | Val Loss: 0.22558\n",
      "[2024-02-12 19:17:35,333] Epoch: 32 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:17:35,333] Epoch: 32 | Ph  | Train Loss: 0.207 | Val Loss: 0.226\n",
      "[2024-02-12 19:17:35,334] Epoch: 32 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.81it/s]\n",
      "[2024-02-12 19:18:11,382] Epoch: 33 | FT  | Train Loss: 0.20463 | Val Loss: 0.22734\n",
      "[2024-02-12 19:18:11,384] Epoch: 33 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:18:11,384] Epoch: 33 | Ph  | Train Loss: 0.205 | Val Loss: 0.227\n",
      "[2024-02-12 19:18:11,385] Epoch: 33 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.71it/s]\n",
      "[2024-02-12 19:18:48,645] Saving improved model after Val Loss improved from 0.22558 to 0.22354\n",
      "[2024-02-12 19:18:48,740] Epoch: 34 | FT  | Train Loss: 0.20295 | Val Loss: 0.22354\n",
      "[2024-02-12 19:18:48,740] Epoch: 34 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:18:48,741] Epoch: 34 | Ph  | Train Loss: 0.203 | Val Loss: 0.224\n",
      "[2024-02-12 19:18:48,741] Epoch: 34 | Ending LR: 0.000028 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.73it/s]\n",
      "[2024-02-12 19:19:25,698] Saving improved model after Val Loss improved from 0.22354 to 0.22205\n",
      "[2024-02-12 19:19:25,792] Epoch: 35 | FT  | Train Loss: 0.20176 | Val Loss: 0.22205\n",
      "[2024-02-12 19:19:25,793] Epoch: 35 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:19:25,794] Epoch: 35 | Ph  | Train Loss: 0.202 | Val Loss: 0.222\n",
      "[2024-02-12 19:19:25,794] Epoch: 35 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.79it/s]\n",
      "[2024-02-12 19:20:02,196] Epoch: 36 | FT  | Train Loss: 0.20052 | Val Loss: 0.22354\n",
      "[2024-02-12 19:20:02,197] Epoch: 36 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:20:02,198] Epoch: 36 | Ph  | Train Loss: 0.201 | Val Loss: 0.224\n",
      "[2024-02-12 19:20:02,198] Epoch: 36 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:31<00:00,  3.82it/s]\n",
      "[2024-02-12 19:20:38,267] Epoch: 37 | FT  | Train Loss: 0.20084 | Val Loss: 0.22405\n",
      "[2024-02-12 19:20:38,268] Epoch: 37 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:20:38,268] Epoch: 37 | Ph  | Train Loss: 0.201 | Val Loss: 0.224\n",
      "[2024-02-12 19:20:38,269] Epoch: 37 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.76it/s]\n",
      "[2024-02-12 19:21:15,291] Epoch: 38 | FT  | Train Loss: 0.20107 | Val Loss: 0.22337\n",
      "[2024-02-12 19:21:15,293] Epoch: 38 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:21:15,294] Epoch: 38 | Ph  | Train Loss: 0.201 | Val Loss: 0.223\n",
      "[2024-02-12 19:21:15,295] Epoch: 38 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:34<00:00,  3.50it/s]\n",
      "[2024-02-12 19:21:54,482] Epoch: 39 | FT  | Train Loss: 0.20031 | Val Loss: 0.22308\n",
      "[2024-02-12 19:21:54,483] Epoch: 39 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:21:54,483] Epoch: 39 | Ph  | Train Loss: 0.200 | Val Loss: 0.223\n",
      "[2024-02-12 19:21:54,484] Epoch: 39 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:33<00:00,  3.69it/s]\n",
      "[2024-02-12 19:22:32,636] Epoch: 40 | FT  | Train Loss: 0.19989 | Val Loss: 0.22579\n",
      "[2024-02-12 19:22:32,637] Epoch: 40 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:22:32,638] Epoch: 40 | Ph  | Train Loss: 0.200 | Val Loss: 0.226\n",
      "[2024-02-12 19:22:32,639] Epoch: 40 | Ending LR: 0.000039 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:37<00:00,  3.23it/s]\n",
      "[2024-02-12 19:23:16,364] Saving improved model after Val Loss improved from 0.22205 to 0.22198\n",
      "[2024-02-12 19:23:16,506] Epoch: 41 | FT  | Train Loss: 0.19992 | Val Loss: 0.22198\n",
      "[2024-02-12 19:23:16,507] Epoch: 41 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:23:16,508] Epoch: 41 | Ph  | Train Loss: 0.200 | Val Loss: 0.222\n",
      "[2024-02-12 19:23:16,510] Epoch: 41 | Ending LR: 0.000043 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:36<00:00,  3.33it/s]\n",
      "[2024-02-12 19:23:59,055] Saving improved model after Val Loss improved from 0.22198 to 0.22138\n",
      "[2024-02-12 19:23:59,118] Epoch: 42 | FT  | Train Loss: 0.19896 | Val Loss: 0.22138\n",
      "[2024-02-12 19:23:59,119] Epoch: 42 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:23:59,121] Epoch: 42 | Ph  | Train Loss: 0.199 | Val Loss: 0.221\n",
      "[2024-02-12 19:23:59,122] Epoch: 42 | Ending LR: 0.000039 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:35<00:00,  3.39it/s]\n",
      "[2024-02-12 19:24:39,546] Epoch: 43 | FT  | Train Loss: 0.19722 | Val Loss: 0.22343\n",
      "[2024-02-12 19:24:39,547] Epoch: 43 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:24:39,548] Epoch: 43 | Ph  | Train Loss: 0.197 | Val Loss: 0.223\n",
      "[2024-02-12 19:24:39,549] Epoch: 43 | Ending LR: 0.000035 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:35<00:00,  3.40it/s]\n",
      "[2024-02-12 19:25:19,700] Saving improved model after Val Loss improved from 0.22138 to 0.21936\n",
      "[2024-02-12 19:25:19,788] Epoch: 44 | FT  | Train Loss: 0.19643 | Val Loss: 0.21936\n",
      "[2024-02-12 19:25:19,789] Epoch: 44 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:25:19,790] Epoch: 44 | Ph  | Train Loss: 0.196 | Val Loss: 0.219\n",
      "[2024-02-12 19:25:19,790] Epoch: 44 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:36<00:00,  3.37it/s]\n",
      "[2024-02-12 19:26:00,412] Epoch: 45 | FT  | Train Loss: 0.19489 | Val Loss: 0.22446\n",
      "[2024-02-12 19:26:00,414] Epoch: 45 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:26:00,415] Epoch: 45 | Ph  | Train Loss: 0.195 | Val Loss: 0.224\n",
      "[2024-02-12 19:26:00,415] Epoch: 45 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:36<00:00,  3.38it/s]\n",
      "[2024-02-12 19:26:40,544] Epoch: 46 | FT  | Train Loss: 0.19451 | Val Loss: 0.22173\n",
      "[2024-02-12 19:26:40,545] Epoch: 46 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:26:40,546] Epoch: 46 | Ph  | Train Loss: 0.195 | Val Loss: 0.222\n",
      "[2024-02-12 19:26:40,547] Epoch: 46 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:36<00:00,  3.38it/s]\n",
      "[2024-02-12 19:27:23,084] Epoch: 47 | FT  | Train Loss: 0.19337 | Val Loss: 0.21981\n",
      "[2024-02-12 19:27:23,086] Epoch: 47 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:27:23,087] Epoch: 47 | Ph  | Train Loss: 0.193 | Val Loss: 0.220\n",
      "[2024-02-12 19:27:23,088] Epoch: 47 | Ending LR: 0.000020 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:33<00:00,  3.69it/s]\n",
      "[2024-02-12 19:28:00,225] Epoch: 48 | FT  | Train Loss: 0.19274 | Val Loss: 0.21963\n",
      "[2024-02-12 19:28:00,226] Epoch: 48 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:28:00,227] Epoch: 48 | Ph  | Train Loss: 0.193 | Val Loss: 0.220\n",
      "[2024-02-12 19:28:00,227] Epoch: 48 | Ending LR: 0.000022 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:31<00:00,  3.82it/s]\n",
      "[2024-02-12 19:28:36,019] Epoch: 49 | FT  | Train Loss: 0.19276 | Val Loss: 0.22004\n",
      "[2024-02-12 19:28:36,020] Epoch: 49 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:28:36,021] Epoch: 49 | Ph  | Train Loss: 0.193 | Val Loss: 0.220\n",
      "[2024-02-12 19:28:36,021] Epoch: 49 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.72it/s]\n",
      "[2024-02-12 19:29:12,931] Saving improved model after Val Loss improved from 0.21936 to 0.21858\n",
      "[2024-02-12 19:29:13,027] Epoch: 50 | FT  | Train Loss: 0.19216 | Val Loss: 0.21858\n",
      "[2024-02-12 19:29:13,028] Epoch: 50 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:29:13,028] Epoch: 50 | Ph  | Train Loss: 0.192 | Val Loss: 0.219\n",
      "[2024-02-12 19:29:13,029] Epoch: 50 | Ending LR: 0.000026 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.75it/s]\n",
      "[2024-02-12 19:29:49,522] Epoch: 51 | FT  | Train Loss: 0.19181 | Val Loss: 0.21892\n",
      "[2024-02-12 19:29:49,523] Epoch: 51 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:29:49,523] Epoch: 51 | Ph  | Train Loss: 0.192 | Val Loss: 0.219\n",
      "[2024-02-12 19:29:49,524] Epoch: 51 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.79it/s]\n",
      "[2024-02-12 19:30:25,778] Epoch: 52 | FT  | Train Loss: 0.19183 | Val Loss: 0.22724\n",
      "[2024-02-12 19:30:25,779] Epoch: 52 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:30:25,779] Epoch: 52 | Ph  | Train Loss: 0.192 | Val Loss: 0.227\n",
      "[2024-02-12 19:30:25,780] Epoch: 52 | Ending LR: 0.000029 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.77it/s]\n",
      "[2024-02-12 19:31:02,251] Epoch: 53 | FT  | Train Loss: 0.19176 | Val Loss: 0.22067\n",
      "[2024-02-12 19:31:02,252] Epoch: 53 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:31:02,252] Epoch: 53 | Ph  | Train Loss: 0.192 | Val Loss: 0.221\n",
      "[2024-02-12 19:31:02,253] Epoch: 53 | Ending LR: 0.000031 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.74it/s]\n",
      "[2024-02-12 19:31:39,074] Epoch: 54 | FT  | Train Loss: 0.19088 | Val Loss: 0.21930\n",
      "[2024-02-12 19:31:39,075] Epoch: 54 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:31:39,076] Epoch: 54 | Ph  | Train Loss: 0.191 | Val Loss: 0.219\n",
      "[2024-02-12 19:31:39,077] Epoch: 54 | Ending LR: 0.000029 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:32<00:00,  3.71it/s]\n",
      "[2024-02-12 19:32:15,995] Saving improved model after Val Loss improved from 0.21858 to 0.21742\n",
      "[2024-02-12 19:32:16,099] Epoch: 55 | FT  | Train Loss: 0.19031 | Val Loss: 0.21742\n",
      "[2024-02-12 19:32:16,100] Epoch: 55 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:32:16,101] Epoch: 55 | Ph  | Train Loss: 0.190 | Val Loss: 0.217\n",
      "[2024-02-12 19:32:16,102] Epoch: 55 | Ending LR: 0.000027 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:33<00:00,  3.68it/s]\n",
      "[2024-02-12 19:32:53,511] Epoch: 56 | FT  | Train Loss: 0.18978 | Val Loss: 0.21951\n",
      "[2024-02-12 19:32:53,512] Epoch: 56 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:32:53,512] Epoch: 56 | Ph  | Train Loss: 0.190 | Val Loss: 0.220\n",
      "[2024-02-12 19:32:53,513] Epoch: 56 | Ending LR: 0.000026 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:33<00:00,  3.68it/s]\n",
      "[2024-02-12 19:33:30,899] Epoch: 57 | FT  | Train Loss: 0.18868 | Val Loss: 0.21862\n",
      "[2024-02-12 19:33:30,900] Epoch: 57 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:33:30,901] Epoch: 57 | Ph  | Train Loss: 0.189 | Val Loss: 0.219\n",
      "[2024-02-12 19:33:30,901] Epoch: 57 | Ending LR: 0.000024 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:33<00:00,  3.62it/s]\n",
      "[2024-02-12 19:34:08,745] Epoch: 58 | FT  | Train Loss: 0.18815 | Val Loss: 0.22157\n",
      "[2024-02-12 19:34:08,746] Epoch: 58 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:34:08,747] Epoch: 58 | Ph  | Train Loss: 0.188 | Val Loss: 0.222\n",
      "[2024-02-12 19:34:08,747] Epoch: 58 | Ending LR: 0.000022 \n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 122/122 [00:33<00:00,  3.60it/s]\n",
      "[2024-02-12 19:34:47,366] Epoch: 59 | FT  | Train Loss: 0.18762 | Val Loss: 0.21856\n",
      "[2024-02-12 19:34:47,367] Epoch: 59 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:34:47,368] Epoch: 59 | Ph  | Train Loss: 0.188 | Val Loss: 0.219\n",
      "[2024-02-12 19:34:47,369] Epoch: 59 | Ending LR: 0.000020 \n",
      "[2024-02-12 19:34:47,501] Decimating dataset to 0.4 of the original size...\n",
      "[2024-02-12 19:34:47,605] Using DataParallel with 2 devices.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:27<00:00,  3.63it/s]\n",
      "[2024-02-12 19:35:18,981] Saving improved model after Val Loss improved from inf to 0.54756\n",
      "[2024-02-12 19:35:19,010] Epoch: 0 | FT  | Train Loss: 0.62373 | Val Loss: 0.54756\n",
      "[2024-02-12 19:35:19,010] Epoch: 0 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:35:19,011] Epoch: 0 | Ph  | Train Loss: 0.624 | Val Loss: 0.548\n",
      "[2024-02-12 19:35:19,011] Epoch: 0 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:28<00:00,  3.43it/s]\n",
      "[2024-02-12 19:35:53,634] Saving improved model after Val Loss improved from 0.54756 to 0.44639\n",
      "[2024-02-12 19:35:53,721] Epoch: 1 | FT  | Train Loss: 0.49462 | Val Loss: 0.44639\n",
      "[2024-02-12 19:35:53,723] Epoch: 1 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:35:53,727] Epoch: 1 | Ph  | Train Loss: 0.495 | Val Loss: 0.446\n",
      "[2024-02-12 19:35:53,733] Epoch: 1 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:26<00:00,  3.72it/s]\n",
      "[2024-02-12 19:36:24,438] Saving improved model after Val Loss improved from 0.44639 to 0.38220\n",
      "[2024-02-12 19:36:24,512] Epoch: 2 | FT  | Train Loss: 0.40463 | Val Loss: 0.38220\n",
      "[2024-02-12 19:36:24,513] Epoch: 2 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:36:24,514] Epoch: 2 | Ph  | Train Loss: 0.405 | Val Loss: 0.382\n",
      "[2024-02-12 19:36:24,514] Epoch: 2 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:25<00:00,  3.77it/s]\n",
      "[2024-02-12 19:36:54,602] Saving improved model after Val Loss improved from 0.38220 to 0.35700\n",
      "[2024-02-12 19:36:54,680] Epoch: 3 | FT  | Train Loss: 0.35697 | Val Loss: 0.35700\n",
      "[2024-02-12 19:36:54,681] Epoch: 3 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:36:54,681] Epoch: 3 | Ph  | Train Loss: 0.357 | Val Loss: 0.357\n",
      "[2024-02-12 19:36:54,682] Epoch: 3 | Ending LR: 0.000140 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:26<00:00,  3.73it/s]\n",
      "[2024-02-12 19:37:25,343] Saving improved model after Val Loss improved from 0.35700 to 0.33169\n",
      "[2024-02-12 19:37:25,433] Epoch: 4 | FT  | Train Loss: 0.32823 | Val Loss: 0.33169\n",
      "[2024-02-12 19:37:25,434] Epoch: 4 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:37:25,435] Epoch: 4 | Ph  | Train Loss: 0.328 | Val Loss: 0.332\n",
      "[2024-02-12 19:37:25,436] Epoch: 4 | Ending LR: 0.000170 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:26<00:00,  3.76it/s]\n",
      "[2024-02-12 19:37:55,825] Saving improved model after Val Loss improved from 0.33169 to 0.32275\n",
      "[2024-02-12 19:37:55,916] Epoch: 5 | FT  | Train Loss: 0.30657 | Val Loss: 0.32275\n",
      "[2024-02-12 19:37:55,917] Epoch: 5 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:37:55,918] Epoch: 5 | Ph  | Train Loss: 0.307 | Val Loss: 0.323\n",
      "[2024-02-12 19:37:55,918] Epoch: 5 | Ending LR: 0.000200 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:26<00:00,  3.76it/s]\n",
      "[2024-02-12 19:38:26,227] Saving improved model after Val Loss improved from 0.32275 to 0.28940\n",
      "[2024-02-12 19:38:26,346] Epoch: 6 | FT  | Train Loss: 0.28855 | Val Loss: 0.28940\n",
      "[2024-02-12 19:38:26,347] Epoch: 6 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:38:26,348] Epoch: 6 | Ph  | Train Loss: 0.289 | Val Loss: 0.289\n",
      "[2024-02-12 19:38:26,349] Epoch: 6 | Ending LR: 0.000170 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:26<00:00,  3.71it/s]\n",
      "[2024-02-12 19:38:57,485] Saving improved model after Val Loss improved from 0.28940 to 0.28360\n",
      "[2024-02-12 19:38:57,567] Epoch: 7 | FT  | Train Loss: 0.27558 | Val Loss: 0.28360\n",
      "[2024-02-12 19:38:57,568] Epoch: 7 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:38:57,569] Epoch: 7 | Ph  | Train Loss: 0.276 | Val Loss: 0.284\n",
      "[2024-02-12 19:38:57,570] Epoch: 7 | Ending LR: 0.000140 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:25<00:00,  3.81it/s]\n",
      "[2024-02-12 19:39:27,573] Saving improved model after Val Loss improved from 0.28360 to 0.27086\n",
      "[2024-02-12 19:39:27,678] Epoch: 8 | FT  | Train Loss: 0.26519 | Val Loss: 0.27086\n",
      "[2024-02-12 19:39:27,679] Epoch: 8 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:39:27,680] Epoch: 8 | Ph  | Train Loss: 0.265 | Val Loss: 0.271\n",
      "[2024-02-12 19:39:27,680] Epoch: 8 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:26<00:00,  3.66it/s]\n",
      "[2024-02-12 19:40:00,235] Saving improved model after Val Loss improved from 0.27086 to 0.26413\n",
      "[2024-02-12 19:40:00,342] Epoch: 9 | FT  | Train Loss: 0.25851 | Val Loss: 0.26413\n",
      "[2024-02-12 19:40:00,344] Epoch: 9 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:40:00,345] Epoch: 9 | Ph  | Train Loss: 0.259 | Val Loss: 0.264\n",
      "[2024-02-12 19:40:00,347] Epoch: 9 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:35<00:00,  2.75it/s]\n",
      "[2024-02-12 19:40:42,929] Epoch: 10 | FT  | Train Loss: 0.25267 | Val Loss: 0.26588\n",
      "[2024-02-12 19:40:42,930] Epoch: 10 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:40:42,931] Epoch: 10 | Ph  | Train Loss: 0.253 | Val Loss: 0.266\n",
      "[2024-02-12 19:40:42,933] Epoch: 10 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:35<00:00,  2.73it/s]\n",
      "[2024-02-12 19:41:25,517] Saving improved model after Val Loss improved from 0.26413 to 0.25824\n",
      "[2024-02-12 19:41:25,617] Epoch: 11 | FT  | Train Loss: 0.24885 | Val Loss: 0.25824\n",
      "[2024-02-12 19:41:25,619] Epoch: 11 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:41:25,621] Epoch: 11 | Ph  | Train Loss: 0.249 | Val Loss: 0.258\n",
      "[2024-02-12 19:41:25,622] Epoch: 11 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:35<00:00,  2.79it/s]\n",
      "[2024-02-12 19:42:07,971] Saving improved model after Val Loss improved from 0.25824 to 0.25744\n",
      "[2024-02-12 19:42:08,075] Epoch: 12 | FT  | Train Loss: 0.24606 | Val Loss: 0.25744\n",
      "[2024-02-12 19:42:08,076] Epoch: 12 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:42:08,077] Epoch: 12 | Ph  | Train Loss: 0.246 | Val Loss: 0.257\n",
      "[2024-02-12 19:42:08,078] Epoch: 12 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:34<00:00,  2.81it/s]\n",
      "[2024-02-12 19:42:49,724] Saving improved model after Val Loss improved from 0.25744 to 0.25726\n",
      "[2024-02-12 19:42:49,875] Epoch: 13 | FT  | Train Loss: 0.24564 | Val Loss: 0.25726\n",
      "[2024-02-12 19:42:49,877] Epoch: 13 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:42:49,878] Epoch: 13 | Ph  | Train Loss: 0.246 | Val Loss: 0.257\n",
      "[2024-02-12 19:42:49,879] Epoch: 13 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.72it/s]\n",
      "[2024-02-12 19:43:33,130] Epoch: 14 | FT  | Train Loss: 0.24569 | Val Loss: 0.26190\n",
      "[2024-02-12 19:43:33,135] Epoch: 14 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:43:33,136] Epoch: 14 | Ph  | Train Loss: 0.246 | Val Loss: 0.262\n",
      "[2024-02-12 19:43:33,137] Epoch: 14 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.71it/s]\n",
      "[2024-02-12 19:44:16,260] Saving improved model after Val Loss improved from 0.25726 to 0.25595\n",
      "[2024-02-12 19:44:16,381] Epoch: 15 | FT  | Train Loss: 0.24411 | Val Loss: 0.25595\n",
      "[2024-02-12 19:44:16,382] Epoch: 15 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:44:16,383] Epoch: 15 | Ph  | Train Loss: 0.244 | Val Loss: 0.256\n",
      "[2024-02-12 19:44:16,384] Epoch: 15 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.72it/s]\n",
      "[2024-02-12 19:44:59,208] Epoch: 16 | FT  | Train Loss: 0.24365 | Val Loss: 0.25804\n",
      "[2024-02-12 19:44:59,219] Epoch: 16 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:44:59,223] Epoch: 16 | Ph  | Train Loss: 0.244 | Val Loss: 0.258\n",
      "[2024-02-12 19:44:59,227] Epoch: 16 | Ending LR: 0.000095 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.70it/s]\n",
      "[2024-02-12 19:45:42,237] Epoch: 17 | FT  | Train Loss: 0.24213 | Val Loss: 0.26667\n",
      "[2024-02-12 19:45:42,249] Epoch: 17 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:45:42,252] Epoch: 17 | Ph  | Train Loss: 0.242 | Val Loss: 0.267\n",
      "[2024-02-12 19:45:42,254] Epoch: 17 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.71it/s]\n",
      "[2024-02-12 19:46:25,452] Saving improved model after Val Loss improved from 0.25595 to 0.25164\n",
      "[2024-02-12 19:46:25,626] Epoch: 18 | FT  | Train Loss: 0.23888 | Val Loss: 0.25164\n",
      "[2024-02-12 19:46:25,628] Epoch: 18 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:46:25,629] Epoch: 18 | Ph  | Train Loss: 0.239 | Val Loss: 0.252\n",
      "[2024-02-12 19:46:25,630] Epoch: 18 | Ending LR: 0.000095 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.70it/s]\n",
      "[2024-02-12 19:47:08,631] Saving improved model after Val Loss improved from 0.25164 to 0.24843\n",
      "[2024-02-12 19:47:08,741] Epoch: 19 | FT  | Train Loss: 0.23479 | Val Loss: 0.24843\n",
      "[2024-02-12 19:47:08,743] Epoch: 19 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:47:08,744] Epoch: 19 | Ph  | Train Loss: 0.235 | Val Loss: 0.248\n",
      "[2024-02-12 19:47:08,745] Epoch: 19 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.70it/s]\n",
      "[2024-02-12 19:47:51,955] Saving improved model after Val Loss improved from 0.24843 to 0.24640\n",
      "[2024-02-12 19:47:52,058] Epoch: 20 | FT  | Train Loss: 0.23122 | Val Loss: 0.24640\n",
      "[2024-02-12 19:47:52,060] Epoch: 20 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:47:52,061] Epoch: 20 | Ph  | Train Loss: 0.231 | Val Loss: 0.246\n",
      "[2024-02-12 19:47:52,062] Epoch: 20 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.69it/s]\n",
      "[2024-02-12 19:48:35,304] Saving improved model after Val Loss improved from 0.24640 to 0.24449\n",
      "[2024-02-12 19:48:35,437] Epoch: 21 | FT  | Train Loss: 0.22825 | Val Loss: 0.24449\n",
      "[2024-02-12 19:48:35,439] Epoch: 21 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:48:35,440] Epoch: 21 | Ph  | Train Loss: 0.228 | Val Loss: 0.244\n",
      "[2024-02-12 19:48:35,442] Epoch: 21 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.70it/s]\n",
      "[2024-02-12 19:49:18,997] Saving improved model after Val Loss improved from 0.24449 to 0.24315\n",
      "[2024-02-12 19:49:19,116] Epoch: 22 | FT  | Train Loss: 0.22540 | Val Loss: 0.24315\n",
      "[2024-02-12 19:49:19,117] Epoch: 22 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:49:19,118] Epoch: 22 | Ph  | Train Loss: 0.225 | Val Loss: 0.243\n",
      "[2024-02-12 19:49:19,119] Epoch: 22 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.70it/s]\n",
      "[2024-02-12 19:50:02,566] Saving improved model after Val Loss improved from 0.24315 to 0.24157\n",
      "[2024-02-12 19:50:02,680] Epoch: 23 | FT  | Train Loss: 0.22304 | Val Loss: 0.24157\n",
      "[2024-02-12 19:50:02,681] Epoch: 23 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:50:02,682] Epoch: 23 | Ph  | Train Loss: 0.223 | Val Loss: 0.242\n",
      "[2024-02-12 19:50:02,683] Epoch: 23 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.67it/s]\n",
      "[2024-02-12 19:50:46,391] Epoch: 24 | FT  | Train Loss: 0.22168 | Val Loss: 0.24198\n",
      "[2024-02-12 19:50:46,396] Epoch: 24 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:50:46,397] Epoch: 24 | Ph  | Train Loss: 0.222 | Val Loss: 0.242\n",
      "[2024-02-12 19:50:46,409] Epoch: 24 | Ending LR: 0.000028 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.69it/s]\n",
      "[2024-02-12 19:51:30,079] Saving improved model after Val Loss improved from 0.24157 to 0.24061\n",
      "[2024-02-12 19:51:30,175] Epoch: 25 | FT  | Train Loss: 0.22182 | Val Loss: 0.24061\n",
      "[2024-02-12 19:51:30,176] Epoch: 25 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:51:30,178] Epoch: 25 | Ph  | Train Loss: 0.222 | Val Loss: 0.241\n",
      "[2024-02-12 19:51:30,179] Epoch: 25 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.71it/s]\n",
      "[2024-02-12 19:52:13,560] Epoch: 26 | FT  | Train Loss: 0.22162 | Val Loss: 0.24080\n",
      "[2024-02-12 19:52:13,561] Epoch: 26 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:52:13,562] Epoch: 26 | Ph  | Train Loss: 0.222 | Val Loss: 0.241\n",
      "[2024-02-12 19:52:13,563] Epoch: 26 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:37<00:00,  2.63it/s]\n",
      "[2024-02-12 19:52:57,888] Epoch: 27 | FT  | Train Loss: 0.22123 | Val Loss: 0.24413\n",
      "[2024-02-12 19:52:57,894] Epoch: 27 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:52:57,895] Epoch: 27 | Ph  | Train Loss: 0.221 | Val Loss: 0.244\n",
      "[2024-02-12 19:52:57,896] Epoch: 27 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.71it/s]\n",
      "[2024-02-12 19:53:41,260] Epoch: 28 | FT  | Train Loss: 0.22094 | Val Loss: 0.24300\n",
      "[2024-02-12 19:53:41,262] Epoch: 28 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:53:41,262] Epoch: 28 | Ph  | Train Loss: 0.221 | Val Loss: 0.243\n",
      "[2024-02-12 19:53:41,268] Epoch: 28 | Ending LR: 0.000057 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.68it/s]\n",
      "[2024-02-12 19:54:24,649] Epoch: 29 | FT  | Train Loss: 0.22067 | Val Loss: 0.24652\n",
      "[2024-02-12 19:54:24,650] Epoch: 29 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:54:24,651] Epoch: 29 | Ph  | Train Loss: 0.221 | Val Loss: 0.247\n",
      "[2024-02-12 19:54:24,653] Epoch: 29 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.68it/s]\n",
      "[2024-02-12 19:55:08,254] Saving improved model after Val Loss improved from 0.24061 to 0.24040\n",
      "[2024-02-12 19:55:08,393] Epoch: 30 | FT  | Train Loss: 0.21849 | Val Loss: 0.24040\n",
      "[2024-02-12 19:55:08,394] Epoch: 30 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:55:08,395] Epoch: 30 | Ph  | Train Loss: 0.218 | Val Loss: 0.240\n",
      "[2024-02-12 19:55:08,396] Epoch: 30 | Ending LR: 0.000057 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:37<00:00,  2.64it/s]\n",
      "[2024-02-12 19:55:52,468] Epoch: 31 | FT  | Train Loss: 0.21745 | Val Loss: 0.24503\n",
      "[2024-02-12 19:55:52,469] Epoch: 31 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:55:52,470] Epoch: 31 | Ph  | Train Loss: 0.217 | Val Loss: 0.245\n",
      "[2024-02-12 19:55:52,471] Epoch: 31 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.69it/s]\n",
      "[2024-02-12 19:56:35,652] Epoch: 32 | FT  | Train Loss: 0.21577 | Val Loss: 0.24199\n",
      "[2024-02-12 19:56:35,655] Epoch: 32 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:56:35,660] Epoch: 32 | Ph  | Train Loss: 0.216 | Val Loss: 0.242\n",
      "[2024-02-12 19:56:35,661] Epoch: 32 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:35<00:00,  2.73it/s]\n",
      "[2024-02-12 19:57:18,527] Saving improved model after Val Loss improved from 0.24040 to 0.23503\n",
      "[2024-02-12 19:57:18,612] Epoch: 33 | FT  | Train Loss: 0.21314 | Val Loss: 0.23503\n",
      "[2024-02-12 19:57:18,614] Epoch: 33 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:57:18,615] Epoch: 33 | Ph  | Train Loss: 0.213 | Val Loss: 0.235\n",
      "[2024-02-12 19:57:18,616] Epoch: 33 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:35<00:00,  2.73it/s]\n",
      "[2024-02-12 19:58:01,222] Epoch: 34 | FT  | Train Loss: 0.21191 | Val Loss: 0.23585\n",
      "[2024-02-12 19:58:01,232] Epoch: 34 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:58:01,234] Epoch: 34 | Ph  | Train Loss: 0.212 | Val Loss: 0.236\n",
      "[2024-02-12 19:58:01,237] Epoch: 34 | Ending LR: 0.000028 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:35<00:00,  2.74it/s]\n",
      "[2024-02-12 19:58:43,847] Epoch: 35 | FT  | Train Loss: 0.21009 | Val Loss: 0.23514\n",
      "[2024-02-12 19:58:43,848] Epoch: 35 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:58:43,849] Epoch: 35 | Ph  | Train Loss: 0.210 | Val Loss: 0.235\n",
      "[2024-02-12 19:58:43,850] Epoch: 35 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:35<00:00,  2.72it/s]\n",
      "[2024-02-12 19:59:26,699] Epoch: 36 | FT  | Train Loss: 0.20898 | Val Loss: 0.23594\n",
      "[2024-02-12 19:59:26,726] Epoch: 36 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 19:59:26,728] Epoch: 36 | Ph  | Train Loss: 0.209 | Val Loss: 0.236\n",
      "[2024-02-12 19:59:26,729] Epoch: 36 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:35<00:00,  2.73it/s]\n",
      "[2024-02-12 20:00:09,385] Epoch: 37 | FT  | Train Loss: 0.20908 | Val Loss: 0.23519\n",
      "[2024-02-12 20:00:09,386] Epoch: 37 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:00:09,387] Epoch: 37 | Ph  | Train Loss: 0.209 | Val Loss: 0.235\n",
      "[2024-02-12 20:00:09,388] Epoch: 37 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.72it/s]\n",
      "[2024-02-12 20:00:52,359] Saving improved model after Val Loss improved from 0.23503 to 0.23449\n",
      "[2024-02-12 20:00:52,453] Epoch: 38 | FT  | Train Loss: 0.20888 | Val Loss: 0.23449\n",
      "[2024-02-12 20:00:52,455] Epoch: 38 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:00:52,456] Epoch: 38 | Ph  | Train Loss: 0.209 | Val Loss: 0.234\n",
      "[2024-02-12 20:00:52,457] Epoch: 38 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.71it/s]\n",
      "[2024-02-12 20:01:35,703] Epoch: 39 | FT  | Train Loss: 0.20860 | Val Loss: 0.23452\n",
      "[2024-02-12 20:01:35,705] Epoch: 39 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:01:35,706] Epoch: 39 | Ph  | Train Loss: 0.209 | Val Loss: 0.235\n",
      "[2024-02-12 20:01:35,707] Epoch: 39 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.68it/s]\n",
      "[2024-02-12 20:02:19,320] Epoch: 40 | FT  | Train Loss: 0.20834 | Val Loss: 0.23628\n",
      "[2024-02-12 20:02:19,322] Epoch: 40 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:02:19,322] Epoch: 40 | Ph  | Train Loss: 0.208 | Val Loss: 0.236\n",
      "[2024-02-12 20:02:19,323] Epoch: 40 | Ending LR: 0.000039 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.71it/s]\n",
      "[2024-02-12 20:03:02,535] Epoch: 41 | FT  | Train Loss: 0.20813 | Val Loss: 0.23822\n",
      "[2024-02-12 20:03:02,549] Epoch: 41 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:03:02,553] Epoch: 41 | Ph  | Train Loss: 0.208 | Val Loss: 0.238\n",
      "[2024-02-12 20:03:02,556] Epoch: 41 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:37<00:00,  2.65it/s]\n",
      "[2024-02-12 20:03:46,694] Epoch: 42 | FT  | Train Loss: 0.20710 | Val Loss: 0.23470\n",
      "[2024-02-12 20:03:46,700] Epoch: 42 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:03:46,703] Epoch: 42 | Ph  | Train Loss: 0.207 | Val Loss: 0.235\n",
      "[2024-02-12 20:03:46,707] Epoch: 42 | Ending LR: 0.000039 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.71it/s]\n",
      "[2024-02-12 20:04:29,676] Epoch: 43 | FT  | Train Loss: 0.20614 | Val Loss: 0.23561\n",
      "[2024-02-12 20:04:29,684] Epoch: 43 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:04:29,685] Epoch: 43 | Ph  | Train Loss: 0.206 | Val Loss: 0.236\n",
      "[2024-02-12 20:04:29,686] Epoch: 43 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.72it/s]\n",
      "[2024-02-12 20:05:12,763] Epoch: 44 | FT  | Train Loss: 0.20541 | Val Loss: 0.23728\n",
      "[2024-02-12 20:05:12,766] Epoch: 44 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:05:12,767] Epoch: 44 | Ph  | Train Loss: 0.205 | Val Loss: 0.237\n",
      "[2024-02-12 20:05:12,769] Epoch: 44 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.71it/s]\n",
      "[2024-02-12 20:05:55,724] Saving improved model after Val Loss improved from 0.23449 to 0.23321\n",
      "[2024-02-12 20:05:55,850] Epoch: 45 | FT  | Train Loss: 0.20334 | Val Loss: 0.23321\n",
      "[2024-02-12 20:05:55,852] Epoch: 45 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:05:55,853] Epoch: 45 | Ph  | Train Loss: 0.203 | Val Loss: 0.233\n",
      "[2024-02-12 20:05:55,855] Epoch: 45 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.71it/s]\n",
      "[2024-02-12 20:06:39,201] Saving improved model after Val Loss improved from 0.23321 to 0.23177\n",
      "[2024-02-12 20:06:39,324] Epoch: 46 | FT  | Train Loss: 0.20252 | Val Loss: 0.23177\n",
      "[2024-02-12 20:06:39,325] Epoch: 46 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:06:39,326] Epoch: 46 | Ph  | Train Loss: 0.203 | Val Loss: 0.232\n",
      "[2024-02-12 20:06:39,327] Epoch: 46 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.69it/s]\n",
      "[2024-02-12 20:07:22,711] Epoch: 47 | FT  | Train Loss: 0.20146 | Val Loss: 0.23278\n",
      "[2024-02-12 20:07:22,717] Epoch: 47 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:07:22,723] Epoch: 47 | Ph  | Train Loss: 0.201 | Val Loss: 0.233\n",
      "[2024-02-12 20:07:22,726] Epoch: 47 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.70it/s]\n",
      "[2024-02-12 20:08:05,954] Saving improved model after Val Loss improved from 0.23177 to 0.23118\n",
      "[2024-02-12 20:08:06,072] Epoch: 48 | FT  | Train Loss: 0.20080 | Val Loss: 0.23118\n",
      "[2024-02-12 20:08:06,074] Epoch: 48 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:08:06,074] Epoch: 48 | Ph  | Train Loss: 0.201 | Val Loss: 0.231\n",
      "[2024-02-12 20:08:06,075] Epoch: 48 | Ending LR: 0.000022 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.70it/s]\n",
      "[2024-02-12 20:08:49,105] Epoch: 49 | FT  | Train Loss: 0.20084 | Val Loss: 0.23343\n",
      "[2024-02-12 20:08:49,107] Epoch: 49 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:08:49,108] Epoch: 49 | Ph  | Train Loss: 0.201 | Val Loss: 0.233\n",
      "[2024-02-12 20:08:49,110] Epoch: 49 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.71it/s]\n",
      "[2024-02-12 20:09:32,215] Epoch: 50 | FT  | Train Loss: 0.20037 | Val Loss: 0.23221\n",
      "[2024-02-12 20:09:32,216] Epoch: 50 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:09:32,216] Epoch: 50 | Ph  | Train Loss: 0.200 | Val Loss: 0.232\n",
      "[2024-02-12 20:09:32,217] Epoch: 50 | Ending LR: 0.000026 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:35<00:00,  2.73it/s]\n",
      "[2024-02-12 20:10:15,094] Epoch: 51 | FT  | Train Loss: 0.20025 | Val Loss: 0.23254\n",
      "[2024-02-12 20:10:15,096] Epoch: 51 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:10:15,097] Epoch: 51 | Ph  | Train Loss: 0.200 | Val Loss: 0.233\n",
      "[2024-02-12 20:10:15,097] Epoch: 51 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:35<00:00,  2.75it/s]\n",
      "[2024-02-12 20:10:57,218] Saving improved model after Val Loss improved from 0.23118 to 0.23075\n",
      "[2024-02-12 20:10:57,315] Epoch: 52 | FT  | Train Loss: 0.20013 | Val Loss: 0.23075\n",
      "[2024-02-12 20:10:57,317] Epoch: 52 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:10:57,318] Epoch: 52 | Ph  | Train Loss: 0.200 | Val Loss: 0.231\n",
      "[2024-02-12 20:10:57,318] Epoch: 52 | Ending LR: 0.000029 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.65it/s]\n",
      "[2024-02-12 20:11:41,061] Epoch: 53 | FT  | Train Loss: 0.20041 | Val Loss: 0.23446\n",
      "[2024-02-12 20:11:41,063] Epoch: 53 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:11:41,064] Epoch: 53 | Ph  | Train Loss: 0.200 | Val Loss: 0.234\n",
      "[2024-02-12 20:11:41,064] Epoch: 53 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:35<00:00,  2.74it/s]\n",
      "[2024-02-12 20:12:23,950] Epoch: 54 | FT  | Train Loss: 0.19906 | Val Loss: 0.23182\n",
      "[2024-02-12 20:12:23,954] Epoch: 54 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:12:23,958] Epoch: 54 | Ph  | Train Loss: 0.199 | Val Loss: 0.232\n",
      "[2024-02-12 20:12:23,961] Epoch: 54 | Ending LR: 0.000029 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:35<00:00,  2.74it/s]\n",
      "[2024-02-12 20:13:06,539] Epoch: 55 | FT  | Train Loss: 0.19829 | Val Loss: 0.23609\n",
      "[2024-02-12 20:13:06,541] Epoch: 55 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:13:06,542] Epoch: 55 | Ph  | Train Loss: 0.198 | Val Loss: 0.236\n",
      "[2024-02-12 20:13:06,543] Epoch: 55 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.69it/s]\n",
      "[2024-02-12 20:13:50,032] Saving improved model after Val Loss improved from 0.23075 to 0.22962\n",
      "[2024-02-12 20:13:50,144] Epoch: 56 | FT  | Train Loss: 0.19826 | Val Loss: 0.22962\n",
      "[2024-02-12 20:13:50,145] Epoch: 56 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:13:50,146] Epoch: 56 | Ph  | Train Loss: 0.198 | Val Loss: 0.230\n",
      "[2024-02-12 20:13:50,147] Epoch: 56 | Ending LR: 0.000026 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.68it/s]\n",
      "[2024-02-12 20:14:33,567] Epoch: 57 | FT  | Train Loss: 0.19720 | Val Loss: 0.22971\n",
      "[2024-02-12 20:14:33,582] Epoch: 57 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:14:33,586] Epoch: 57 | Ph  | Train Loss: 0.197 | Val Loss: 0.230\n",
      "[2024-02-12 20:14:33,590] Epoch: 57 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:36<00:00,  2.71it/s]\n",
      "[2024-02-12 20:15:16,471] Epoch: 58 | FT  | Train Loss: 0.19641 | Val Loss: 0.23113\n",
      "[2024-02-12 20:15:16,474] Epoch: 58 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:15:16,477] Epoch: 58 | Ph  | Train Loss: 0.196 | Val Loss: 0.231\n",
      "[2024-02-12 20:15:16,479] Epoch: 58 | Ending LR: 0.000022 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 98/98 [00:35<00:00,  2.72it/s]\n",
      "[2024-02-12 20:15:59,141] Epoch: 59 | FT  | Train Loss: 0.19526 | Val Loss: 0.23017\n",
      "[2024-02-12 20:15:59,142] Epoch: 59 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:15:59,143] Epoch: 59 | Ph  | Train Loss: 0.195 | Val Loss: 0.230\n",
      "[2024-02-12 20:15:59,144] Epoch: 59 | Ending LR: 0.000020 \n",
      "[2024-02-12 20:15:59,259] Decimating dataset to 0.3 of the original size...\n",
      "[2024-02-12 20:15:59,377] Using DataParallel with 2 devices.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:26<00:00,  2.76it/s]\n",
      "[2024-02-12 20:16:32,713] Saving improved model after Val Loss improved from inf to 0.57105\n",
      "[2024-02-12 20:16:32,780] Epoch: 0 | FT  | Train Loss: 0.64534 | Val Loss: 0.57105\n",
      "[2024-02-12 20:16:32,782] Epoch: 0 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:16:32,783] Epoch: 0 | Ph  | Train Loss: 0.645 | Val Loss: 0.571\n",
      "[2024-02-12 20:16:32,784] Epoch: 0 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:26<00:00,  2.71it/s]\n",
      "[2024-02-12 20:17:06,822] Saving improved model after Val Loss improved from 0.57105 to 0.49494\n",
      "[2024-02-12 20:17:06,917] Epoch: 1 | FT  | Train Loss: 0.53028 | Val Loss: 0.49494\n",
      "[2024-02-12 20:17:06,918] Epoch: 1 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:17:06,919] Epoch: 1 | Ph  | Train Loss: 0.530 | Val Loss: 0.495\n",
      "[2024-02-12 20:17:06,920] Epoch: 1 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:27<00:00,  2.69it/s]\n",
      "[2024-02-12 20:17:41,164] Saving improved model after Val Loss improved from 0.49494 to 0.42123\n",
      "[2024-02-12 20:17:41,287] Epoch: 2 | FT  | Train Loss: 0.44583 | Val Loss: 0.42123\n",
      "[2024-02-12 20:17:41,289] Epoch: 2 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:17:41,290] Epoch: 2 | Ph  | Train Loss: 0.446 | Val Loss: 0.421\n",
      "[2024-02-12 20:17:41,291] Epoch: 2 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:26<00:00,  2.71it/s]\n",
      "[2024-02-12 20:18:15,172] Saving improved model after Val Loss improved from 0.42123 to 0.38059\n",
      "[2024-02-12 20:18:15,246] Epoch: 3 | FT  | Train Loss: 0.38785 | Val Loss: 0.38059\n",
      "[2024-02-12 20:18:15,248] Epoch: 3 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:18:15,249] Epoch: 3 | Ph  | Train Loss: 0.388 | Val Loss: 0.381\n",
      "[2024-02-12 20:18:15,250] Epoch: 3 | Ending LR: 0.000140 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:26<00:00,  2.71it/s]\n",
      "[2024-02-12 20:18:49,387] Saving improved model after Val Loss improved from 0.38059 to 0.35805\n",
      "[2024-02-12 20:18:49,533] Epoch: 4 | FT  | Train Loss: 0.35265 | Val Loss: 0.35805\n",
      "[2024-02-12 20:18:49,534] Epoch: 4 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:18:49,535] Epoch: 4 | Ph  | Train Loss: 0.353 | Val Loss: 0.358\n",
      "[2024-02-12 20:18:49,536] Epoch: 4 | Ending LR: 0.000170 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:27<00:00,  2.67it/s]\n",
      "[2024-02-12 20:19:23,458] Saving improved model after Val Loss improved from 0.35805 to 0.32735\n",
      "[2024-02-12 20:19:23,562] Epoch: 5 | FT  | Train Loss: 0.32709 | Val Loss: 0.32735\n",
      "[2024-02-12 20:19:23,564] Epoch: 5 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:19:23,565] Epoch: 5 | Ph  | Train Loss: 0.327 | Val Loss: 0.327\n",
      "[2024-02-12 20:19:23,566] Epoch: 5 | Ending LR: 0.000200 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:26<00:00,  2.72it/s]\n",
      "[2024-02-12 20:19:57,344] Saving improved model after Val Loss improved from 0.32735 to 0.31342\n",
      "[2024-02-12 20:19:57,403] Epoch: 6 | FT  | Train Loss: 0.30768 | Val Loss: 0.31342\n",
      "[2024-02-12 20:19:57,407] Epoch: 6 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:19:57,411] Epoch: 6 | Ph  | Train Loss: 0.308 | Val Loss: 0.313\n",
      "[2024-02-12 20:19:57,412] Epoch: 6 | Ending LR: 0.000170 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:26<00:00,  2.71it/s]\n",
      "[2024-02-12 20:20:31,047] Saving improved model after Val Loss improved from 0.31342 to 0.29814\n",
      "[2024-02-12 20:20:31,158] Epoch: 7 | FT  | Train Loss: 0.29213 | Val Loss: 0.29814\n",
      "[2024-02-12 20:20:31,161] Epoch: 7 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:20:31,163] Epoch: 7 | Ph  | Train Loss: 0.292 | Val Loss: 0.298\n",
      "[2024-02-12 20:20:31,164] Epoch: 7 | Ending LR: 0.000140 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:27<00:00,  2.70it/s]\n",
      "[2024-02-12 20:21:04,787] Saving improved model after Val Loss improved from 0.29814 to 0.29137\n",
      "[2024-02-12 20:21:04,872] Epoch: 8 | FT  | Train Loss: 0.28086 | Val Loss: 0.29137\n",
      "[2024-02-12 20:21:04,874] Epoch: 8 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:21:04,875] Epoch: 8 | Ph  | Train Loss: 0.281 | Val Loss: 0.291\n",
      "[2024-02-12 20:21:04,876] Epoch: 8 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:27<00:00,  2.67it/s]\n",
      "[2024-02-12 20:21:39,051] Saving improved model after Val Loss improved from 0.29137 to 0.28073\n",
      "[2024-02-12 20:21:39,136] Epoch: 9 | FT  | Train Loss: 0.27251 | Val Loss: 0.28073\n",
      "[2024-02-12 20:21:39,137] Epoch: 9 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:21:39,138] Epoch: 9 | Ph  | Train Loss: 0.273 | Val Loss: 0.281\n",
      "[2024-02-12 20:21:39,138] Epoch: 9 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:27<00:00,  2.70it/s]\n",
      "[2024-02-12 20:22:13,283] Saving improved model after Val Loss improved from 0.28073 to 0.27912\n",
      "[2024-02-12 20:22:13,393] Epoch: 10 | FT  | Train Loss: 0.26579 | Val Loss: 0.27912\n",
      "[2024-02-12 20:22:13,395] Epoch: 10 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:22:13,396] Epoch: 10 | Ph  | Train Loss: 0.266 | Val Loss: 0.279\n",
      "[2024-02-12 20:22:13,398] Epoch: 10 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:27<00:00,  2.68it/s]\n",
      "[2024-02-12 20:22:47,242] Saving improved model after Val Loss improved from 0.27912 to 0.27205\n",
      "[2024-02-12 20:22:47,358] Epoch: 11 | FT  | Train Loss: 0.26040 | Val Loss: 0.27205\n",
      "[2024-02-12 20:22:47,360] Epoch: 11 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:22:47,361] Epoch: 11 | Ph  | Train Loss: 0.260 | Val Loss: 0.272\n",
      "[2024-02-12 20:22:47,362] Epoch: 11 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:26<00:00,  2.79it/s]\n",
      "[2024-02-12 20:23:19,657] Epoch: 12 | FT  | Train Loss: 0.25802 | Val Loss: 0.27345\n",
      "[2024-02-12 20:23:19,661] Epoch: 12 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:23:19,665] Epoch: 12 | Ph  | Train Loss: 0.258 | Val Loss: 0.273\n",
      "[2024-02-12 20:23:19,668] Epoch: 12 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:25<00:00,  2.86it/s]\n",
      "[2024-02-12 20:23:51,340] Epoch: 13 | FT  | Train Loss: 0.25753 | Val Loss: 0.27278\n",
      "[2024-02-12 20:23:51,355] Epoch: 13 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:23:51,358] Epoch: 13 | Ph  | Train Loss: 0.258 | Val Loss: 0.273\n",
      "[2024-02-12 20:23:51,362] Epoch: 13 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:24<00:00,  2.93it/s]\n",
      "[2024-02-12 20:24:22,578] Epoch: 14 | FT  | Train Loss: 0.25661 | Val Loss: 0.27421\n",
      "[2024-02-12 20:24:22,590] Epoch: 14 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:24:22,591] Epoch: 14 | Ph  | Train Loss: 0.257 | Val Loss: 0.274\n",
      "[2024-02-12 20:24:22,595] Epoch: 14 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:25<00:00,  2.82it/s]\n",
      "[2024-02-12 20:24:54,834] Saving improved model after Val Loss improved from 0.27205 to 0.27103\n",
      "[2024-02-12 20:24:54,987] Epoch: 15 | FT  | Train Loss: 0.25577 | Val Loss: 0.27103\n",
      "[2024-02-12 20:24:54,988] Epoch: 15 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:24:54,989] Epoch: 15 | Ph  | Train Loss: 0.256 | Val Loss: 0.271\n",
      "[2024-02-12 20:24:54,990] Epoch: 15 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:26<00:00,  2.78it/s]\n",
      "[2024-02-12 20:25:27,478] Epoch: 16 | FT  | Train Loss: 0.25471 | Val Loss: 0.27541\n",
      "[2024-02-12 20:25:27,483] Epoch: 16 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:25:27,484] Epoch: 16 | Ph  | Train Loss: 0.255 | Val Loss: 0.275\n",
      "[2024-02-12 20:25:27,486] Epoch: 16 | Ending LR: 0.000095 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:25<00:00,  2.86it/s]\n",
      "[2024-02-12 20:25:59,436] Saving improved model after Val Loss improved from 0.27103 to 0.26795\n",
      "[2024-02-12 20:25:59,543] Epoch: 17 | FT  | Train Loss: 0.25249 | Val Loss: 0.26795\n",
      "[2024-02-12 20:25:59,545] Epoch: 17 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:25:59,546] Epoch: 17 | Ph  | Train Loss: 0.252 | Val Loss: 0.268\n",
      "[2024-02-12 20:25:59,547] Epoch: 17 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:26<00:00,  2.79it/s]\n",
      "[2024-02-12 20:26:32,067] Epoch: 18 | FT  | Train Loss: 0.24960 | Val Loss: 0.27313\n",
      "[2024-02-12 20:26:32,069] Epoch: 18 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:26:32,086] Epoch: 18 | Ph  | Train Loss: 0.250 | Val Loss: 0.273\n",
      "[2024-02-12 20:26:32,090] Epoch: 18 | Ending LR: 0.000095 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:24<00:00,  2.97it/s]\n",
      "[2024-02-12 20:27:02,691] Saving improved model after Val Loss improved from 0.26795 to 0.26457\n",
      "[2024-02-12 20:27:02,799] Epoch: 19 | FT  | Train Loss: 0.24488 | Val Loss: 0.26457\n",
      "[2024-02-12 20:27:02,800] Epoch: 19 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:27:02,801] Epoch: 19 | Ph  | Train Loss: 0.245 | Val Loss: 0.265\n",
      "[2024-02-12 20:27:02,802] Epoch: 19 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:24<00:00,  2.95it/s]\n",
      "[2024-02-12 20:27:33,093] Saving improved model after Val Loss improved from 0.26457 to 0.26333\n",
      "[2024-02-12 20:27:33,196] Epoch: 20 | FT  | Train Loss: 0.24002 | Val Loss: 0.26333\n",
      "[2024-02-12 20:27:33,198] Epoch: 20 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:27:33,199] Epoch: 20 | Ph  | Train Loss: 0.240 | Val Loss: 0.263\n",
      "[2024-02-12 20:27:33,200] Epoch: 20 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:24<00:00,  2.97it/s]\n",
      "[2024-02-12 20:28:03,694] Saving improved model after Val Loss improved from 0.26333 to 0.26195\n",
      "[2024-02-12 20:28:03,784] Epoch: 21 | FT  | Train Loss: 0.23669 | Val Loss: 0.26195\n",
      "[2024-02-12 20:28:03,785] Epoch: 21 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:28:03,786] Epoch: 21 | Ph  | Train Loss: 0.237 | Val Loss: 0.262\n",
      "[2024-02-12 20:28:03,787] Epoch: 21 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:24<00:00,  2.93it/s]\n",
      "[2024-02-12 20:28:34,720] Saving improved model after Val Loss improved from 0.26195 to 0.25447\n",
      "[2024-02-12 20:28:34,822] Epoch: 22 | FT  | Train Loss: 0.23388 | Val Loss: 0.25447\n",
      "[2024-02-12 20:28:34,823] Epoch: 22 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:28:34,825] Epoch: 22 | Ph  | Train Loss: 0.234 | Val Loss: 0.254\n",
      "[2024-02-12 20:28:34,826] Epoch: 22 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:24<00:00,  3.03it/s]\n",
      "[2024-02-12 20:29:04,987] Saving improved model after Val Loss improved from 0.25447 to 0.25412\n",
      "[2024-02-12 20:29:05,071] Epoch: 23 | FT  | Train Loss: 0.23064 | Val Loss: 0.25412\n",
      "[2024-02-12 20:29:05,072] Epoch: 23 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:29:05,072] Epoch: 23 | Ph  | Train Loss: 0.231 | Val Loss: 0.254\n",
      "[2024-02-12 20:29:05,073] Epoch: 23 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:23<00:00,  3.07it/s]\n",
      "[2024-02-12 20:29:34,806] Epoch: 24 | FT  | Train Loss: 0.22938 | Val Loss: 0.25437\n",
      "[2024-02-12 20:29:34,811] Epoch: 24 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:29:34,812] Epoch: 24 | Ph  | Train Loss: 0.229 | Val Loss: 0.254\n",
      "[2024-02-12 20:29:34,814] Epoch: 24 | Ending LR: 0.000028 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:24<00:00,  3.00it/s]\n",
      "[2024-02-12 20:30:05,066] Epoch: 25 | FT  | Train Loss: 0.22946 | Val Loss: 0.25679\n",
      "[2024-02-12 20:30:05,068] Epoch: 25 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:30:05,071] Epoch: 25 | Ph  | Train Loss: 0.229 | Val Loss: 0.257\n",
      "[2024-02-12 20:30:05,073] Epoch: 25 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:24<00:00,  3.02it/s]\n",
      "[2024-02-12 20:30:35,226] Epoch: 26 | FT  | Train Loss: 0.22910 | Val Loss: 0.25809\n",
      "[2024-02-12 20:30:35,229] Epoch: 26 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:30:35,231] Epoch: 26 | Ph  | Train Loss: 0.229 | Val Loss: 0.258\n",
      "[2024-02-12 20:30:35,232] Epoch: 26 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:24<00:00,  3.01it/s]\n",
      "[2024-02-12 20:31:05,638] Saving improved model after Val Loss improved from 0.25412 to 0.25309\n",
      "[2024-02-12 20:31:05,749] Epoch: 27 | FT  | Train Loss: 0.22837 | Val Loss: 0.25309\n",
      "[2024-02-12 20:31:05,751] Epoch: 27 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:31:05,752] Epoch: 27 | Ph  | Train Loss: 0.228 | Val Loss: 0.253\n",
      "[2024-02-12 20:31:05,753] Epoch: 27 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:23<00:00,  3.05it/s]\n",
      "[2024-02-12 20:31:35,362] Epoch: 28 | FT  | Train Loss: 0.22819 | Val Loss: 0.25615\n",
      "[2024-02-12 20:31:35,364] Epoch: 28 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:31:35,364] Epoch: 28 | Ph  | Train Loss: 0.228 | Val Loss: 0.256\n",
      "[2024-02-12 20:31:35,367] Epoch: 28 | Ending LR: 0.000057 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:23<00:00,  3.06it/s]\n",
      "[2024-02-12 20:32:04,985] Epoch: 29 | FT  | Train Loss: 0.22818 | Val Loss: 0.25781\n",
      "[2024-02-12 20:32:04,987] Epoch: 29 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:32:04,990] Epoch: 29 | Ph  | Train Loss: 0.228 | Val Loss: 0.258\n",
      "[2024-02-12 20:32:04,991] Epoch: 29 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:23<00:00,  3.11it/s]\n",
      "[2024-02-12 20:32:34,012] Epoch: 30 | FT  | Train Loss: 0.22804 | Val Loss: 0.25776\n",
      "[2024-02-12 20:32:34,015] Epoch: 30 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:32:34,016] Epoch: 30 | Ph  | Train Loss: 0.228 | Val Loss: 0.258\n",
      "[2024-02-12 20:32:34,017] Epoch: 30 | Ending LR: 0.000057 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:23<00:00,  3.13it/s]\n",
      "[2024-02-12 20:33:03,230] Saving improved model after Val Loss improved from 0.25309 to 0.24959\n",
      "[2024-02-12 20:33:03,314] Epoch: 31 | FT  | Train Loss: 0.22430 | Val Loss: 0.24959\n",
      "[2024-02-12 20:33:03,315] Epoch: 31 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:33:03,316] Epoch: 31 | Ph  | Train Loss: 0.224 | Val Loss: 0.250\n",
      "[2024-02-12 20:33:03,317] Epoch: 31 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:23<00:00,  3.17it/s]\n",
      "[2024-02-12 20:33:31,617] Epoch: 32 | FT  | Train Loss: 0.22147 | Val Loss: 0.25298\n",
      "[2024-02-12 20:33:31,619] Epoch: 32 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:33:31,620] Epoch: 32 | Ph  | Train Loss: 0.221 | Val Loss: 0.253\n",
      "[2024-02-12 20:33:31,621] Epoch: 32 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:26<00:00,  2.79it/s]\n",
      "[2024-02-12 20:34:03,443] Epoch: 33 | FT  | Train Loss: 0.22012 | Val Loss: 0.25306\n",
      "[2024-02-12 20:34:03,449] Epoch: 33 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:34:03,450] Epoch: 33 | Ph  | Train Loss: 0.220 | Val Loss: 0.253\n",
      "[2024-02-12 20:34:03,450] Epoch: 33 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:24<00:00,  2.95it/s]\n",
      "[2024-02-12 20:34:33,675] Saving improved model after Val Loss improved from 0.24959 to 0.24934\n",
      "[2024-02-12 20:34:33,741] Epoch: 34 | FT  | Train Loss: 0.21809 | Val Loss: 0.24934\n",
      "[2024-02-12 20:34:33,745] Epoch: 34 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:34:33,748] Epoch: 34 | Ph  | Train Loss: 0.218 | Val Loss: 0.249\n",
      "[2024-02-12 20:34:33,751] Epoch: 34 | Ending LR: 0.000028 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.18it/s]\n",
      "[2024-02-12 20:35:02,268] Saving improved model after Val Loss improved from 0.24934 to 0.24878\n",
      "[2024-02-12 20:35:02,371] Epoch: 35 | FT  | Train Loss: 0.21635 | Val Loss: 0.24878\n",
      "[2024-02-12 20:35:02,372] Epoch: 35 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:35:02,372] Epoch: 35 | Ph  | Train Loss: 0.216 | Val Loss: 0.249\n",
      "[2024-02-12 20:35:02,373] Epoch: 35 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.23it/s]\n",
      "[2024-02-12 20:35:30,518] Saving improved model after Val Loss improved from 0.24878 to 0.24662\n",
      "[2024-02-12 20:35:30,634] Epoch: 36 | FT  | Train Loss: 0.21511 | Val Loss: 0.24662\n",
      "[2024-02-12 20:35:30,635] Epoch: 36 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:35:30,636] Epoch: 36 | Ph  | Train Loss: 0.215 | Val Loss: 0.247\n",
      "[2024-02-12 20:35:30,638] Epoch: 36 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.24it/s]\n",
      "[2024-02-12 20:35:58,960] Epoch: 37 | FT  | Train Loss: 0.21538 | Val Loss: 0.24711\n",
      "[2024-02-12 20:35:58,962] Epoch: 37 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:35:58,963] Epoch: 37 | Ph  | Train Loss: 0.215 | Val Loss: 0.247\n",
      "[2024-02-12 20:35:58,964] Epoch: 37 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.21it/s]\n",
      "[2024-02-12 20:36:27,054] Epoch: 38 | FT  | Train Loss: 0.21505 | Val Loss: 0.24697\n",
      "[2024-02-12 20:36:27,055] Epoch: 38 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:36:27,057] Epoch: 38 | Ph  | Train Loss: 0.215 | Val Loss: 0.247\n",
      "[2024-02-12 20:36:27,058] Epoch: 38 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.19it/s]\n",
      "[2024-02-12 20:36:55,676] Epoch: 39 | FT  | Train Loss: 0.21481 | Val Loss: 0.24671\n",
      "[2024-02-12 20:36:55,679] Epoch: 39 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:36:55,680] Epoch: 39 | Ph  | Train Loss: 0.215 | Val Loss: 0.247\n",
      "[2024-02-12 20:36:55,680] Epoch: 39 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:23<00:00,  3.13it/s]\n",
      "[2024-02-12 20:37:24,496] Epoch: 40 | FT  | Train Loss: 0.21587 | Val Loss: 0.25278\n",
      "[2024-02-12 20:37:24,501] Epoch: 40 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:37:24,502] Epoch: 40 | Ph  | Train Loss: 0.216 | Val Loss: 0.253\n",
      "[2024-02-12 20:37:24,502] Epoch: 40 | Ending LR: 0.000039 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:23<00:00,  3.17it/s]\n",
      "[2024-02-12 20:37:52,921] Epoch: 41 | FT  | Train Loss: 0.21410 | Val Loss: 0.25387\n",
      "[2024-02-12 20:37:52,924] Epoch: 41 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:37:52,926] Epoch: 41 | Ph  | Train Loss: 0.214 | Val Loss: 0.254\n",
      "[2024-02-12 20:37:52,927] Epoch: 41 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.25it/s]\n",
      "[2024-02-12 20:38:20,936] Saving improved model after Val Loss improved from 0.24662 to 0.24659\n",
      "[2024-02-12 20:38:21,021] Epoch: 42 | FT  | Train Loss: 0.21345 | Val Loss: 0.24659\n",
      "[2024-02-12 20:38:21,022] Epoch: 42 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:38:21,023] Epoch: 42 | Ph  | Train Loss: 0.213 | Val Loss: 0.247\n",
      "[2024-02-12 20:38:21,024] Epoch: 42 | Ending LR: 0.000039 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:23<00:00,  3.12it/s]\n",
      "[2024-02-12 20:38:50,126] Saving improved model after Val Loss improved from 0.24659 to 0.24536\n",
      "[2024-02-12 20:38:50,230] Epoch: 43 | FT  | Train Loss: 0.21173 | Val Loss: 0.24536\n",
      "[2024-02-12 20:38:50,231] Epoch: 43 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:38:50,232] Epoch: 43 | Ph  | Train Loss: 0.212 | Val Loss: 0.245\n",
      "[2024-02-12 20:38:50,233] Epoch: 43 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:23<00:00,  3.17it/s]\n",
      "[2024-02-12 20:39:18,596] Epoch: 44 | FT  | Train Loss: 0.21059 | Val Loss: 0.24714\n",
      "[2024-02-12 20:39:18,598] Epoch: 44 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:39:18,598] Epoch: 44 | Ph  | Train Loss: 0.211 | Val Loss: 0.247\n",
      "[2024-02-12 20:39:18,599] Epoch: 44 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.18it/s]\n",
      "[2024-02-12 20:39:46,610] Epoch: 45 | FT  | Train Loss: 0.20976 | Val Loss: 0.24751\n",
      "[2024-02-12 20:39:46,611] Epoch: 45 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:39:46,612] Epoch: 45 | Ph  | Train Loss: 0.210 | Val Loss: 0.248\n",
      "[2024-02-12 20:39:46,613] Epoch: 45 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:21<00:00,  3.37it/s]\n",
      "[2024-02-12 20:40:13,562] Saving improved model after Val Loss improved from 0.24536 to 0.24425\n",
      "[2024-02-12 20:40:13,670] Epoch: 46 | FT  | Train Loss: 0.20820 | Val Loss: 0.24425\n",
      "[2024-02-12 20:40:13,673] Epoch: 46 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:40:13,674] Epoch: 46 | Ph  | Train Loss: 0.208 | Val Loss: 0.244\n",
      "[2024-02-12 20:40:13,676] Epoch: 46 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:21<00:00,  3.32it/s]\n",
      "[2024-02-12 20:40:41,010] Epoch: 47 | FT  | Train Loss: 0.20713 | Val Loss: 0.24468\n",
      "[2024-02-12 20:40:41,012] Epoch: 47 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:40:41,014] Epoch: 47 | Ph  | Train Loss: 0.207 | Val Loss: 0.245\n",
      "[2024-02-12 20:40:41,014] Epoch: 47 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.26it/s]\n",
      "[2024-02-12 20:41:10,652] Saving improved model after Val Loss improved from 0.24425 to 0.24398\n",
      "[2024-02-12 20:41:10,759] Epoch: 48 | FT  | Train Loss: 0.20657 | Val Loss: 0.24398\n",
      "[2024-02-12 20:41:10,761] Epoch: 48 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:41:10,762] Epoch: 48 | Ph  | Train Loss: 0.207 | Val Loss: 0.244\n",
      "[2024-02-12 20:41:10,763] Epoch: 48 | Ending LR: 0.000022 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:23<00:00,  3.08it/s]\n",
      "[2024-02-12 20:41:39,691] Epoch: 49 | FT  | Train Loss: 0.20633 | Val Loss: 0.24612\n",
      "[2024-02-12 20:41:39,692] Epoch: 49 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:41:39,693] Epoch: 49 | Ph  | Train Loss: 0.206 | Val Loss: 0.246\n",
      "[2024-02-12 20:41:39,694] Epoch: 49 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.20it/s]\n",
      "[2024-02-12 20:42:07,757] Epoch: 50 | FT  | Train Loss: 0.20638 | Val Loss: 0.24687\n",
      "[2024-02-12 20:42:07,758] Epoch: 50 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:42:07,759] Epoch: 50 | Ph  | Train Loss: 0.206 | Val Loss: 0.247\n",
      "[2024-02-12 20:42:07,759] Epoch: 50 | Ending LR: 0.000026 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.24it/s]\n",
      "[2024-02-12 20:42:35,777] Epoch: 51 | FT  | Train Loss: 0.20569 | Val Loss: 0.24404\n",
      "[2024-02-12 20:42:35,778] Epoch: 51 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:42:35,779] Epoch: 51 | Ph  | Train Loss: 0.206 | Val Loss: 0.244\n",
      "[2024-02-12 20:42:35,780] Epoch: 51 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.23it/s]\n",
      "[2024-02-12 20:43:04,007] Epoch: 52 | FT  | Train Loss: 0.20554 | Val Loss: 0.24587\n",
      "[2024-02-12 20:43:04,012] Epoch: 52 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:43:04,014] Epoch: 52 | Ph  | Train Loss: 0.206 | Val Loss: 0.246\n",
      "[2024-02-12 20:43:04,015] Epoch: 52 | Ending LR: 0.000029 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.19it/s]\n",
      "[2024-02-12 20:43:31,962] Epoch: 53 | FT  | Train Loss: 0.20501 | Val Loss: 0.24400\n",
      "[2024-02-12 20:43:31,964] Epoch: 53 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:43:31,965] Epoch: 53 | Ph  | Train Loss: 0.205 | Val Loss: 0.244\n",
      "[2024-02-12 20:43:31,965] Epoch: 53 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.22it/s]\n",
      "[2024-02-12 20:43:59,368] Epoch: 54 | FT  | Train Loss: 0.20491 | Val Loss: 0.24579\n",
      "[2024-02-12 20:43:59,374] Epoch: 54 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:43:59,375] Epoch: 54 | Ph  | Train Loss: 0.205 | Val Loss: 0.246\n",
      "[2024-02-12 20:43:59,376] Epoch: 54 | Ending LR: 0.000029 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.27it/s]\n",
      "[2024-02-12 20:44:26,742] Epoch: 55 | FT  | Train Loss: 0.20430 | Val Loss: 0.24540\n",
      "[2024-02-12 20:44:26,743] Epoch: 55 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:44:26,744] Epoch: 55 | Ph  | Train Loss: 0.204 | Val Loss: 0.245\n",
      "[2024-02-12 20:44:26,745] Epoch: 55 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.20it/s]\n",
      "[2024-02-12 20:44:55,060] Epoch: 56 | FT  | Train Loss: 0.20332 | Val Loss: 0.24500\n",
      "[2024-02-12 20:44:55,062] Epoch: 56 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:44:55,064] Epoch: 56 | Ph  | Train Loss: 0.203 | Val Loss: 0.245\n",
      "[2024-02-12 20:44:55,066] Epoch: 56 | Ending LR: 0.000026 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:23<00:00,  3.15it/s]\n",
      "[2024-02-12 20:45:23,713] Saving improved model after Val Loss improved from 0.24398 to 0.24390\n",
      "[2024-02-12 20:45:23,850] Epoch: 57 | FT  | Train Loss: 0.20219 | Val Loss: 0.24390\n",
      "[2024-02-12 20:45:23,851] Epoch: 57 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:45:23,852] Epoch: 57 | Ph  | Train Loss: 0.202 | Val Loss: 0.244\n",
      "[2024-02-12 20:45:23,853] Epoch: 57 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.19it/s]\n",
      "[2024-02-12 20:45:52,473] Saving improved model after Val Loss improved from 0.24390 to 0.24240\n",
      "[2024-02-12 20:45:52,591] Epoch: 58 | FT  | Train Loss: 0.20142 | Val Loss: 0.24240\n",
      "[2024-02-12 20:45:52,592] Epoch: 58 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:45:52,593] Epoch: 58 | Ph  | Train Loss: 0.201 | Val Loss: 0.242\n",
      "[2024-02-12 20:45:52,594] Epoch: 58 | Ending LR: 0.000022 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.31it/s]\n",
      "[2024-02-12 20:46:19,191] Epoch: 59 | FT  | Train Loss: 0.20063 | Val Loss: 0.24246\n",
      "[2024-02-12 20:46:19,194] Epoch: 59 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:46:19,194] Epoch: 59 | Ph  | Train Loss: 0.201 | Val Loss: 0.242\n",
      "[2024-02-12 20:46:19,195] Epoch: 59 | Ending LR: 0.000020 \n",
      "[2024-02-12 20:46:19,301] Decimating dataset to 0.2 of the original size...\n",
      "[2024-02-12 20:46:19,374] Using DataParallel with 2 devices.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.33it/s]\n",
      "[2024-02-12 20:46:39,766] Saving improved model after Val Loss improved from inf to 0.58541\n",
      "[2024-02-12 20:46:39,799] Epoch: 0 | FT  | Train Loss: 0.67125 | Val Loss: 0.58541\n",
      "[2024-02-12 20:46:39,800] Epoch: 0 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:46:39,801] Epoch: 0 | Ph  | Train Loss: 0.671 | Val Loss: 0.585\n",
      "[2024-02-12 20:46:39,801] Epoch: 0 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.30it/s]\n",
      "[2024-02-12 20:46:59,704] Saving improved model after Val Loss improved from 0.58541 to 0.55713\n",
      "[2024-02-12 20:46:59,784] Epoch: 1 | FT  | Train Loss: 0.55600 | Val Loss: 0.55713\n",
      "[2024-02-12 20:46:59,786] Epoch: 1 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:46:59,787] Epoch: 1 | Ph  | Train Loss: 0.556 | Val Loss: 0.557\n",
      "[2024-02-12 20:46:59,789] Epoch: 1 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.21it/s]\n",
      "[2024-02-12 20:47:20,148] Saving improved model after Val Loss improved from 0.55713 to 0.45534\n",
      "[2024-02-12 20:47:20,252] Epoch: 2 | FT  | Train Loss: 0.48549 | Val Loss: 0.45534\n",
      "[2024-02-12 20:47:20,253] Epoch: 2 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:47:20,255] Epoch: 2 | Ph  | Train Loss: 0.485 | Val Loss: 0.455\n",
      "[2024-02-12 20:47:20,256] Epoch: 2 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.29it/s]\n",
      "[2024-02-12 20:47:40,776] Saving improved model after Val Loss improved from 0.45534 to 0.42026\n",
      "[2024-02-12 20:47:40,867] Epoch: 3 | FT  | Train Loss: 0.41988 | Val Loss: 0.42026\n",
      "[2024-02-12 20:47:40,868] Epoch: 3 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:47:40,869] Epoch: 3 | Ph  | Train Loss: 0.420 | Val Loss: 0.420\n",
      "[2024-02-12 20:47:40,870] Epoch: 3 | Ending LR: 0.000140 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.26it/s]\n",
      "[2024-02-12 20:48:01,040] Saving improved model after Val Loss improved from 0.42026 to 0.39836\n",
      "[2024-02-12 20:48:01,133] Epoch: 4 | FT  | Train Loss: 0.38130 | Val Loss: 0.39836\n",
      "[2024-02-12 20:48:01,134] Epoch: 4 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:48:01,135] Epoch: 4 | Ph  | Train Loss: 0.381 | Val Loss: 0.398\n",
      "[2024-02-12 20:48:01,136] Epoch: 4 | Ending LR: 0.000170 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.32it/s]\n",
      "[2024-02-12 20:48:21,282] Saving improved model after Val Loss improved from 0.39836 to 0.35953\n",
      "[2024-02-12 20:48:21,375] Epoch: 5 | FT  | Train Loss: 0.35297 | Val Loss: 0.35953\n",
      "[2024-02-12 20:48:21,377] Epoch: 5 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:48:21,377] Epoch: 5 | Ph  | Train Loss: 0.353 | Val Loss: 0.360\n",
      "[2024-02-12 20:48:21,378] Epoch: 5 | Ending LR: 0.000200 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.27it/s]\n",
      "[2024-02-12 20:48:41,201] Saving improved model after Val Loss improved from 0.35953 to 0.33409\n",
      "[2024-02-12 20:48:41,288] Epoch: 6 | FT  | Train Loss: 0.33134 | Val Loss: 0.33409\n",
      "[2024-02-12 20:48:41,289] Epoch: 6 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:48:41,290] Epoch: 6 | Ph  | Train Loss: 0.331 | Val Loss: 0.334\n",
      "[2024-02-12 20:48:41,290] Epoch: 6 | Ending LR: 0.000170 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.29it/s]\n",
      "[2024-02-12 20:49:01,729] Saving improved model after Val Loss improved from 0.33409 to 0.32167\n",
      "[2024-02-12 20:49:01,813] Epoch: 7 | FT  | Train Loss: 0.31339 | Val Loss: 0.32167\n",
      "[2024-02-12 20:49:01,814] Epoch: 7 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:49:01,815] Epoch: 7 | Ph  | Train Loss: 0.313 | Val Loss: 0.322\n",
      "[2024-02-12 20:49:01,816] Epoch: 7 | Ending LR: 0.000140 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.25it/s]\n",
      "[2024-02-12 20:49:21,863] Saving improved model after Val Loss improved from 0.32167 to 0.30824\n",
      "[2024-02-12 20:49:21,966] Epoch: 8 | FT  | Train Loss: 0.30012 | Val Loss: 0.30824\n",
      "[2024-02-12 20:49:21,967] Epoch: 8 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:49:21,967] Epoch: 8 | Ph  | Train Loss: 0.300 | Val Loss: 0.308\n",
      "[2024-02-12 20:49:21,968] Epoch: 8 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.28it/s]\n",
      "[2024-02-12 20:49:42,664] Saving improved model after Val Loss improved from 0.30824 to 0.30626\n",
      "[2024-02-12 20:49:42,777] Epoch: 9 | FT  | Train Loss: 0.29031 | Val Loss: 0.30626\n",
      "[2024-02-12 20:49:42,779] Epoch: 9 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:49:42,781] Epoch: 9 | Ph  | Train Loss: 0.290 | Val Loss: 0.306\n",
      "[2024-02-12 20:49:42,782] Epoch: 9 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.29it/s]\n",
      "[2024-02-12 20:50:03,095] Saving improved model after Val Loss improved from 0.30626 to 0.29726\n",
      "[2024-02-12 20:50:03,193] Epoch: 10 | FT  | Train Loss: 0.28315 | Val Loss: 0.29726\n",
      "[2024-02-12 20:50:03,195] Epoch: 10 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:50:03,196] Epoch: 10 | Ph  | Train Loss: 0.283 | Val Loss: 0.297\n",
      "[2024-02-12 20:50:03,197] Epoch: 10 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.37it/s]\n",
      "[2024-02-12 20:50:23,297] Saving improved model after Val Loss improved from 0.29726 to 0.29471\n",
      "[2024-02-12 20:50:23,392] Epoch: 11 | FT  | Train Loss: 0.27790 | Val Loss: 0.29471\n",
      "[2024-02-12 20:50:23,394] Epoch: 11 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:50:23,395] Epoch: 11 | Ph  | Train Loss: 0.278 | Val Loss: 0.295\n",
      "[2024-02-12 20:50:23,397] Epoch: 11 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.33it/s]\n",
      "[2024-02-12 20:50:43,505] Saving improved model after Val Loss improved from 0.29471 to 0.29200\n",
      "[2024-02-12 20:50:43,601] Epoch: 12 | FT  | Train Loss: 0.27506 | Val Loss: 0.29200\n",
      "[2024-02-12 20:50:43,603] Epoch: 12 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:50:43,604] Epoch: 12 | Ph  | Train Loss: 0.275 | Val Loss: 0.292\n",
      "[2024-02-12 20:50:43,607] Epoch: 12 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.22it/s]\n",
      "[2024-02-12 20:51:04,547] Epoch: 13 | FT  | Train Loss: 0.27370 | Val Loss: 0.29451\n",
      "[2024-02-12 20:51:04,550] Epoch: 13 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:51:04,551] Epoch: 13 | Ph  | Train Loss: 0.274 | Val Loss: 0.295\n",
      "[2024-02-12 20:51:04,552] Epoch: 13 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.20it/s]\n",
      "[2024-02-12 20:51:25,190] Epoch: 14 | FT  | Train Loss: 0.27298 | Val Loss: 0.29356\n",
      "[2024-02-12 20:51:25,192] Epoch: 14 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:51:25,193] Epoch: 14 | Ph  | Train Loss: 0.273 | Val Loss: 0.294\n",
      "[2024-02-12 20:51:25,194] Epoch: 14 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.25it/s]\n",
      "[2024-02-12 20:51:45,409] Saving improved model after Val Loss improved from 0.29200 to 0.29185\n",
      "[2024-02-12 20:51:45,509] Epoch: 15 | FT  | Train Loss: 0.27106 | Val Loss: 0.29185\n",
      "[2024-02-12 20:51:45,510] Epoch: 15 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:51:45,510] Epoch: 15 | Ph  | Train Loss: 0.271 | Val Loss: 0.292\n",
      "[2024-02-12 20:51:45,510] Epoch: 15 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.15it/s]\n",
      "[2024-02-12 20:52:06,621] Saving improved model after Val Loss improved from 0.29185 to 0.29048\n",
      "[2024-02-12 20:52:06,712] Epoch: 16 | FT  | Train Loss: 0.27002 | Val Loss: 0.29048\n",
      "[2024-02-12 20:52:06,714] Epoch: 16 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:52:06,715] Epoch: 16 | Ph  | Train Loss: 0.270 | Val Loss: 0.290\n",
      "[2024-02-12 20:52:06,716] Epoch: 16 | Ending LR: 0.000095 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.16it/s]\n",
      "[2024-02-12 20:52:27,875] Saving improved model after Val Loss improved from 0.29048 to 0.28906\n",
      "[2024-02-12 20:52:27,975] Epoch: 17 | FT  | Train Loss: 0.26695 | Val Loss: 0.28906\n",
      "[2024-02-12 20:52:27,977] Epoch: 17 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:52:27,978] Epoch: 17 | Ph  | Train Loss: 0.267 | Val Loss: 0.289\n",
      "[2024-02-12 20:52:27,979] Epoch: 17 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.25it/s]\n",
      "[2024-02-12 20:52:48,500] Saving improved model after Val Loss improved from 0.28906 to 0.28150\n",
      "[2024-02-12 20:52:48,590] Epoch: 18 | FT  | Train Loss: 0.26249 | Val Loss: 0.28150\n",
      "[2024-02-12 20:52:48,591] Epoch: 18 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:52:48,592] Epoch: 18 | Ph  | Train Loss: 0.262 | Val Loss: 0.282\n",
      "[2024-02-12 20:52:48,593] Epoch: 18 | Ending LR: 0.000095 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.23it/s]\n",
      "[2024-02-12 20:53:09,019] Epoch: 19 | FT  | Train Loss: 0.25819 | Val Loss: 0.29434\n",
      "[2024-02-12 20:53:09,021] Epoch: 19 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:53:09,021] Epoch: 19 | Ph  | Train Loss: 0.258 | Val Loss: 0.294\n",
      "[2024-02-12 20:53:09,022] Epoch: 19 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.32it/s]\n",
      "[2024-02-12 20:53:28,796] Saving improved model after Val Loss improved from 0.28150 to 0.27796\n",
      "[2024-02-12 20:53:28,884] Epoch: 20 | FT  | Train Loss: 0.25361 | Val Loss: 0.27796\n",
      "[2024-02-12 20:53:28,885] Epoch: 20 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:53:28,886] Epoch: 20 | Ph  | Train Loss: 0.254 | Val Loss: 0.278\n",
      "[2024-02-12 20:53:28,886] Epoch: 20 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.34it/s]\n",
      "[2024-02-12 20:53:49,118] Saving improved model after Val Loss improved from 0.27796 to 0.27615\n",
      "[2024-02-12 20:53:49,198] Epoch: 21 | FT  | Train Loss: 0.25097 | Val Loss: 0.27615\n",
      "[2024-02-12 20:53:49,200] Epoch: 21 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:53:49,201] Epoch: 21 | Ph  | Train Loss: 0.251 | Val Loss: 0.276\n",
      "[2024-02-12 20:53:49,201] Epoch: 21 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.28it/s]\n",
      "[2024-02-12 20:54:08,812] Saving improved model after Val Loss improved from 0.27615 to 0.27584\n",
      "[2024-02-12 20:54:08,903] Epoch: 22 | FT  | Train Loss: 0.24641 | Val Loss: 0.27584\n",
      "[2024-02-12 20:54:08,904] Epoch: 22 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:54:08,904] Epoch: 22 | Ph  | Train Loss: 0.246 | Val Loss: 0.276\n",
      "[2024-02-12 20:54:08,905] Epoch: 22 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.46it/s]\n",
      "[2024-02-12 20:54:28,214] Saving improved model after Val Loss improved from 0.27584 to 0.27288\n",
      "[2024-02-12 20:54:28,314] Epoch: 23 | FT  | Train Loss: 0.24428 | Val Loss: 0.27288\n",
      "[2024-02-12 20:54:28,316] Epoch: 23 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:54:28,318] Epoch: 23 | Ph  | Train Loss: 0.244 | Val Loss: 0.273\n",
      "[2024-02-12 20:54:28,319] Epoch: 23 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.33it/s]\n",
      "[2024-02-12 20:54:48,492] Epoch: 24 | FT  | Train Loss: 0.24242 | Val Loss: 0.27465\n",
      "[2024-02-12 20:54:48,494] Epoch: 24 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:54:48,495] Epoch: 24 | Ph  | Train Loss: 0.242 | Val Loss: 0.275\n",
      "[2024-02-12 20:54:48,496] Epoch: 24 | Ending LR: 0.000028 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.29it/s]\n",
      "[2024-02-12 20:55:08,528] Epoch: 25 | FT  | Train Loss: 0.24220 | Val Loss: 0.27632\n",
      "[2024-02-12 20:55:08,531] Epoch: 25 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:55:08,532] Epoch: 25 | Ph  | Train Loss: 0.242 | Val Loss: 0.276\n",
      "[2024-02-12 20:55:08,533] Epoch: 25 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.10it/s]\n",
      "[2024-02-12 20:55:30,113] Epoch: 26 | FT  | Train Loss: 0.24260 | Val Loss: 0.27551\n",
      "[2024-02-12 20:55:30,115] Epoch: 26 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:55:30,116] Epoch: 26 | Ph  | Train Loss: 0.243 | Val Loss: 0.276\n",
      "[2024-02-12 20:55:30,117] Epoch: 26 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.17it/s]\n",
      "[2024-02-12 20:55:51,004] Epoch: 27 | FT  | Train Loss: 0.24226 | Val Loss: 0.27336\n",
      "[2024-02-12 20:55:51,006] Epoch: 27 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:55:51,006] Epoch: 27 | Ph  | Train Loss: 0.242 | Val Loss: 0.273\n",
      "[2024-02-12 20:55:51,007] Epoch: 27 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.12it/s]\n",
      "[2024-02-12 20:56:12,057] Epoch: 28 | FT  | Train Loss: 0.24017 | Val Loss: 0.27455\n",
      "[2024-02-12 20:56:12,060] Epoch: 28 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:56:12,062] Epoch: 28 | Ph  | Train Loss: 0.240 | Val Loss: 0.275\n",
      "[2024-02-12 20:56:12,063] Epoch: 28 | Ending LR: 0.000057 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.20it/s]\n",
      "[2024-02-12 20:56:32,284] Epoch: 29 | FT  | Train Loss: 0.24040 | Val Loss: 0.27557\n",
      "[2024-02-12 20:56:32,285] Epoch: 29 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:56:32,286] Epoch: 29 | Ph  | Train Loss: 0.240 | Val Loss: 0.276\n",
      "[2024-02-12 20:56:32,287] Epoch: 29 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.25it/s]\n",
      "[2024-02-12 20:56:52,792] Epoch: 30 | FT  | Train Loss: 0.23915 | Val Loss: 0.27536\n",
      "[2024-02-12 20:56:52,793] Epoch: 30 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:56:52,794] Epoch: 30 | Ph  | Train Loss: 0.239 | Val Loss: 0.275\n",
      "[2024-02-12 20:56:52,795] Epoch: 30 | Ending LR: 0.000057 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.24it/s]\n",
      "[2024-02-12 20:57:13,448] Saving improved model after Val Loss improved from 0.27288 to 0.27065\n",
      "[2024-02-12 20:57:13,548] Epoch: 31 | FT  | Train Loss: 0.23739 | Val Loss: 0.27065\n",
      "[2024-02-12 20:57:13,549] Epoch: 31 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:57:13,550] Epoch: 31 | Ph  | Train Loss: 0.237 | Val Loss: 0.271\n",
      "[2024-02-12 20:57:13,551] Epoch: 31 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.13it/s]\n",
      "[2024-02-12 20:57:34,939] Saving improved model after Val Loss improved from 0.27065 to 0.27037\n",
      "[2024-02-12 20:57:35,054] Epoch: 32 | FT  | Train Loss: 0.23412 | Val Loss: 0.27037\n",
      "[2024-02-12 20:57:35,056] Epoch: 32 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:57:35,058] Epoch: 32 | Ph  | Train Loss: 0.234 | Val Loss: 0.270\n",
      "[2024-02-12 20:57:35,059] Epoch: 32 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.24it/s]\n",
      "[2024-02-12 20:57:55,510] Saving improved model after Val Loss improved from 0.27037 to 0.26912\n",
      "[2024-02-12 20:57:55,605] Epoch: 33 | FT  | Train Loss: 0.23291 | Val Loss: 0.26912\n",
      "[2024-02-12 20:57:55,606] Epoch: 33 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:57:55,607] Epoch: 33 | Ph  | Train Loss: 0.233 | Val Loss: 0.269\n",
      "[2024-02-12 20:57:55,608] Epoch: 33 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.18it/s]\n",
      "[2024-02-12 20:58:16,595] Epoch: 34 | FT  | Train Loss: 0.23032 | Val Loss: 0.26938\n",
      "[2024-02-12 20:58:16,597] Epoch: 34 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:58:16,599] Epoch: 34 | Ph  | Train Loss: 0.230 | Val Loss: 0.269\n",
      "[2024-02-12 20:58:16,600] Epoch: 34 | Ending LR: 0.000028 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.22it/s]\n",
      "[2024-02-12 20:58:36,969] Saving improved model after Val Loss improved from 0.26912 to 0.26767\n",
      "[2024-02-12 20:58:37,080] Epoch: 35 | FT  | Train Loss: 0.22866 | Val Loss: 0.26767\n",
      "[2024-02-12 20:58:37,082] Epoch: 35 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:58:37,083] Epoch: 35 | Ph  | Train Loss: 0.229 | Val Loss: 0.268\n",
      "[2024-02-12 20:58:37,085] Epoch: 35 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.32it/s]\n",
      "[2024-02-12 20:58:57,566] Epoch: 36 | FT  | Train Loss: 0.22727 | Val Loss: 0.26807\n",
      "[2024-02-12 20:58:57,567] Epoch: 36 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:58:57,568] Epoch: 36 | Ph  | Train Loss: 0.227 | Val Loss: 0.268\n",
      "[2024-02-12 20:58:57,569] Epoch: 36 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.30it/s]\n",
      "[2024-02-12 20:59:17,955] Epoch: 37 | FT  | Train Loss: 0.22743 | Val Loss: 0.26939\n",
      "[2024-02-12 20:59:17,956] Epoch: 37 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:59:17,957] Epoch: 37 | Ph  | Train Loss: 0.227 | Val Loss: 0.269\n",
      "[2024-02-12 20:59:17,958] Epoch: 37 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.18it/s]\n",
      "[2024-02-12 20:59:38,982] Epoch: 38 | FT  | Train Loss: 0.22694 | Val Loss: 0.26903\n",
      "[2024-02-12 20:59:38,985] Epoch: 38 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:59:38,986] Epoch: 38 | Ph  | Train Loss: 0.227 | Val Loss: 0.269\n",
      "[2024-02-12 20:59:38,988] Epoch: 38 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.14it/s]\n",
      "[2024-02-12 20:59:59,713] Epoch: 39 | FT  | Train Loss: 0.22684 | Val Loss: 0.26822\n",
      "[2024-02-12 20:59:59,715] Epoch: 39 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 20:59:59,715] Epoch: 39 | Ph  | Train Loss: 0.227 | Val Loss: 0.268\n",
      "[2024-02-12 20:59:59,716] Epoch: 39 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.28it/s]\n",
      "[2024-02-12 21:00:19,729] Epoch: 40 | FT  | Train Loss: 0.22625 | Val Loss: 0.27069\n",
      "[2024-02-12 21:00:19,730] Epoch: 40 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:00:19,731] Epoch: 40 | Ph  | Train Loss: 0.226 | Val Loss: 0.271\n",
      "[2024-02-12 21:00:19,732] Epoch: 40 | Ending LR: 0.000039 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.18it/s]\n",
      "[2024-02-12 21:00:40,701] Saving improved model after Val Loss improved from 0.26767 to 0.26696\n",
      "[2024-02-12 21:00:40,812] Epoch: 41 | FT  | Train Loss: 0.22598 | Val Loss: 0.26696\n",
      "[2024-02-12 21:00:40,813] Epoch: 41 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:00:40,815] Epoch: 41 | Ph  | Train Loss: 0.226 | Val Loss: 0.267\n",
      "[2024-02-12 21:00:40,816] Epoch: 41 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.09it/s]\n",
      "[2024-02-12 21:01:02,291] Epoch: 42 | FT  | Train Loss: 0.22433 | Val Loss: 0.27056\n",
      "[2024-02-12 21:01:02,292] Epoch: 42 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:01:02,294] Epoch: 42 | Ph  | Train Loss: 0.224 | Val Loss: 0.271\n",
      "[2024-02-12 21:01:02,294] Epoch: 42 | Ending LR: 0.000039 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.16it/s]\n",
      "[2024-02-12 21:01:23,490] Saving improved model after Val Loss improved from 0.26696 to 0.26674\n",
      "[2024-02-12 21:01:23,581] Epoch: 43 | FT  | Train Loss: 0.22354 | Val Loss: 0.26674\n",
      "[2024-02-12 21:01:23,584] Epoch: 43 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:01:23,585] Epoch: 43 | Ph  | Train Loss: 0.224 | Val Loss: 0.267\n",
      "[2024-02-12 21:01:23,587] Epoch: 43 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.12it/s]\n",
      "[2024-02-12 21:01:44,910] Epoch: 44 | FT  | Train Loss: 0.22146 | Val Loss: 0.26902\n",
      "[2024-02-12 21:01:44,912] Epoch: 44 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:01:44,912] Epoch: 44 | Ph  | Train Loss: 0.221 | Val Loss: 0.269\n",
      "[2024-02-12 21:01:44,913] Epoch: 44 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.16it/s]\n",
      "[2024-02-12 21:02:05,985] Epoch: 45 | FT  | Train Loss: 0.22002 | Val Loss: 0.26900\n",
      "[2024-02-12 21:02:05,988] Epoch: 45 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:02:05,989] Epoch: 45 | Ph  | Train Loss: 0.220 | Val Loss: 0.269\n",
      "[2024-02-12 21:02:05,990] Epoch: 45 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.13it/s]\n",
      "[2024-02-12 21:02:26,684] Saving improved model after Val Loss improved from 0.26674 to 0.26611\n",
      "[2024-02-12 21:02:26,779] Epoch: 46 | FT  | Train Loss: 0.21928 | Val Loss: 0.26611\n",
      "[2024-02-12 21:02:26,780] Epoch: 46 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:02:26,782] Epoch: 46 | Ph  | Train Loss: 0.219 | Val Loss: 0.266\n",
      "[2024-02-12 21:02:26,782] Epoch: 46 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.38it/s]\n",
      "[2024-02-12 21:02:46,077] Saving improved model after Val Loss improved from 0.26611 to 0.26500\n",
      "[2024-02-12 21:02:46,176] Epoch: 47 | FT  | Train Loss: 0.21806 | Val Loss: 0.26500\n",
      "[2024-02-12 21:02:46,177] Epoch: 47 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:02:46,178] Epoch: 47 | Ph  | Train Loss: 0.218 | Val Loss: 0.265\n",
      "[2024-02-12 21:02:46,178] Epoch: 47 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.17it/s]\n",
      "[2024-02-12 21:03:06,646] Saving improved model after Val Loss improved from 0.26500 to 0.26498\n",
      "[2024-02-12 21:03:06,765] Epoch: 48 | FT  | Train Loss: 0.21732 | Val Loss: 0.26498\n",
      "[2024-02-12 21:03:06,767] Epoch: 48 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:03:06,768] Epoch: 48 | Ph  | Train Loss: 0.217 | Val Loss: 0.265\n",
      "[2024-02-12 21:03:06,770] Epoch: 48 | Ending LR: 0.000022 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.11it/s]\n",
      "[2024-02-12 21:03:28,231] Epoch: 49 | FT  | Train Loss: 0.21792 | Val Loss: 0.26635\n",
      "[2024-02-12 21:03:28,233] Epoch: 49 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:03:28,233] Epoch: 49 | Ph  | Train Loss: 0.218 | Val Loss: 0.266\n",
      "[2024-02-12 21:03:28,234] Epoch: 49 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.22it/s]\n",
      "[2024-02-12 21:03:48,909] Epoch: 50 | FT  | Train Loss: 0.21705 | Val Loss: 0.26756\n",
      "[2024-02-12 21:03:48,911] Epoch: 50 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:03:48,911] Epoch: 50 | Ph  | Train Loss: 0.217 | Val Loss: 0.268\n",
      "[2024-02-12 21:03:48,912] Epoch: 50 | Ending LR: 0.000026 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.20it/s]\n",
      "[2024-02-12 21:04:09,640] Epoch: 51 | FT  | Train Loss: 0.21656 | Val Loss: 0.26684\n",
      "[2024-02-12 21:04:09,642] Epoch: 51 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:04:09,642] Epoch: 51 | Ph  | Train Loss: 0.217 | Val Loss: 0.267\n",
      "[2024-02-12 21:04:09,643] Epoch: 51 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.23it/s]\n",
      "[2024-02-12 21:04:30,086] Epoch: 52 | FT  | Train Loss: 0.21643 | Val Loss: 0.26550\n",
      "[2024-02-12 21:04:30,087] Epoch: 52 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:04:30,088] Epoch: 52 | Ph  | Train Loss: 0.216 | Val Loss: 0.265\n",
      "[2024-02-12 21:04:30,089] Epoch: 52 | Ending LR: 0.000029 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.19it/s]\n",
      "[2024-02-12 21:04:51,423] Epoch: 53 | FT  | Train Loss: 0.21636 | Val Loss: 0.26566\n",
      "[2024-02-12 21:04:51,424] Epoch: 53 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:04:51,425] Epoch: 53 | Ph  | Train Loss: 0.216 | Val Loss: 0.266\n",
      "[2024-02-12 21:04:51,425] Epoch: 53 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.09it/s]\n",
      "[2024-02-12 21:05:12,625] Epoch: 54 | FT  | Train Loss: 0.21513 | Val Loss: 0.26541\n",
      "[2024-02-12 21:05:12,627] Epoch: 54 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:05:12,628] Epoch: 54 | Ph  | Train Loss: 0.215 | Val Loss: 0.265\n",
      "[2024-02-12 21:05:12,629] Epoch: 54 | Ending LR: 0.000029 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.21it/s]\n",
      "[2024-02-12 21:05:33,628] Epoch: 55 | FT  | Train Loss: 0.21448 | Val Loss: 0.26538\n",
      "[2024-02-12 21:05:33,629] Epoch: 55 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:05:33,630] Epoch: 55 | Ph  | Train Loss: 0.214 | Val Loss: 0.265\n",
      "[2024-02-12 21:05:33,632] Epoch: 55 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.07it/s]\n",
      "[2024-02-12 21:05:55,300] Epoch: 56 | FT  | Train Loss: 0.21332 | Val Loss: 0.26683\n",
      "[2024-02-12 21:05:55,301] Epoch: 56 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:05:55,301] Epoch: 56 | Ph  | Train Loss: 0.213 | Val Loss: 0.267\n",
      "[2024-02-12 21:05:55,302] Epoch: 56 | Ending LR: 0.000026 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.21it/s]\n",
      "[2024-02-12 21:06:16,231] Epoch: 57 | FT  | Train Loss: 0.21265 | Val Loss: 0.26521\n",
      "[2024-02-12 21:06:16,234] Epoch: 57 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:06:16,235] Epoch: 57 | Ph  | Train Loss: 0.213 | Val Loss: 0.265\n",
      "[2024-02-12 21:06:16,236] Epoch: 57 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:14<00:00,  3.28it/s]\n",
      "[2024-02-12 21:06:36,405] Saving improved model after Val Loss improved from 0.26498 to 0.26412\n",
      "[2024-02-12 21:06:36,515] Epoch: 58 | FT  | Train Loss: 0.21202 | Val Loss: 0.26412\n",
      "[2024-02-12 21:06:36,516] Epoch: 58 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:06:36,517] Epoch: 58 | Ph  | Train Loss: 0.212 | Val Loss: 0.264\n",
      "[2024-02-12 21:06:36,518] Epoch: 58 | Ending LR: 0.000022 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:15<00:00,  3.23it/s]\n",
      "[2024-02-12 21:06:57,432] Epoch: 59 | FT  | Train Loss: 0.21125 | Val Loss: 0.26556\n",
      "[2024-02-12 21:06:57,434] Epoch: 59 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:06:57,434] Epoch: 59 | Ph  | Train Loss: 0.211 | Val Loss: 0.266\n",
      "[2024-02-12 21:06:57,435] Epoch: 59 | Ending LR: 0.000020 \n",
      "[2024-02-12 21:06:57,559] Decimating dataset to 0.1 of the original size...\n",
      "[2024-02-12 21:06:57,662] Using DataParallel with 2 devices.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.24it/s]\n",
      "[2024-02-12 21:07:10,317] Saving improved model after Val Loss improved from inf to 0.67765\n",
      "[2024-02-12 21:07:10,401] Epoch: 0 | FT  | Train Loss: 0.72260 | Val Loss: 0.67765\n",
      "[2024-02-12 21:07:10,402] Epoch: 0 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:07:10,402] Epoch: 0 | Ph  | Train Loss: 0.723 | Val Loss: 0.678\n",
      "[2024-02-12 21:07:10,403] Epoch: 0 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.42it/s]\n",
      "[2024-02-12 21:07:23,568] Saving improved model after Val Loss improved from 0.67765 to 0.57715\n",
      "[2024-02-12 21:07:23,659] Epoch: 1 | FT  | Train Loss: 0.59414 | Val Loss: 0.57715\n",
      "[2024-02-12 21:07:23,661] Epoch: 1 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:07:23,662] Epoch: 1 | Ph  | Train Loss: 0.594 | Val Loss: 0.577\n",
      "[2024-02-12 21:07:23,663] Epoch: 1 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:08<00:00,  3.05it/s]\n",
      "[2024-02-12 21:07:37,489] Saving improved model after Val Loss improved from 0.57715 to 0.55777\n",
      "[2024-02-12 21:07:37,572] Epoch: 2 | FT  | Train Loss: 0.55810 | Val Loss: 0.55777\n",
      "[2024-02-12 21:07:37,574] Epoch: 2 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:07:37,575] Epoch: 2 | Ph  | Train Loss: 0.558 | Val Loss: 0.558\n",
      "[2024-02-12 21:07:37,576] Epoch: 2 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.17it/s]\n",
      "[2024-02-12 21:07:51,366] Saving improved model after Val Loss improved from 0.55777 to 0.51785\n",
      "[2024-02-12 21:07:51,467] Epoch: 3 | FT  | Train Loss: 0.52496 | Val Loss: 0.51785\n",
      "[2024-02-12 21:07:51,469] Epoch: 3 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:07:51,470] Epoch: 3 | Ph  | Train Loss: 0.525 | Val Loss: 0.518\n",
      "[2024-02-12 21:07:51,471] Epoch: 3 | Ending LR: 0.000140 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.34it/s]\n",
      "[2024-02-12 21:08:04,388] Saving improved model after Val Loss improved from 0.51785 to 0.49691\n",
      "[2024-02-12 21:08:04,476] Epoch: 4 | FT  | Train Loss: 0.48181 | Val Loss: 0.49691\n",
      "[2024-02-12 21:08:04,478] Epoch: 4 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:08:04,480] Epoch: 4 | Ph  | Train Loss: 0.482 | Val Loss: 0.497\n",
      "[2024-02-12 21:08:04,481] Epoch: 4 | Ending LR: 0.000170 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.25it/s]\n",
      "[2024-02-12 21:08:17,939] Saving improved model after Val Loss improved from 0.49691 to 0.44477\n",
      "[2024-02-12 21:08:18,026] Epoch: 5 | FT  | Train Loss: 0.43819 | Val Loss: 0.44477\n",
      "[2024-02-12 21:08:18,027] Epoch: 5 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:08:18,028] Epoch: 5 | Ph  | Train Loss: 0.438 | Val Loss: 0.445\n",
      "[2024-02-12 21:08:18,030] Epoch: 5 | Ending LR: 0.000200 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.30it/s]\n",
      "[2024-02-12 21:08:30,874] Saving improved model after Val Loss improved from 0.44477 to 0.40933\n",
      "[2024-02-12 21:08:30,974] Epoch: 6 | FT  | Train Loss: 0.40462 | Val Loss: 0.40933\n",
      "[2024-02-12 21:08:30,976] Epoch: 6 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:08:30,978] Epoch: 6 | Ph  | Train Loss: 0.405 | Val Loss: 0.409\n",
      "[2024-02-12 21:08:30,979] Epoch: 6 | Ending LR: 0.000170 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.20it/s]\n",
      "[2024-02-12 21:08:44,116] Saving improved model after Val Loss improved from 0.40933 to 0.39557\n",
      "[2024-02-12 21:08:44,201] Epoch: 7 | FT  | Train Loss: 0.37750 | Val Loss: 0.39557\n",
      "[2024-02-12 21:08:44,203] Epoch: 7 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:08:44,204] Epoch: 7 | Ph  | Train Loss: 0.377 | Val Loss: 0.396\n",
      "[2024-02-12 21:08:44,205] Epoch: 7 | Ending LR: 0.000140 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:08<00:00,  2.84it/s]\n",
      "[2024-02-12 21:08:58,847] Saving improved model after Val Loss improved from 0.39557 to 0.38440\n",
      "[2024-02-12 21:08:58,955] Epoch: 8 | FT  | Train Loss: 0.36070 | Val Loss: 0.38440\n",
      "[2024-02-12 21:08:58,957] Epoch: 8 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:08:58,959] Epoch: 8 | Ph  | Train Loss: 0.361 | Val Loss: 0.384\n",
      "[2024-02-12 21:08:58,962] Epoch: 8 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.19it/s]\n",
      "[2024-02-12 21:09:12,851] Saving improved model after Val Loss improved from 0.38440 to 0.36097\n",
      "[2024-02-12 21:09:12,942] Epoch: 9 | FT  | Train Loss: 0.34638 | Val Loss: 0.36097\n",
      "[2024-02-12 21:09:12,943] Epoch: 9 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:09:12,944] Epoch: 9 | Ph  | Train Loss: 0.346 | Val Loss: 0.361\n",
      "[2024-02-12 21:09:12,945] Epoch: 9 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.24it/s]\n",
      "[2024-02-12 21:09:25,655] Saving improved model after Val Loss improved from 0.36097 to 0.35450\n",
      "[2024-02-12 21:09:25,783] Epoch: 10 | FT  | Train Loss: 0.33601 | Val Loss: 0.35450\n",
      "[2024-02-12 21:09:25,784] Epoch: 10 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:09:25,786] Epoch: 10 | Ph  | Train Loss: 0.336 | Val Loss: 0.354\n",
      "[2024-02-12 21:09:25,787] Epoch: 10 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.26it/s]\n",
      "[2024-02-12 21:09:39,210] Saving improved model after Val Loss improved from 0.35450 to 0.35194\n",
      "[2024-02-12 21:09:39,317] Epoch: 11 | FT  | Train Loss: 0.32771 | Val Loss: 0.35194\n",
      "[2024-02-12 21:09:39,319] Epoch: 11 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:09:39,320] Epoch: 11 | Ph  | Train Loss: 0.328 | Val Loss: 0.352\n",
      "[2024-02-12 21:09:39,322] Epoch: 11 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:08<00:00,  3.06it/s]\n",
      "[2024-02-12 21:09:53,193] Epoch: 12 | FT  | Train Loss: 0.32407 | Val Loss: 0.35197\n",
      "[2024-02-12 21:09:53,195] Epoch: 12 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:09:53,196] Epoch: 12 | Ph  | Train Loss: 0.324 | Val Loss: 0.352\n",
      "[2024-02-12 21:09:53,197] Epoch: 12 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.20it/s]\n",
      "[2024-02-12 21:10:06,248] Saving improved model after Val Loss improved from 0.35194 to 0.34853\n",
      "[2024-02-12 21:10:06,334] Epoch: 13 | FT  | Train Loss: 0.32248 | Val Loss: 0.34853\n",
      "[2024-02-12 21:10:06,335] Epoch: 13 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:10:06,335] Epoch: 13 | Ph  | Train Loss: 0.322 | Val Loss: 0.349\n",
      "[2024-02-12 21:10:06,336] Epoch: 13 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.31it/s]\n",
      "[2024-02-12 21:10:19,556] Saving improved model after Val Loss improved from 0.34853 to 0.34697\n",
      "[2024-02-12 21:10:19,634] Epoch: 14 | FT  | Train Loss: 0.31906 | Val Loss: 0.34697\n",
      "[2024-02-12 21:10:19,635] Epoch: 14 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:10:19,636] Epoch: 14 | Ph  | Train Loss: 0.319 | Val Loss: 0.347\n",
      "[2024-02-12 21:10:19,636] Epoch: 14 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.17it/s]\n",
      "[2024-02-12 21:10:32,692] Saving improved model after Val Loss improved from 0.34697 to 0.34683\n",
      "[2024-02-12 21:10:32,779] Epoch: 15 | FT  | Train Loss: 0.31533 | Val Loss: 0.34683\n",
      "[2024-02-12 21:10:32,781] Epoch: 15 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:10:32,782] Epoch: 15 | Ph  | Train Loss: 0.315 | Val Loss: 0.347\n",
      "[2024-02-12 21:10:32,783] Epoch: 15 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:08<00:00,  3.08it/s]\n",
      "[2024-02-12 21:10:46,520] Epoch: 16 | FT  | Train Loss: 0.31291 | Val Loss: 0.35434\n",
      "[2024-02-12 21:10:46,523] Epoch: 16 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:10:46,525] Epoch: 16 | Ph  | Train Loss: 0.313 | Val Loss: 0.354\n",
      "[2024-02-12 21:10:46,526] Epoch: 16 | Ending LR: 0.000095 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.36it/s]\n",
      "[2024-02-12 21:10:59,085] Epoch: 17 | FT  | Train Loss: 0.30952 | Val Loss: 0.36253\n",
      "[2024-02-12 21:10:59,087] Epoch: 17 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:10:59,088] Epoch: 17 | Ph  | Train Loss: 0.310 | Val Loss: 0.363\n",
      "[2024-02-12 21:10:59,089] Epoch: 17 | Ending LR: 0.000110 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.26it/s]\n",
      "[2024-02-12 21:11:12,328] Saving improved model after Val Loss improved from 0.34683 to 0.34550\n",
      "[2024-02-12 21:11:12,424] Epoch: 18 | FT  | Train Loss: 0.30447 | Val Loss: 0.34550\n",
      "[2024-02-12 21:11:12,426] Epoch: 18 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:11:12,427] Epoch: 18 | Ph  | Train Loss: 0.304 | Val Loss: 0.345\n",
      "[2024-02-12 21:11:12,429] Epoch: 18 | Ending LR: 0.000095 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:08<00:00,  2.85it/s]\n",
      "[2024-02-12 21:11:27,127] Saving improved model after Val Loss improved from 0.34550 to 0.33848\n",
      "[2024-02-12 21:11:27,209] Epoch: 19 | FT  | Train Loss: 0.29948 | Val Loss: 0.33848\n",
      "[2024-02-12 21:11:27,211] Epoch: 19 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:11:27,212] Epoch: 19 | Ph  | Train Loss: 0.299 | Val Loss: 0.338\n",
      "[2024-02-12 21:11:27,213] Epoch: 19 | Ending LR: 0.000080 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.25it/s]\n",
      "[2024-02-12 21:11:40,833] Saving improved model after Val Loss improved from 0.33848 to 0.33141\n",
      "[2024-02-12 21:11:40,929] Epoch: 20 | FT  | Train Loss: 0.29261 | Val Loss: 0.33141\n",
      "[2024-02-12 21:11:40,931] Epoch: 20 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:11:40,932] Epoch: 20 | Ph  | Train Loss: 0.293 | Val Loss: 0.331\n",
      "[2024-02-12 21:11:40,934] Epoch: 20 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.31it/s]\n",
      "[2024-02-12 21:11:53,825] Saving improved model after Val Loss improved from 0.33141 to 0.32825\n",
      "[2024-02-12 21:11:53,927] Epoch: 21 | FT  | Train Loss: 0.28603 | Val Loss: 0.32825\n",
      "[2024-02-12 21:11:53,929] Epoch: 21 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:11:53,932] Epoch: 21 | Ph  | Train Loss: 0.286 | Val Loss: 0.328\n",
      "[2024-02-12 21:11:53,933] Epoch: 21 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:08<00:00,  3.11it/s]\n",
      "[2024-02-12 21:12:07,803] Saving improved model after Val Loss improved from 0.32825 to 0.32717\n",
      "[2024-02-12 21:12:07,902] Epoch: 22 | FT  | Train Loss: 0.28211 | Val Loss: 0.32717\n",
      "[2024-02-12 21:12:07,903] Epoch: 22 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:12:07,905] Epoch: 22 | Ph  | Train Loss: 0.282 | Val Loss: 0.327\n",
      "[2024-02-12 21:12:07,906] Epoch: 22 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.22it/s]\n",
      "[2024-02-12 21:12:21,265] Saving improved model after Val Loss improved from 0.32717 to 0.32473\n",
      "[2024-02-12 21:12:21,369] Epoch: 23 | FT  | Train Loss: 0.27802 | Val Loss: 0.32473\n",
      "[2024-02-12 21:12:21,371] Epoch: 23 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:12:21,372] Epoch: 23 | Ph  | Train Loss: 0.278 | Val Loss: 0.325\n",
      "[2024-02-12 21:12:21,374] Epoch: 23 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:08<00:00,  3.07it/s]\n",
      "[2024-02-12 21:12:35,176] Epoch: 24 | FT  | Train Loss: 0.27657 | Val Loss: 0.32650\n",
      "[2024-02-12 21:12:35,179] Epoch: 24 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:12:35,180] Epoch: 24 | Ph  | Train Loss: 0.277 | Val Loss: 0.326\n",
      "[2024-02-12 21:12:35,181] Epoch: 24 | Ending LR: 0.000028 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.16it/s]\n",
      "[2024-02-12 21:12:48,585] Epoch: 25 | FT  | Train Loss: 0.27562 | Val Loss: 0.32798\n",
      "[2024-02-12 21:12:48,589] Epoch: 25 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:12:48,590] Epoch: 25 | Ph  | Train Loss: 0.276 | Val Loss: 0.328\n",
      "[2024-02-12 21:12:48,591] Epoch: 25 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.19it/s]\n",
      "[2024-02-12 21:13:01,998] Epoch: 26 | FT  | Train Loss: 0.27542 | Val Loss: 0.33165\n",
      "[2024-02-12 21:13:02,000] Epoch: 26 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:13:02,000] Epoch: 26 | Ph  | Train Loss: 0.275 | Val Loss: 0.332\n",
      "[2024-02-12 21:13:02,001] Epoch: 26 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.23it/s]\n",
      "[2024-02-12 21:13:15,458] Epoch: 27 | FT  | Train Loss: 0.27517 | Val Loss: 0.33068\n",
      "[2024-02-12 21:13:15,461] Epoch: 27 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:13:15,461] Epoch: 27 | Ph  | Train Loss: 0.275 | Val Loss: 0.331\n",
      "[2024-02-12 21:13:15,462] Epoch: 27 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.21it/s]\n",
      "[2024-02-12 21:13:28,650] Epoch: 28 | FT  | Train Loss: 0.27362 | Val Loss: 0.32764\n",
      "[2024-02-12 21:13:28,652] Epoch: 28 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:13:28,653] Epoch: 28 | Ph  | Train Loss: 0.274 | Val Loss: 0.328\n",
      "[2024-02-12 21:13:28,654] Epoch: 28 | Ending LR: 0.000057 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.18it/s]\n",
      "[2024-02-12 21:13:41,982] Saving improved model after Val Loss improved from 0.32473 to 0.32410\n",
      "[2024-02-12 21:13:42,061] Epoch: 29 | FT  | Train Loss: 0.27290 | Val Loss: 0.32410\n",
      "[2024-02-12 21:13:42,062] Epoch: 29 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:13:42,063] Epoch: 29 | Ph  | Train Loss: 0.273 | Val Loss: 0.324\n",
      "[2024-02-12 21:13:42,064] Epoch: 29 | Ending LR: 0.000065 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.27it/s]\n",
      "[2024-02-12 21:13:55,055] Epoch: 30 | FT  | Train Loss: 0.26957 | Val Loss: 0.33593\n",
      "[2024-02-12 21:13:55,057] Epoch: 30 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:13:55,058] Epoch: 30 | Ph  | Train Loss: 0.270 | Val Loss: 0.336\n",
      "[2024-02-12 21:13:55,058] Epoch: 30 | Ending LR: 0.000057 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.25it/s]\n",
      "[2024-02-12 21:14:07,870] Saving improved model after Val Loss improved from 0.32410 to 0.32373\n",
      "[2024-02-12 21:14:07,967] Epoch: 31 | FT  | Train Loss: 0.26745 | Val Loss: 0.32373\n",
      "[2024-02-12 21:14:07,968] Epoch: 31 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:14:07,968] Epoch: 31 | Ph  | Train Loss: 0.267 | Val Loss: 0.324\n",
      "[2024-02-12 21:14:07,969] Epoch: 31 | Ending LR: 0.000050 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.38it/s]\n",
      "[2024-02-12 21:14:20,173] Epoch: 32 | FT  | Train Loss: 0.26457 | Val Loss: 0.32524\n",
      "[2024-02-12 21:14:20,174] Epoch: 32 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:14:20,175] Epoch: 32 | Ph  | Train Loss: 0.265 | Val Loss: 0.325\n",
      "[2024-02-12 21:14:20,176] Epoch: 32 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:08<00:00,  3.01it/s]\n",
      "[2024-02-12 21:14:34,786] Saving improved model after Val Loss improved from 0.32373 to 0.32253\n",
      "[2024-02-12 21:14:34,873] Epoch: 33 | FT  | Train Loss: 0.26043 | Val Loss: 0.32253\n",
      "[2024-02-12 21:14:34,874] Epoch: 33 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:14:34,875] Epoch: 33 | Ph  | Train Loss: 0.260 | Val Loss: 0.323\n",
      "[2024-02-12 21:14:34,876] Epoch: 33 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.70it/s]\n",
      "[2024-02-12 21:14:45,964] Saving improved model after Val Loss improved from 0.32253 to 0.31991\n",
      "[2024-02-12 21:14:46,057] Epoch: 34 | FT  | Train Loss: 0.25734 | Val Loss: 0.31991\n",
      "[2024-02-12 21:14:46,059] Epoch: 34 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:14:46,059] Epoch: 34 | Ph  | Train Loss: 0.257 | Val Loss: 0.320\n",
      "[2024-02-12 21:14:46,060] Epoch: 34 | Ending LR: 0.000028 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.55it/s]\n",
      "[2024-02-12 21:14:57,345] Epoch: 35 | FT  | Train Loss: 0.25660 | Val Loss: 0.32059\n",
      "[2024-02-12 21:14:57,346] Epoch: 35 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:14:57,347] Epoch: 35 | Ph  | Train Loss: 0.257 | Val Loss: 0.321\n",
      "[2024-02-12 21:14:57,347] Epoch: 35 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.61it/s]\n",
      "[2024-02-12 21:15:08,746] Epoch: 36 | FT  | Train Loss: 0.25415 | Val Loss: 0.32056\n",
      "[2024-02-12 21:15:08,748] Epoch: 36 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:15:08,748] Epoch: 36 | Ph  | Train Loss: 0.254 | Val Loss: 0.321\n",
      "[2024-02-12 21:15:08,749] Epoch: 36 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.52it/s]\n",
      "[2024-02-12 21:15:20,082] Epoch: 37 | FT  | Train Loss: 0.25388 | Val Loss: 0.32171\n",
      "[2024-02-12 21:15:20,083] Epoch: 37 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:15:20,084] Epoch: 37 | Ph  | Train Loss: 0.254 | Val Loss: 0.322\n",
      "[2024-02-12 21:15:20,084] Epoch: 37 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.65it/s]\n",
      "[2024-02-12 21:15:31,085] Epoch: 38 | FT  | Train Loss: 0.25435 | Val Loss: 0.32235\n",
      "[2024-02-12 21:15:31,087] Epoch: 38 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:15:31,088] Epoch: 38 | Ph  | Train Loss: 0.254 | Val Loss: 0.322\n",
      "[2024-02-12 21:15:31,088] Epoch: 38 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.71it/s]\n",
      "[2024-02-12 21:15:42,048] Epoch: 39 | FT  | Train Loss: 0.25370 | Val Loss: 0.32115\n",
      "[2024-02-12 21:15:42,050] Epoch: 39 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:15:42,050] Epoch: 39 | Ph  | Train Loss: 0.254 | Val Loss: 0.321\n",
      "[2024-02-12 21:15:42,051] Epoch: 39 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.71it/s]\n",
      "[2024-02-12 21:15:53,037] Epoch: 40 | FT  | Train Loss: 0.25220 | Val Loss: 0.32042\n",
      "[2024-02-12 21:15:53,038] Epoch: 40 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:15:53,039] Epoch: 40 | Ph  | Train Loss: 0.252 | Val Loss: 0.320\n",
      "[2024-02-12 21:15:53,040] Epoch: 40 | Ending LR: 0.000039 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.63it/s]\n",
      "[2024-02-12 21:16:04,516] Epoch: 41 | FT  | Train Loss: 0.25225 | Val Loss: 0.32302\n",
      "[2024-02-12 21:16:04,519] Epoch: 41 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:16:04,520] Epoch: 41 | Ph  | Train Loss: 0.252 | Val Loss: 0.323\n",
      "[2024-02-12 21:16:04,520] Epoch: 41 | Ending LR: 0.000043 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.77it/s]\n",
      "[2024-02-12 21:16:15,463] Epoch: 42 | FT  | Train Loss: 0.25061 | Val Loss: 0.32318\n",
      "[2024-02-12 21:16:15,466] Epoch: 42 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:16:15,466] Epoch: 42 | Ph  | Train Loss: 0.251 | Val Loss: 0.323\n",
      "[2024-02-12 21:16:15,467] Epoch: 42 | Ending LR: 0.000039 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.55it/s]\n",
      "[2024-02-12 21:16:28,202] Epoch: 43 | FT  | Train Loss: 0.24936 | Val Loss: 0.32038\n",
      "[2024-02-12 21:16:28,207] Epoch: 43 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:16:28,209] Epoch: 43 | Ph  | Train Loss: 0.249 | Val Loss: 0.320\n",
      "[2024-02-12 21:16:28,210] Epoch: 43 | Ending LR: 0.000035 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.34it/s]\n",
      "[2024-02-12 21:16:41,222] Epoch: 44 | FT  | Train Loss: 0.24828 | Val Loss: 0.32336\n",
      "[2024-02-12 21:16:41,224] Epoch: 44 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:16:41,224] Epoch: 44 | Ph  | Train Loss: 0.248 | Val Loss: 0.323\n",
      "[2024-02-12 21:16:41,225] Epoch: 44 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.21it/s]\n",
      "[2024-02-12 21:16:54,347] Saving improved model after Val Loss improved from 0.31991 to 0.31898\n",
      "[2024-02-12 21:16:54,436] Epoch: 45 | FT  | Train Loss: 0.24574 | Val Loss: 0.31898\n",
      "[2024-02-12 21:16:54,437] Epoch: 45 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:16:54,437] Epoch: 45 | Ph  | Train Loss: 0.246 | Val Loss: 0.319\n",
      "[2024-02-12 21:16:54,438] Epoch: 45 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.18it/s]\n",
      "[2024-02-12 21:17:07,519] Epoch: 46 | FT  | Train Loss: 0.24504 | Val Loss: 0.32008\n",
      "[2024-02-12 21:17:07,520] Epoch: 46 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:17:07,521] Epoch: 46 | Ph  | Train Loss: 0.245 | Val Loss: 0.320\n",
      "[2024-02-12 21:17:07,522] Epoch: 46 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.23it/s]\n",
      "[2024-02-12 21:17:20,018] Epoch: 47 | FT  | Train Loss: 0.24390 | Val Loss: 0.31982\n",
      "[2024-02-12 21:17:20,020] Epoch: 47 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:17:20,021] Epoch: 47 | Ph  | Train Loss: 0.244 | Val Loss: 0.320\n",
      "[2024-02-12 21:17:20,022] Epoch: 47 | Ending LR: 0.000020 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.42it/s]\n",
      "[2024-02-12 21:17:32,790] Epoch: 48 | FT  | Train Loss: 0.24143 | Val Loss: 0.32286\n",
      "[2024-02-12 21:17:32,792] Epoch: 48 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:17:32,792] Epoch: 48 | Ph  | Train Loss: 0.241 | Val Loss: 0.323\n",
      "[2024-02-12 21:17:32,793] Epoch: 48 | Ending LR: 0.000022 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.33it/s]\n",
      "[2024-02-12 21:17:44,988] Epoch: 49 | FT  | Train Loss: 0.24219 | Val Loss: 0.32101\n",
      "[2024-02-12 21:17:44,989] Epoch: 49 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:17:44,990] Epoch: 49 | Ph  | Train Loss: 0.242 | Val Loss: 0.321\n",
      "[2024-02-12 21:17:44,993] Epoch: 49 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.41it/s]\n",
      "[2024-02-12 21:17:57,669] Epoch: 50 | FT  | Train Loss: 0.24194 | Val Loss: 0.32399\n",
      "[2024-02-12 21:17:57,670] Epoch: 50 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:17:57,671] Epoch: 50 | Ph  | Train Loss: 0.242 | Val Loss: 0.324\n",
      "[2024-02-12 21:17:57,672] Epoch: 50 | Ending LR: 0.000026 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.39it/s]\n",
      "[2024-02-12 21:18:10,526] Epoch: 51 | FT  | Train Loss: 0.24000 | Val Loss: 0.32152\n",
      "[2024-02-12 21:18:10,527] Epoch: 51 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:18:10,528] Epoch: 51 | Ph  | Train Loss: 0.240 | Val Loss: 0.322\n",
      "[2024-02-12 21:18:10,529] Epoch: 51 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.26it/s]\n",
      "[2024-02-12 21:18:23,663] Epoch: 52 | FT  | Train Loss: 0.24018 | Val Loss: 0.32176\n",
      "[2024-02-12 21:18:23,667] Epoch: 52 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:18:23,668] Epoch: 52 | Ph  | Train Loss: 0.240 | Val Loss: 0.322\n",
      "[2024-02-12 21:18:23,669] Epoch: 52 | Ending LR: 0.000029 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.29it/s]\n",
      "[2024-02-12 21:18:36,589] Epoch: 53 | FT  | Train Loss: 0.23992 | Val Loss: 0.32184\n",
      "[2024-02-12 21:18:36,592] Epoch: 53 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:18:36,593] Epoch: 53 | Ph  | Train Loss: 0.240 | Val Loss: 0.322\n",
      "[2024-02-12 21:18:36,594] Epoch: 53 | Ending LR: 0.000031 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.32it/s]\n",
      "[2024-02-12 21:18:49,743] Epoch: 54 | FT  | Train Loss: 0.23887 | Val Loss: 0.32002\n",
      "[2024-02-12 21:18:49,745] Epoch: 54 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:18:49,745] Epoch: 54 | Ph  | Train Loss: 0.239 | Val Loss: 0.320\n",
      "[2024-02-12 21:18:49,746] Epoch: 54 | Ending LR: 0.000029 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.26it/s]\n",
      "[2024-02-12 21:19:02,526] Epoch: 55 | FT  | Train Loss: 0.23838 | Val Loss: 0.32497\n",
      "[2024-02-12 21:19:02,527] Epoch: 55 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:19:02,528] Epoch: 55 | Ph  | Train Loss: 0.238 | Val Loss: 0.325\n",
      "[2024-02-12 21:19:02,530] Epoch: 55 | Ending LR: 0.000027 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.38it/s]\n",
      "[2024-02-12 21:19:15,358] Epoch: 56 | FT  | Train Loss: 0.23659 | Val Loss: 0.32350\n",
      "[2024-02-12 21:19:15,359] Epoch: 56 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:19:15,361] Epoch: 56 | Ph  | Train Loss: 0.237 | Val Loss: 0.324\n",
      "[2024-02-12 21:19:15,362] Epoch: 56 | Ending LR: 0.000026 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.31it/s]\n",
      "[2024-02-12 21:19:28,504] Saving improved model after Val Loss improved from 0.31898 to 0.31841\n",
      "[2024-02-12 21:19:28,594] Epoch: 57 | FT  | Train Loss: 0.23559 | Val Loss: 0.31841\n",
      "[2024-02-12 21:19:28,595] Epoch: 57 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:19:28,596] Epoch: 57 | Ph  | Train Loss: 0.236 | Val Loss: 0.318\n",
      "[2024-02-12 21:19:28,596] Epoch: 57 | Ending LR: 0.000024 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.22it/s]\n",
      "[2024-02-12 21:19:41,451] Epoch: 58 | FT  | Train Loss: 0.23466 | Val Loss: 0.31934\n",
      "[2024-02-12 21:19:41,456] Epoch: 58 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:19:41,458] Epoch: 58 | Ph  | Train Loss: 0.235 | Val Loss: 0.319\n",
      "[2024-02-12 21:19:41,459] Epoch: 58 | Ending LR: 0.000022 \n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:07<00:00,  3.30it/s]\n",
      "[2024-02-12 21:19:54,464] Epoch: 59 | FT  | Train Loss: 0.23441 | Val Loss: 0.31969\n",
      "[2024-02-12 21:19:54,466] Epoch: 59 | Amp | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "[2024-02-12 21:19:54,467] Epoch: 59 | Ph  | Train Loss: 0.234 | Val Loss: 0.320\n",
      "[2024-02-12 21:19:54,468] Epoch: 59 | Ending LR: 0.000020 \n"
     ]
    }
   ],
   "source": [
    "decimate_ratio_list = [0.1, 0.05, 0.02, 0.01, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "#decimate_ratio_list = [0.1, 0.05, 0.02, 0.01]\n",
    "\n",
    "dataset = HDF5Dataset('data/data_train_meanSubStdData.h5', verbose=False, \n",
    "                      transform_func=None, transform_func_kwargs=None, standardized=True)\n",
    "\n",
    "for decimate_ratio in decimate_ratio_list:\n",
    "    config_dict = PtychoNNTrainingConfigDict(\n",
    "        dataset_decimation_ratio=decimate_ratio,\n",
    "        batch_size_per_process=64,\n",
    "        num_epochs=60,\n",
    "        learning_rate_per_process=1e-4,\n",
    "        optimizer='adam',\n",
    "        model_save_dir='../../trained_models/model_phaseOnly_BN_36SpiralDatasets_meanSubStdData_dataDecimation_{}'.format(decimate_ratio),\n",
    "        validation_ratio=0.1,\n",
    "        dataset=dataset,\n",
    "        model=(PtychoNNPhaseOnlyModel, {'use_batchnorm': True}),\n",
    "        debug=False\n",
    "    )\n",
    "    \n",
    "    trainer = PtychoNNTrainer(config_dict)\n",
    "    trainer.build(seed=196)\n",
    "    \n",
    "    trainer.run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cee9d34-d678-46a0-adcf-664720e39601",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot_training_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bfc498-9e85-4d79-8131-f47edc12b5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9e2b5-d639-45da-bb4a-a7adccf85137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
